<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.2.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Shishir Shakya">

  
  
  
  
    
  
  <meta name="description" content="Parametric desity estimation (univariate) Draw from a normal distribution Given ${{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}}$ $i.i.d$ draw from a normal distribution with mean of $\mu$ and variance of ${{\sigma }^{2}}$ the joint $PDF$ can be expressed as: $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\prod\limits_{i=1}^{n}{\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{i}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}}$$ $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{1}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{2}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \cdots \times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{n}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}$$
The term $\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}$ is a constant multiplying this term for $n$ times gives ${{\left( \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}} \right)}^{n}}=\frac{1}{{{\left( 2\pi \sigma \right)}^{\frac{n}{2}}}}$.">

  
  <link rel="alternate" hreflang="en-us" href="../../tutorial/np01/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="../../styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-131353943-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="../../index.xml" type="application/rss+xml" title="Shishir Shakya">
  <link rel="feed" href="../../index.xml" type="application/rss+xml" title="Shishir Shakya">
  

  <link rel="manifest" href="../../site.webmanifest">
  <link rel="icon" type="image/png" href="../../img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="../../img/icon-192.png">

  <link rel="canonical" href="../../tutorial/np01/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@econshishir">
  <meta property="twitter:creator" content="@econshishir">
  
  <meta property="og:site_name" content="Shishir Shakya">
  <meta property="og:url" content="/tutorial/np01/">
  <meta property="og:title" content="Nonparametric Econometrics | Shishir Shakya">
  <meta property="og:description" content="Parametric desity estimation (univariate) Draw from a normal distribution Given ${{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}}$ $i.i.d$ draw from a normal distribution with mean of $\mu$ and variance of ${{\sigma }^{2}}$ the joint $PDF$ can be expressed as: $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\prod\limits_{i=1}^{n}{\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{i}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}}$$ $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{1}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{2}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \cdots \times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{n}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}$$
The term $\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}$ is a constant multiplying this term for $n$ times gives ${{\left( \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}} \right)}^{n}}=\frac{1}{{{\left( 2\pi \sigma \right)}^{\frac{n}{2}}}}$."><meta property="og:image" content="/img/portrait.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-09-09T00:00:00-04:00">
  
  <meta property="article:modified_time" content="2018-09-09T00:00:00-04:00">
  

  

  

  <title>Nonparametric Econometrics | Shishir Shakya</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="../../">Shishir Shakya</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#publications_selected">
            
            <span>Research</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../tutorial/">
            
            <span>Tutorials</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../files/cv.pdf">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>



<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
      




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
  <input name="q" type="search" class="form-control" id="search-query" placeholder="Search..." autocomplete="off">
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="../../tutorial/">Overview</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="../../tutorial/ci01/">Causal Inference</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="../../tutorial/ci01/">C1: Fundamental problem of causal inference</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="../../tutorial/np01/">Nonparametric Econometrics</a>
    <ul class="nav docs-sidenav">
      
      <li class="active">
        <a href="../../tutorial/np01/">NP1: Parametric density estimation (univariate case)</a>
      </li>
      
      <li >
        <a href="../../tutorial/np02/">NP2: NonParametric density estimation (univariate case)</a>
      </li>
      
      <li >
        <a href="../../tutorial/np03/">NP3: Three properties of kernel</a>
      </li>
      
      <li >
        <a href="../../tutorial/np04/">NP4: Taylor, MSE, Bias, Variance, Big O and Small o</a>
      </li>
      
      <li >
        <a href="../../tutorial/np05/">NP5: Nonparametric density estimation (Proof of Theorem 1.1)</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="../../tutorial/shiny01/">Shiny Tutorial</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="../../tutorial/shiny01/">Two Shiny</a>
      </li>
      
    </ul>
    

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="../../tutorial/slt01/">Statistical Learning Theory</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="../../tutorial/slt01/">S1: Gaussian Tail Inequality</a>
      </li>
      
      <li >
        <a href="../../tutorial/slt02/">S2: Hoeffding Inequality</a>
      </li>
      
      <li >
        <a href="../../tutorial/slt03/">S3: Kullback Leibler Distance</a>
      </li>
      
      <li >
        <a href="../../tutorial/slt04/">S4: Maximum of a random variable</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

    </div>

    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
      <p class="docs-toc-title">On this page</p>
      

      <nav id="TableOfContents">
<ul>
<li><a href="#parametric-desity-estimation-univariate">Parametric desity estimation (univariate)</a>
<ul>
<li><a href="#draw-from-a-normal-distribution">Draw from a normal distribution</a></li>
<li><a href="#the-log-likelihood-function">The log-likelihood function</a></li>
<li><a href="#logliklihood-function-optimization">Logliklihood function optimization</a></li>
<li><a href="#simulation-example">Simulation example</a></li>
<li><a href="#density-plot-example">Density plot example</a></li>
</ul></li>
</ul>
</nav>

      <ul class="nav toc-top">
        <li><a href="#">Back to top</a></li>
      </ul>

      
    </div>
    

    <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article" itemscope itemtype="http://schema.org/Article">

        <div class="docs-article-container">
          <h1 itemprop="name">Nonparametric Econometrics</h1>

          <div class="article-style" itemprop="articleBody">
            

<h1 id="parametric-desity-estimation-univariate">Parametric desity estimation (univariate)</h1>

<h2 id="draw-from-a-normal-distribution">Draw from a normal distribution</h2>

<p>Given ${{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}}$ $i.i.d$ draw from a normal distribution with mean of $\mu$ and variance of ${{\sigma }^{2}}$ the joint $PDF$ can be expressed as:
    $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\prod\limits_{i=1}^{n}{\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}}$$
    $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{1}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{2}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \cdots \times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{n}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}$$</p>

<p>The term $\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}$ is a constant multiplying this term for $n$ times gives ${{\left( \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}} \right)}^{n}}=\frac{1}{{{\left( 2\pi \sigma  \right)}^{\frac{n}{2}}}}$.
    $$f\left( {{X}_{1}},{{X}_{2}},\cdots ,{{X}_{3}} \right)=\frac{1}{{{\left( 2\pi \sigma  \right)}^{\frac{n}{2}}}}{{e}^{-\frac{{{\left( {{X}_{1}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}\times {{e}^{-\frac{{{\left( {{X}_{2}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \cdots \times {{e}^{-\frac{{{\left( {{X}_{n}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}$$</p>

<p>With the index law of addition i.e. we can add the indices for the same base
$f\left( {{X}_{1}},{{X}_{2}},\cdots ,{{X}_{3}} \right)={{e}^{-\frac{{{\left( {{X}_{1}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}\times {{e}^{-\frac{{{\left( {{X}_{2}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \cdots \times {{e}^{-\frac{{{\left( {{X}_{n}}-\mu  \right)}^{2}}}{2{{\sigma }^{2}}}}}$
$f\left( {{X}_{1}},{{X}_{2}},\cdots ,{{X}_{3}} \right)={{e}^{-\frac{1}{2{{\sigma }^{2}}}\left[ {{\left( {{X}_{1}}-\mu  \right)}^{2}}+{{\left( {{X}_{2}}-\mu  \right)}^{2}}+\cdots +{{\left( {{X}_{n}}-\mu  \right)}^{2}} \right]}}$
$f\left( {{X}_{1}},{{X}_{2}},\cdots ,{{X}_{3}} \right)={{e}^{-\frac{1}{2{{\sigma }^{2}}}\sum\limits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}}}$</p>

<p>Therefore,
    $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\frac{1}{{{\left( 2\pi {{\sigma }^{2}} \right)}^{\frac{n}{2}}}}{{e}^{-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}}}$$</p>

<h2 id="the-log-likelihood-function">The log-likelihood function</h2>

<p>Taking the logarithm, we get the log-likelihood function as:
    $$\ln f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\ln \left[ \frac{1}{{{\left( 2\pi {{\sigma }^{2}} \right)}^{\frac{n}{2}}}}{{e}^{-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}}} \right]$$</p>

<p>With the property of log i.e. multiplication inside the log can be turned into addition outside the log, and vice versa or $\ln (ab)=\ln (a)+\ln (b)$
    $$L\left( \mu ,{{\sigma }^{2}} \right)\equiv \ln \left( \frac{1}{{{\left( 2\pi {{\sigma }^{2}} \right)}^{\frac{n}{2}}}} \right)+\ln \left( {{e}^{-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}}} \right)$$</p>

<p>With natural log property i.e. when ${{e}^{y}}=x$ then $\ln (x)=\ln ({{e}^{y}})=y$
    $$L\left( \mu ,{{\sigma }^{2}} \right)\equiv \ln \left( {{\left( 2\pi {{\sigma }^{2}} \right)}^{-\frac{n}{2}}} \right)-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}$$
    $$L\left( \mu ,{{\sigma }^{2}} \right)\equiv -\frac{n}{2}\ln \left( 2\pi  \right)-\frac{n}{2}\ln {{\sigma }^{2}}-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}$$
    $$L\left( \mu ,{{\sigma }^{2}} \right)\equiv \ln f\left( {{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}};\mu ,{{\sigma }^{2}} \right)=-\frac{n}{2}\ln \left( 2\pi  \right)-\frac{n}{2}\ln \left( {{\sigma }^{2}} \right)-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}$$</p>

<h2 id="logliklihood-function-optimization">Logliklihood function optimization</h2>

<p>To find the optimum value of
    $L\left( \mu ,{{\sigma }^{2}} \right)\equiv \ln f\left( {{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}};\mu ,{{\sigma }^{2}} \right)=-\frac{n}{2}\ln \left( 2\pi  \right)-\frac{n}{2}\ln \left( {{\sigma }^{2}} \right)-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}$, we take the first order condition w.r.t. $\mu$ and ${{\sigma }^{2}}$. The necessary first order condition w.r.t $\mu$ is given as:
    $$\frac{\partial L\left( \mu ,{{\sigma }^{2}} \right)}{\partial \mu }=\frac{\partial }{\partial \mu }\left[ -\frac{n}{2}\ln \left( 2\pi  \right)-\frac{n}{2}\ln \left( {{\sigma }^{2}} \right)-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}} \right]=0$$</p>

<p>Here, $\frac{\partial }{\partial \mu }\left[ -\frac{n}{2}\ln \left( 2\pi  \right) \right]=0$ and $\frac{\partial }{\partial \mu }\left[ -\frac{n}{2}\ln \left( {{\sigma }^{2}} \right) \right]=0$, so we only need to solve for:
    $$\frac{\partial }{\partial \mu }\left( -\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}} \right)=0$$
    $$-\frac{1}{2{{\sigma }^{2}}}\frac{\partial }{\partial \mu }\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}=0$$<br />
    $$-\frac{1}{2{{\sigma }^{2}}}\left[ \frac{\partial }{\partial \mu }\left( {{\left( {{X}_{1}}-\mu  \right)}^{2}}+{{\left( {{X}_{2}}-\mu  \right)}^{2}}+\cdots +{{\left( {{X}_{n}}-\mu  \right)}^{2}} \right) \right]=0$$<br />
    $$-\frac{1}{2{{\sigma }^{2}}}\left[ \frac{\partial {{\left( {{X}_{1}}-\mu  \right)}^{2}}}{\partial \mu }+\frac{\partial {{\left( {{X}_{2}}-\mu  \right)}^{2}}}{\partial \mu }+\cdots +\frac{\partial {{\left( {{X}_{n}}-\mu  \right)}^{2}}}{\partial \mu } \right]=0$$</p>

<p>With the chain rule i.e $\frac{\partial {{\left( {{X}_{1}}-\mu  \right)}^{2}}}{\partial \mu }=\frac{\partial {{\left( {{X}_{1}}-\mu  \right)}^{2}}}{\partial \left( {{X}_{1}}-\mu  \right)}\frac{\partial \left( {{X}_{1}}-\mu  \right)}{\partial \mu }=2\left( {{X}_{1}}-\mu  \right)\left( 1 \right)=2\left( {{X}_{1}}-\mu  \right)$.  Hence,
    $$-\frac{1}{2{{\sigma }^{2}}}\left[ 2\left( {{X}_{1}}-\mu  \right)+2\left( {{X}_{2}}-\mu  \right)+\cdots +2\left( {{X}_{n}}-\mu  \right) \right]=0$$
    $$-\frac{1}{{{\sigma }^{2}}}\sum\limits_{i=1}^{n}{\left( {{X}_{i}}-\mu  \right)}=0$$</p>

<p>Since ${{\sigma }^{2}}\ne 0$, So,
    $$\sum\limits_{i=1}^{n}{\left( {{X}_{i}}-\mu  \right)}=0$$
    $$\sum\limits_{i=1}^{n}{{{X}_{i}}}-\sum\limits_{i=1}^{n}{\mu }=0$$
    $$\sum\limits_{i=1}^{n}{{{X}_{i}}}-n\mu =0$$<br />
    $$\hat{\mu }={{n}^{-1}}\sum\limits_{i=1}^{n}{{{X}_{i}}}$$</p>

<p>The necessary first order condition w.r.t ${{\sigma }^{2}}$ is given as:
    $$\frac{\partial L\left( \mu ,{{\sigma }^{2}} \right)}{\partial {{\sigma }^{2}}}=\frac{\partial }{\partial {{\sigma }^{2}}}\left[ -\frac{n}{2}\ln \left( 2\pi  \right)-\frac{n}{2}\ln \left( {{\sigma }^{2}} \right)-\frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}} \right]=0$$
    $$-\frac{\partial }{\partial {{\sigma }^{2}}}\left[ \frac{n}{2}\ln \left( 2\pi  \right) \right]-\frac{\partial }{\partial {{\sigma }^{2}}}\left[ -\frac{n}{2}\ln \left( {{\sigma }^{2}} \right) \right]-\frac{\partial }{\partial {{\sigma }^{2}}}\left[ \frac{1}{2{{\sigma }^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}} \right]=0$$
    $$0-\frac{n}{2}\frac{1}{{{\sigma }^{2}}}-\frac{1}{2}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}\frac{\partial {{\left( {{\sigma }^{2}} \right)}^{-1}}}{\partial {{\sigma }^{2}}}=0$$
    $$-\frac{1}{2}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}\left( -1 \right){{\left( {{\sigma }^{2}} \right)}^{-2}}=\frac{n}{2{{\sigma }^{2}}}$$
    $$\frac{1}{2{{\left( {{\sigma }^{2}} \right)}^{2}}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}=\frac{n}{2{{\sigma }^{2}}}$$
    $${{\hat{\sigma }}^{2}}={{n}^{-1}}\sum\nolimits_{i=1}^{n}{{{\left( {{X}_{i}}-\mu  \right)}^{2}}}$$</p>

<p>$\hat{\mu }$ and ${{\hat{\sigma }}^{2}}$ above are the maximum likelihood estimator of $\mu$ and ${{\sigma }^{2}}$, respectively, the resulting estimator of $f\left( x \right)$ is:
    $$\hat{f}\left( x \right)=\frac{1}{\sqrt{2\pi {{{\hat{\sigma }}}^{2}}}}{{e}^{\left[ -\frac{1}{2}\left( \frac{x-\hat{\mu }}{{{{\hat{\sigma }}}^{2}}} \right) \right]}}$$</p>

<h2 id="simulation-example">Simulation example</h2>

<p>Let&rsquo;s simulate 10000 random observation from a normal distribution with mean of 2 and standard deviation of 1.5. To reproduce the results we will use <code>set.seed()</code> function.</p>

<pre><code class="language-r">set.seed(1234)
N &lt;- 10000
mu &lt;- 2
sigma &lt;- 1.5
x &lt;- rnorm(n = N, mean = mu, sd = sigma)
</code></pre>

<p>We can use <code>mean()</code> and <code>sd()</code> function to find the mean and sigma</p>

<pre><code class="language-r"># mean
sum(x)/length(x)
mean(x)

# standard deviation
sqrt(sum((x-mean(x))^2)/(length(x)-1))
sd(x)
</code></pre>

<p>However, if can also simulate and try the optimization using the <code>mle</code> function from the <code>stat 4</code> package in R.</p>

<pre><code class="language-r">LL &lt;- function(mu, sigma){
  R &lt;- dnorm(x, mu, sigma)
  -sum(log(R))
}

stats4::mle(LL, start = list(mu = 1, sigma = 1))
</code></pre>

<p>To supress the warnings in <code>R</code> and garanatee the solution we can use following codes.</p>

<pre><code class="language-r">stats4::mle(LL, start = list(mu = 1, sigma = 1), method = &quot;L-BFGS-B&quot;,
            lower = c(-Inf, 0), upper = c(Inf, Inf))
</code></pre>

<h2 id="density-plot-example">Density plot example</h2>

<p>Given the data of x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59), we can estimate as <code>sum(x)/length(x)</code> or <code>mean(x)</code> and as <code>sum((x-mean(x))^2)/(length(x)-1)</code> or <code>var(x)</code>. Note I use the sample variance formula.</p>

<pre><code class="language-r">x &lt;- c(-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59)

# mean
sum(x)/length(x)
mean(x)

# variance
sum((x-mean(x))^2)/(length(x)-1)
var(x)
</code></pre>

<p>We can also plot a parametric density function. But prior we plot, we have to sort the data.</p>

<pre><code class="language-r">x &lt;- sort(x)
plot(x ,dnorm(x,mean=mean(x),sd=sd(x)),ylab=&quot;Density&quot;,type=&quot;l&quot;, col = &quot;blue&quot;, lwd = 3)
</code></pre>

<p>Let&rsquo;s also plot a graph of histogram using bin width ranging from -1.5 through 2.0.</p>

<pre><code class="language-r">hist(x, breaks=seq(-1.5,2,by=0.5), prob = TRUE)
</code></pre>

          </div>

          

        </div>

        <div class="body-footer">
          Last updated on Sep 9, 2018
        </div>

      </article>

      <footer class="site-footer">
  
  <p class="powered-by">
    <a href="../../privacy/">Privacy Policy</a>
  </p>
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
  </p>
</footer>


    </main>
  </div>
</div>

    

    
    
    
    <script src="../../js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//SS.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    <script src="../../js/academic.min.d037ee5294b166a79dec317c58aea9cc.js"></script>

    

  </body>
</html>


