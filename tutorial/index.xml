<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview on Shishir Shakya</title>
    <link>/tutorial/</link>
    <description>Recent content in Overview on Shishir Shakya</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 09 Sep 2018 00:00:00 -0400</lastBuildDate>
    
	<atom:link href="/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Causal Inference</title>
      <link>/tutorial/ci01/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/ci01/</guid>
      <description>Potential Outcome Framework There are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference.
For an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say $T$) on some outcomes (say capacity to solve riddle quizzes say $Y$).</description>
    </item>
    
    <item>
      <title>Nonparametric Econometrics</title>
      <link>/tutorial/np01/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/np01/</guid>
      <description>Parametric desity estimation (univariate) Draw from a normal distribution Given ${{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}}$ $i.i.d$ draw from a normal distribution with mean of $\mu$ and variance of ${{\sigma }^{2}}$ the joint $PDF$ can be expressed as: $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\prod\limits_{i=1}^{n}{\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{i}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}}$$ $$f\left( {{X}_{1}},{{X}_{2}},\ldots {{X}_{3}} \right)=\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{1}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{2}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}\times \cdots \times \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}{{e}^{-\frac{{{\left( {{X}_{n}}-\mu \right)}^{2}}}{2{{\sigma }^{2}}}}}$$
The term $\frac{1}{\sqrt{2\pi {{\sigma }^{2}}}}$ is a constant multiplying this term for $n$ times gives ${{\left( \frac{1}{\sqrt{2\pi {{\sigma }^{2}}}} \right)}^{n}}=\frac{1}{{{\left( 2\pi \sigma \right)}^{\frac{n}{2}}}}$.</description>
    </item>
    
    <item>
      <title>Nonparametric Econometrics</title>
      <link>/tutorial/np02/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/np02/</guid>
      <description>Univariate NonParametric Density Estimation Set-up Consider $i.i.d$ data ${{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}}$ with $F\left( \centerdot \right)$ an unknown $CDF$ where $F\left( x \right)=P\left[ X\le x \right]$ or $CDF$ of $X$ evaluated at $x$. We can do a naive estimation as $F\left( x \right)=P\left[ X\le x \right]$ as cumulative sums of relative frequency as: $${{F}_{n}}\left( x \right)={{n}^{-1}}\left( numbers\ of\ {{X}_{i}}\le x \right)$$ and $n\to\infty$ yields ${{F}_{n}}\left( x \right)\to F\left( x \right)$.
The $PDF$ of $F\left( x \right)=P\left[ X\le x \right]$ is given as $f\left( x \right)=\frac{d}{dx}F\left( x \right)$ and an obvious estimator is: $$\hat{f}\left( x \right)=\frac{rise}{run}=\frac{{{F}_{n}}\left( x+h \right)-{{F}_{n}}\left( x-h \right)}{x+h-\left( x-h \right)}=\frac{{{F}_{n}}\left( x+h \right)-{{F}_{n}}\left( x-h \right)}{2h}$$ $$\hat{f}\left( x \right)={{n}^{-1}}\frac{1}{2h}\left( numbers\ of\ {{X}_{i}}\ in\ between\ \left[ x-h,x+h \right] \right)$$</description>
    </item>
    
    <item>
      <title>Nonparametric Econometrics</title>
      <link>/tutorial/np03/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/np03/</guid>
      <description>Three properties of kernel estimator For any general nonnegative bounded kernel function $k\left( v \right)$ where $v=\left( \frac{{{X}_{i}}-x}{h} \right)$, the kernel estimator $\hat{f}\left( x \right)$ is a consistent estimator of $f\left( x \right)$ that satisfies three conditions:
Propetry-1 First is area under a kernel to be unity. $$\int{k(v)dv=1}$$
Propetry-2 Second is the symmetry kernel $$\int{vk(v)dv=0}$$ which implies symmetry condition i.e. $k(v)=k(-v)$. For asymmetric kernels see Abadir and Lawford (2004).</description>
    </item>
    
    <item>
      <title>Nonparametric Econometrics</title>
      <link>/tutorial/np04/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/np04/</guid>
      <description>Some review of concepts Taylor series expansion For a univariate function $g(x)$evaluated at ${{x}_{0}}$ , we can express with Taylor series expansion as: $$g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\cdots +\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\cdots$$
For a univariate function $g(x)$evaluated at ${{x}_{0}}$ that is $m$ times differentiable, we can express with Taylor series expansion as: $$g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\cdots +\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\frac{1}{(m)!}{{g}^{(m)}}(\xi ){{(x-{{x}_{0}})}^{m}}$$ where ${{g}^{(s)}}={{\left. \frac{{{\partial }^{s}}g(x)}{\partial {{x}^{2}}} \right|}_{x={{x}_{0}}}}$ and and $\xi$ lies between $x$ and ${{x}_{0}}$
MSE, variance and biases Say $\hat{\theta }$ is an estimator for true $\theta$, then the Mean Squared Error $MSE$ of an estimator $\hat{\theta }$ is the mean of squared deviation between $\hat{\theta }$ and $\theta$ and given as: $$MSE=E\left[ {{\left( \hat{\theta }-\theta \right)}^{2}} \right]$$ $$MSE=E\left[ \left( {{{\hat{\theta }}}^{2}}-2\hat{\theta }\theta +{{\theta }^{2}} \right) \right]$$ $$MSE=E[{{\hat{\theta }}^{2}}]+E[{{\theta }^{2}}]-2\theta E[\hat{\theta }]$$ $$MSE=\underbrace{E[{{{\hat{\theta }}}^{2}}]-{{E}^{2}}[{{{\hat{\theta }}}^{2}}]}_{\operatorname{var}(\hat{\theta })}+\underbrace{{{E}^{2}}[{{{\hat{\theta }}}^{2}}]+E[{{\theta }^{2}}]-2\theta E[\hat{\theta }]}_{E{{\left( E[\hat{\theta }]-\theta \right)}^{2}}}$$ $$MSE=\operatorname{var}(\hat{\theta })+\underbrace{E{{\left( E[\hat{\theta }]-\theta \right)}^{2}}}_{squared\ of\ bias\ of\ \hat{\theta }}$$ $$MSE=\operatorname{var}(\hat{\theta })+{{\left[ bias(\hat{\theta }) \right]}^{2}}$$</description>
    </item>
    
    <item>
      <title>Nonparametric Econometrics</title>
      <link>/tutorial/np05/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/np05/</guid>
      <description>Theorem 1.1. and detailed proofs Let ${{X}_{1}},{{X}_{2}},\ldots ,{{X}_{n}}$ $i.i.d$ observation having a three-times differentiable $PDF$ $f(x)$, and ${{f}^{s}}(x)$ denote the $s-th$ order derivative of $f(x)\ s=(1,2,3)$. Let $x$ be an interior point in the support of $X$, and let $\hat{f}(x)$ be $\frac{1}{2h}{{n}^{-1}}\left( numbers\ of\ {{X}_{i}}\ in\ between\ \left[ x-h,x+h \right] \right)$. Assume that the kernel function $k\left( \centerdot \right)$ bounded and satisfies: $\int{k(v)dv=1}$, $k(v)=k(-v)$ and $\int{{{v}^{2}}k(v)dv={{\kappa }_{2}}&amp;gt;0}$. And as $n\to\infty$, $h\to 0$ and $nh\to\infty$ then, the $MSE$ of estimator $\hat{f}(x)$ is given as: $$MSE\left( \hat{f}(x) \right)=\frac{{{h}^{4}}}{4}{{\left[ {{\kappa }_{2}}{{f}^{\left( 2 \right)}}(x) \right]}^{2}}+\frac{\kappa f(x)}{nh}+o\left( {{h}^{4}}+{{(nh)}^{-1}} \right)=O\left( {{h}^{4}}+{{(nh)}^{-1}} \right)$$ Where, $v=\left( \frac{{{X}_{i}}-x}{h} \right)$, ${{\kappa }_{2}}=\int{{{v}^{2}}k(v)dv}$ and $\kappa =\int{{{k}^{2}}(v)dv}$</description>
    </item>
    
    <item>
      <title>Statistical Learning Theory</title>
      <link>/tutorial/slt01/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/slt01/</guid>
      <description>Gaussian Tail Inequality Given ${{x}_{1}},\cdots ,{{x}_{n}}\sim N\left( 0,1 \right)$ then, $P\left( \left| X \right|&amp;gt;\varepsilon \right)\le \frac{2{{e}^{-{{{\varepsilon }^{2}}}/{2}\;}}}{\varepsilon }$ and $P\left( \left| {{{\bar{X}}}_{n}} \right|&amp;gt;\varepsilon \right)\le \frac{2}{\sqrt{n}\varepsilon }{{e}^{-{n{{\varepsilon }^{2}}}/{2}\;}}\overset{l\arg e\ n}{\mathop{\le }}\,{{e}^{-{n{{\varepsilon }^{2}}}/{2}\;}}$.
Proof of Gaussian Tail Inequality Consider a univariate ${{x}_{1}},\cdots ,{{x}_{n}}\sim N\left( 0,1 \right)$, then the probability density function is given as $\phi \left( x \right)=\frac{1}{\sqrt{2\pi }}{{e}^{-\frac{{{x}^{2}}}{2}}}$.
Let&amp;rsquo;s take the derivative w.r.t $x$ we get: $$\frac{d\phi \left( x \right)}{dx}={\phi }&amp;rsquo;\left( x \right)=\frac{d\left( \frac{1}{\sqrt{2\pi }}{{e}^{-\frac{{{x}^{2}}}{2}}} \right)}{dx}=\frac{1}{\sqrt{2\pi }}\frac{d\left( \,{{e}^{-\frac{{{x}^{2}}}{2}}} \right)}{dx}=\frac{1}{\sqrt{2\pi }}\frac{d\left( \,{{e}^{-\frac{{{x}^{2}}}{2}}} \right)}{d\left( -\frac{{{x}^{2}}}{2} \right)}\frac{d\left( -\frac{{{x}^{2}}}{2} \right)}{dx}=\frac{1}{\sqrt{2\pi }}{{e}^{-\frac{{{x}^{2}}}{2}}}\left( -x \right)=-x\phi \left( x \right)$$</description>
    </item>
    
    <item>
      <title>Statistical Learning Theory</title>
      <link>/tutorial/slt02/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/slt02/</guid>
      <description>Before, I start the solution of Hoeffding&amp;rsquo;s Inequality, let me first quickly note Markov&amp;rsquo;s Inequality and Chebyshev&amp;rsquo;s Inequality.
Markov&amp;rsquo;s Inequality Let $X$ be a non-negative random variable and $E\left( X \right)$ exists, For any $t&amp;gt;0$; $P\left( X&amp;gt;t \right)\le \frac{E\left( X \right)}{t}$
Proof of Markov&amp;rsquo;s Inequality For $X&amp;gt;0$ we can write expectation of $X$ as: $$E\left( X \right)=\int\limits_{0}^{\infty }{xp\left( x \right)dx}=\int\limits_{0}^{t}{xp\left( x \right)dx}+\int\limits_{t}^{\infty }{xp\left( x \right)dx}\ge \int\limits_{t}^{\infty }{xp\left( x \right)dx}$$ $$E\left( X \right)\ge \int\limits_{t}^{\infty }{xp\left( x \right)dx}\ge t\int\limits_{t}^{\infty }{p\left( x \right)dx}=tP\left( X&amp;gt;t \right)$$ $$\frac{E\left( X \right)}{t}\ge P\left( X&amp;gt;t \right)$$ $$P\left( X&amp;gt;t \right)\le \frac{E\left( X \right)}{t}$$</description>
    </item>
    
    <item>
      <title>Statistical Learning Theory</title>
      <link>/tutorial/slt03/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/slt03/</guid>
      <description>Kullback Leibler Distance Proof for distance between density is greater than zero. Proof that the distance between any two density $p$ and $q$ whose random variable is $X\tilde{\ }p$ ($p$ is some distribution) is always greater than or equal to zero.
Prior answering this, let&amp;rsquo;s quickly note two inequality, namely Cauchy-Swartz Inequality and Jensen&amp;rsquo;s inequality.
Cauchy-Swartz Inequality $\left| EXY \right|\le E\left| XY \right|\le \sqrt{E\left( {{X}^{2}} \right)}\sqrt{E\left( {{Y}^{2}} \right)}$.
Jensen&amp;rsquo;s inequality If $g$ is convex then$Eg\left( X \right)\ge g\left( EX \right)$.</description>
    </item>
    
    <item>
      <title>Statistical Learning Theory</title>
      <link>/tutorial/slt04/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/slt04/</guid>
      <description>Maximum of a random variable Let ${{X}_{i}},\ldots {{X}_{n}}$ be random variable. Suppose there exist $\sigma &amp;gt;0$ such that $E\left( {{e}^{t{{X}_{i}}}} \right)\le {{e}^{\frac{{{t}^{2}}{{\sigma }^{2}}}{2}}}$. Then, $E\underset{1\le i\le n}{\mathop{\max }}\,{{X}_{i}}\le \sigma \sqrt{2\log n}$.
Maximum of a random variable represents how to bound maximum value of a random variable? i.e. Say the random variable is ${{X}_{1}},\ldots ,{{X}_{n}}$ and say it is arranged in ascending order such that ${{X}_{\left( 1 \right)}}\le {{X}_{\left( 2 \right)}}\le \ldots \le {{X}_{\left( n \right)}}$ and ${{X}_{\left( n \right)}}={{E}_{\max }}\left[ {{X}_{1}},\cdots ,{{X}_{n}} \right]$ how to compute the distribution of that maximum value?</description>
    </item>
    
    <item>
      <title>Shiny Tutorial</title>
      <link>/tutorial/shiny01/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/tutorial/shiny01/</guid>
      <description>Shiny Tutorial The following two links show my online course on the Shiny App development and deployment. This course was co-designed with Prof. Greg DeAngelo and was taught in summer of 2017 and 2018 for BUDA 550: Business Data Visualization course of Business and Data Analytics (M.S).
Part-1 | Part-2</description>
    </item>
    
  </channel>
</rss>