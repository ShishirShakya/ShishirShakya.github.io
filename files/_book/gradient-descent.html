<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Lecture 13 Gradient descent | Mostly Handsdirty Metrics</title>
<meta name="author" content="Shishir Shakya">
<meta name="description" content="13.1 Intution Gradient descent is an iterative optimization algorithm used to find a function’s “minimum” value by taking small steps in the direction opposite to the gradient (slope) of the...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Lecture 13 Gradient descent | Mostly Handsdirty Metrics">
<meta property="og:type" content="book">
<meta property="og:description" content="13.1 Intution Gradient descent is an iterative optimization algorithm used to find a function’s “minimum” value by taking small steps in the direction opposite to the gradient (slope) of the...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lecture 13 Gradient descent | Mostly Handsdirty Metrics">
<meta name="twitter:description" content="13.1 Intution Gradient descent is an iterative optimization algorithm used to find a function’s “minimum” value by taking small steps in the direction opposite to the gradient (slope) of the...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Mostly Handsdirty Metrics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> This is a work in progress!</a></li>
<li><a class="" href="data-types-structures-and-extractions.html"><span class="header-section-number">2</span> Data types, structures, and extractions</a></li>
<li><a class="" href="sampling.html"><span class="header-section-number">3</span> Sampling</a></li>
<li><a class="" href="for-loop-and-probability.html"><span class="header-section-number">4</span> For loop and probability</a></li>
<li><a class="" href="if-else-and-probability.html"><span class="header-section-number">5</span> If else and probability</a></li>
<li><a class="" href="basic-statistics.html"><span class="header-section-number">6</span> Basic statistics</a></li>
<li><a class="" href="investing-basics.html"><span class="header-section-number">7</span> Investing basics</a></li>
<li><a class="" href="recursive-investment.html"><span class="header-section-number">8</span> Recursive investment</a></li>
<li><a class="" href="index-universal-life-insurance.html"><span class="header-section-number">9</span> Index universal life insurance</a></li>
<li><a class="" href="stock-market-monte-carlo-simulation.html"><span class="header-section-number">10</span> Stock market monte-carlo simulation</a></li>
<li><a class="" href="confidence-interval-z-scores-hypothesis-testing.html"><span class="header-section-number">11</span> Confidence interval, z-scores, hypothesis testing</a></li>
<li><a class="" href="unconstrainted-optimization.html"><span class="header-section-number">12</span> Unconstrainted optimization</a></li>
<li><a class="active" href="gradient-descent.html"><span class="header-section-number">13</span> Gradient descent</a></li>
<li><a class="" href="gradient-descent-for-more-than-one-variable.html"><span class="header-section-number">14</span> Gradient descent for more than one variable</a></li>
<li><a class="" href="gradient-descent-for-simple-regression.html"><span class="header-section-number">15</span> Gradient descent for simple regression</a></li>
<li><a class="" href="standard-error.html"><span class="header-section-number">16</span> Standard error</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="gradient-descent" class="section level1" number="13">
<h1>
<span class="header-section-number">Lecture 13</span> Gradient descent<a class="anchor" aria-label="anchor" href="#gradient-descent"><i class="fas fa-link"></i></a>
</h1>
<div id="intution" class="section level2" number="13.1">
<h2>
<span class="header-section-number">13.1</span> Intution<a class="anchor" aria-label="anchor" href="#intution"><i class="fas fa-link"></i></a>
</h2>
<p>Gradient descent is an iterative optimization algorithm used to find a function’s “minimum” value by taking small steps in the direction opposite to the gradient (slope) of the function at the current position.</p>
<p>Let me give an intuitive example.</p>
<ol style="list-style-type: decimal">
<li><p>Given a function, first, analytically find the gradient. For example, given the function <span class="math inline">\(y = f(x) = 2x^2 - 8x + 3\)</span>, the gradient is <span class="math inline">\(f'(x) = 4x - 8\)</span>.</p></li>
<li><p>Calculate the gradient of the function at the initial guess. Guess can be any number. It does not matter. Usually, we start at zero. So let’s evaluate the gradient at 0. <span class="math inline">\(f'(x_0 = 0) = 4x - 8 = 4(0) - 8 = -8\)</span>.</p></li>
<li><p>We want to learn from this gradient. How much we want to learn is defined by the learning rate parameter. For our example, let’s define the learning parameter as <span class="math inline">\(\alpha = 0.01\)</span>. Then we learn say <span class="math inline">\(\alpha f'(x_0 = 0)\)</span> = <span class="math inline">\(0.01 (-8) = -0.08\)</span>.</p></li>
<li><p>Now, we update the value of the initial guess of 0 with what we learn. Thus new guess say, <span class="math inline">\(x_1\)</span>, is <span class="math inline">\(x_1 = x_0 - \alpha f'(x_0 = 0)\)</span>. In our example, <span class="math inline">\(x_1 = 0-(-0.08) = 0.08\)</span>. Note if the gradient is decreasing, you would do the opposite– increase the value of x. And if the gradient is positive or increasing, you would do the opposite– decrease the value of x.</p></li>
<li><p>Next, we calculate the function’s gradient using <span class="math inline">\(x_1\)</span> such that <span class="math inline">\(f'(x_0 = 0) = 4(0.08) - 8 = -7.68\)</span>. Then again update the value <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_2\)</span> as <span class="math inline">\(x_2 = x_1 - \alpha f'(x_1 = 0.01)\)</span> or <span class="math inline">\(x_2 = 0.08 - 0.01(-7.68) = 0.157\)</span>.</p></li>
<li><p>We iterate this process for a long time, say 1000 times, which enables us to find the value of <span class="math inline">\(x\)</span> that minimizes the function <span class="math inline">\(y = f(x) = 2x^2 - 8x + 3\)</span>.</p></li>
</ol>
</div>
<div id="gradient-descent-using-for-loop-in-r-codes" class="section level2" number="13.2">
<h2>
<span class="header-section-number">13.2</span> Gradient descent using “for loop” in R codes<a class="anchor" aria-label="anchor" href="#gradient-descent-using-for-loop-in-r-codes"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s first define the function and gradient of the function.</p>
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define the function</span></span>
<span><span class="va">f</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="fl">8</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Define the gradient of the function</span></span>
<span><span class="va">f_grad</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fl">4</span><span class="op">*</span><span class="va">x</span> <span class="op">-</span> <span class="fl">8</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Now, we define three criteria. First is the initialization value. We choose <span class="math inline">\(x = 0\)</span>. Second is the learning rate. We choose 0.01. Third is the how many time we want to iterate it, say 1000 times.</p>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set the initial value of x</span></span>
<span><span class="va">x</span> <span class="op">=</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Set the learning rate</span></span>
<span><span class="va">alpha</span> <span class="op">=</span> <span class="fl">0.01</span></span>
<span></span>
<span><span class="co"># Set the number of iterations</span></span>
<span><span class="va">n_iter</span> <span class="op">=</span> <span class="fl">1000</span></span></code></pre></div>
<p>Now, with this information, let’s implement gradient descent. I am suppressing the results of this table because it will print 1000 rows of x values. There is a better way to do it. I will show that latter. When you use this code, simply un-comment <code>print(x)</code>.</p>
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform gradient descent</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n_iter</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">=</span> <span class="va">x</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">*</span> <span class="fu">f_grad</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="co">#print(x)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>After running this algorithm, just simply check the value of x, you will find the answer of x.</p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Print the final value of x</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2</span></span></code></pre></div>
</div>
<div id="gradient-descent-using-while-in-r-codes" class="section level2" number="13.3">
<h2>
<span class="header-section-number">13.3</span> Gradient descent using “while” in R codes<a class="anchor" aria-label="anchor" href="#gradient-descent-using-while-in-r-codes"><i class="fas fa-link"></i></a>
</h2>
<p>Lets use the while conditional statement to implement the loop. We have similar set up.</p>
<div class="sourceCode" id="cb105"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define the function</span></span>
<span><span class="va">f</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fl">2</span><span class="op">*</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="fl">8</span><span class="op">*</span><span class="va">x</span> <span class="op">+</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Define the gradient of the function</span></span>
<span><span class="va">f_grad</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fl">4</span><span class="op">*</span><span class="va">x</span> <span class="op">-</span> <span class="fl">8</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Set the initial value of x</span></span>
<span><span class="va">x</span> <span class="op">=</span> <span class="fl">0</span></span>
<span></span>
<span><span class="co"># Set the learning rate</span></span>
<span><span class="va">alpha</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span></span>
<span><span class="co"># Set the maximum number of iterations</span></span>
<span><span class="va">max_iter</span> <span class="op">=</span> <span class="fl">1000</span></span></code></pre></div>
<p>However, here we want to initialize the iteration from 0. We also want to set some tolerance of error, say 0.1. This means we are okay to have a gradient near 0.1 rather than perfectly zero.</p>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set the tolerance</span></span>
<span><span class="va">tol</span> <span class="op">=</span> <span class="fl">0.1</span></span>
<span></span>
<span><span class="co"># Initialize the number of iterations</span></span>
<span><span class="va">n_iter</span> <span class="op">=</span> <span class="fl">0</span></span></code></pre></div>
<p>Next, we use the while statement. In R, a while statement is used to execute a block of code repeatedly as long as a specified condition is true. The condition is an expression that is evaluated before each iteration of the loop. If the condition evaluates to TRUE, the code within the loop is executed. If the condition evaluates to FALSE, the loop is terminated, and control is passed to the next statement after the loop.</p>
<p>In our case, while the n_iter &lt; max_iter, calculate the gradient of the function. If the absolute value of the gradient of the function is less than our tolerance of error, in our case 0.1, stop or break the for a loop.</p>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Perform gradient descent</span></span>
<span><span class="kw">while</span> <span class="op">(</span><span class="va">n_iter</span> <span class="op">&lt;</span> <span class="va">max_iter</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Calculate the gradient of the function</span></span>
<span>  <span class="va">grad</span> <span class="op">=</span> <span class="fu">f_grad</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Check if the gradient is below the tolerance</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">grad</span><span class="op">)</span> <span class="op">&lt;</span> <span class="va">tol</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co">#print("converged!")</span></span>
<span>    <span class="kw">break</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Update the value of x</span></span>
<span>  <span class="va">x</span> <span class="op">=</span> <span class="va">x</span> <span class="op">-</span> <span class="va">alpha</span> <span class="op">*</span> <span class="va">grad</span></span>
<span>  </span>
<span>  <span class="co"># Increment the number of iterations</span></span>
<span>  <span class="va">n_iter</span> <span class="op">=</span> <span class="va">n_iter</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>  </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">n_iter</span>,<span class="st">" "</span>, <span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; [1] "1 0.8"</span></span>
<span><span class="co">#&gt; [1] "2 1.28"</span></span>
<span><span class="co">#&gt; [1] "3 1.568"</span></span>
<span><span class="co">#&gt; [1] "4 1.7408"</span></span>
<span><span class="co">#&gt; [1] "5 1.84448"</span></span>
<span><span class="co">#&gt; [1] "6 1.906688"</span></span>
<span><span class="co">#&gt; [1] "7 1.9440128"</span></span>
<span><span class="co">#&gt; [1] "8 1.96640768"</span></span>
<span><span class="co">#&gt; [1] "9 1.979844608"</span></span>
<span></span>
<span><span class="co"># Print the final value of x</span></span>
<span><span class="co">#print(x)</span></span></code></pre></div>
</div>
<div id="exercise-4" class="section level2" number="13.4">
<h2>
<span class="header-section-number">13.4</span> Exercise<a class="anchor" aria-label="anchor" href="#exercise-4"><i class="fas fa-link"></i></a>
</h2>
<blockquote>
<p>I want you to play with a few of these parameters, change the initial value to 5 or -1, and see if you can get the results. Next, change the learning rate from 0.01 to 0.1 or 0.001. Examine if you converge to find the answer. Try changing the number of iterations as well. Explain your understanding.</p>
</blockquote>
<blockquote>
<p>How would you determine if you got the correct solution or not? For example, can you adjust the above code to exhibit a prompt that “The solution doesnot converge, please adjust either intialization or learning rate or iteration.”</p>
</blockquote>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="unconstrainted-optimization.html"><span class="header-section-number">12</span> Unconstrainted optimization</a></div>
<div class="next"><a href="gradient-descent-for-more-than-one-variable.html"><span class="header-section-number">14</span> Gradient descent for more than one variable</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#gradient-descent"><span class="header-section-number">13</span> Gradient descent</a></li>
<li><a class="nav-link" href="#intution"><span class="header-section-number">13.1</span> Intution</a></li>
<li><a class="nav-link" href="#gradient-descent-using-for-loop-in-r-codes"><span class="header-section-number">13.2</span> Gradient descent using “for loop” in R codes</a></li>
<li><a class="nav-link" href="#gradient-descent-using-while-in-r-codes"><span class="header-section-number">13.3</span> Gradient descent using “while” in R codes</a></li>
<li><a class="nav-link" href="#exercise-4"><span class="header-section-number">13.4</span> Exercise</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/rstudio/bookdown-demo/blob/master/12-gradient-descent.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/rstudio/bookdown-demo/edit/master/12-gradient-descent.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Mostly Handsdirty Metrics</strong>" was written by Shishir Shakya. It was last built on 2022-12-15.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
