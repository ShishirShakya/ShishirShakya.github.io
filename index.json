[{"authors":null,"categories":null,"content":"My tutorials comprises content such as:\n Project or software documentation Online courses Tutorials on several topics  ","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536465600,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/","section":"tutorial","summary":"My tutorials comprises content such as:\n Project or software documentation Online courses Tutorials on several topics  ","tags":null,"title":"Overview","type":"docs"},{"authors":["Shishir Shakya"],"categories":null,"content":"","date":1551416400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551416400,"objectID":"b1648a326bcbf3f13f967471169e3128","permalink":"/publication/2019-pdmp/","publishdate":"2019-03-01T00:00:00-05:00","relpermalink":"/publication/2019-pdmp/","section":"publication","summary":"The Prescription Drugs Monitoring Programs (PDMP) was introduced to reduce the opioid-related overdosage death. The relationship of PDMP and overall opioid-related overdosage death has been found to be negative as expected but statistically insignificant by several studies. I argue that PDMP program is successful to reduce the overdosage death related to legal opioids, however, after reintroduction of the abuse-deterrent formulation (ADF) of OxyContin in 2010, users of opioid medication might have switched to cheaper and illicit opioid options like heroin and synthetic fentanyl. The FDA support for ADF OxyContin led to evergreening ADF of OxyContin which imposes unnecessary cost on health insurance purchasing patients suffering from chronic pain which induce the demand for illicit opioid options like heroin. Even thou, the PDMP program was successful in its objective but the impact of ADF OxyContin cause the effects of PDMPs to be insignificant. I recommend that the FDA should stop promoting the ADF opioid.","tags":[],"title":"Impact of “Must Access” Prescription Drug Monitoring Program: A Generalize Synthetic Control Approach","type":"publication"},{"authors":null,"categories":["Non Parametric Econometrics"],"content":"\rUnivariate Density Estimation\rParametric Desity Estimation\rDraw from a normal distribution\rGiven \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) draw from a normal distribution with mean of \\(\\mu\\) and variance of \\({{\\sigma }^{2}}\\) the joint \\(PDF\\) can be expressed as: \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\prod\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}}\\] \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\] The term \\(\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}\\) is a constant multiplying this term for \\(n\\) times gives \\({{\\left( \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}} \\right)}^{n}}=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}\\). \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times {{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times {{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\] With the index law of addition i.e. we can add the indices for the same base \\[{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times {{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times {{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}={{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\left\\{ {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}+{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}+\\cdots +{{\\left( {{X}_{n}}-\\mu \\right)}^{2}} \\right\\}}}={{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\limits_{i=1}^{n}{{{\\left( Xi-\\mu \\right)}^{2}}}}}\\] Therefore, \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}}\\]\n\rThe log-likelihood function\rTaking the logarithm, we get the log-likelihood function as: \\[\\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\ln \\left[ \\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}} \\right]\\]\nWith the property of log i.e. multiplication inside the log can be turned into addition outside the log, and vice versa or \\(\\ln (ab)=\\ln (a)+\\ln (b)\\) \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln \\left( \\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}} \\right)+\\ln \\left( {{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}} \\right)\\] With natural log property i.e. when \\({{e}^{y}}=x\\) then \\(\\ln (x)=\\ln ({{e}^{y}})=y\\) \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln \\left( {{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{-\\frac{n}{2}}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\] \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln {{\\sigma }^{2}}-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\] \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}};\\mu ,{{\\sigma }^{2}} \\right)=-\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\n\rLogliklihood function optimization\rTo find the optimum value of \\(L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}};\\mu ,{{\\sigma }^{2}} \\right)=-\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\), we take the first order condition w.r.t. \\(\\mu\\) and \\({{\\sigma }^{2}}\\). The necessary first order condition w.r.t \\(\\mu\\) is given as: \\[\\frac{\\partial L\\left( \\mu ,{{\\sigma }^{2}} \\right)}{\\partial \\mu }=\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\] Here, \\(\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right) \\right\\}=0\\) and \\(\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right) \\right\\}=0\\), so we only need to solve for \\[\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\] \\[-\\frac{1}{2{{\\sigma }^{2}}}\\frac{\\partial }{\\partial \\mu }\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}=0\\] \\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ \\frac{\\partial }{\\partial \\mu }\\left\\{ {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}+{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}+\\cdots +{{\\left( {{X}_{n}}-\\mu \\right)}^{2}} \\right\\} \\right]=0\\] \\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ \\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\mu }+\\frac{\\partial {{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{\\partial \\mu }+\\cdots +\\frac{\\partial {{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{\\partial \\mu } \\right]=0\\] With the chain rule i.e \\(\\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\mu }=\\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\left( {{X}_{1}}-\\mu \\right)}\\frac{\\partial \\left( {{X}_{1}}-\\mu \\right)}{\\partial \\mu }=2\\left( {{X}_{1}}-\\mu \\right)\\left( 1 \\right)=2\\left( {{X}_{1}}-\\mu \\right)\\). Hence, \\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ 2\\left( {{X}_{1}}-\\mu \\right)+2\\left( {{X}_{2}}-\\mu \\right)+\\cdots +2\\left( {{X}_{n}}-\\mu \\right) \\right]=0\\] \\[-\\frac{1}{{{\\sigma }^{2}}}\\sum\\limits_{i=1}^{n}{\\left( {{X}_{i}}-\\mu \\right)}=0\\] Since \\({{\\sigma }^{2}}\\ne 0\\), So, \\[\\sum\\limits_{i=1}^{n}{\\left( {{X}_{i}}-\\mu \\right)}=0\\] \\[\\sum\\limits_{i=1}^{n}{{{X}_{i}}}-\\sum\\limits_{i=1}^{n}{\\mu }=0\\] \\[\\sum\\limits_{i=1}^{n}{{{X}_{i}}}-n\\mu =0\\] \\[\\hat{\\mu }={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{X}_{i}}}\\]\nThe necessary first order condition w.r.t \\({{\\sigma }^{2}}\\) is given as:\n\\[\\frac{\\partial L\\left( \\mu ,{{\\sigma }^{2}} \\right)}{\\partial {{\\sigma }^{2}}}=\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\n\\[-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ \\frac{n}{2}\\ln \\left( 2\\pi \\right) \\right\\}-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ -\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right) \\right\\}-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ \\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\n\\[0-\\frac{n}{2}\\frac{1}{{{\\sigma }^{2}}}-\\frac{1}{2}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\frac{\\partial {{\\left( {{\\sigma }^{2}} \\right)}^{-1}}}{\\partial {{\\sigma }^{2}}}=0\\]\n\\[-\\frac{1}{2}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\left( -1 \\right){{\\left( {{\\sigma }^{2}} \\right)}^{-2}}=\\frac{n}{2{{\\sigma }^{2}}}\\]\n\\[\\frac{1}{2{{\\left( {{\\sigma }^{2}} \\right)}^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}=\\frac{n}{2{{\\sigma }^{2}}}\\]\n\\[{{\\hat{\\sigma }}^{2}}={{n}^{-1}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\n\\(\\hat{\\mu }\\) and \\({{\\hat{\\sigma }}^{2}}\\) above are the maximum likelihood estimator of \\(\\mu\\) and \\({{\\sigma }^{2}}\\), respectively, the resulting estimator of \\(f\\left( x \\right)\\) is: \\[\\hat{f}\\left( x \\right)=\\frac{1}{\\sqrt{2\\pi {{{\\hat{\\sigma }}}^{2}}}}{{e}^{\\left[ -\\frac{1}{2}\\left( \\frac{x-\\hat{\\mu }}{{{{\\hat{\\sigma }}}^{2}}} \\right) \\right]}}\\]\n\rSimulation example\rLet’s simulate 10000 random observation from a normal distribution with mean of 2 and standard deviation of 1.5. To reproduce the results we will use set.seed() function.\nset.seed(1234)\rN \u0026lt;- 10000\rmu \u0026lt;- 2\rsigma \u0026lt;- 1.5\rx \u0026lt;- rnorm(n = N, mean = mu, sd = sigma)\rWe can use mean() and sd() function to find the mean and sigma\n# mean\rsum(x)/length(x)\r## [1] 2.009174\rmean(x)\r## [1] 2.009174\r# standard deviation\rsqrt(sum((x - mean(x))^2)/(length(x) - 1))\r## [1] 1.481294\rsd(x)\r## [1] 1.481294\rHowever, if can also simulate and try the optimization using the mle function from the stat 4 package in R.\nLL \u0026lt;- function(mu, sigma) {\rR \u0026lt;- dnorm(x, mu, sigma)\r-sum(log(R))\r}\rstats4::mle(LL, start = list(mu = 1, sigma = 1))\r## ## Call:\r## stats4::mle(minuslogl = LL, start = list(mu = 1, sigma = 1))\r## ## Coefficients:\r## mu sigma ## 2.009338 1.481157\rTo supress the warnings in R and garanatee the solution we can use following codes.\nstats4::mle(LL, start = list(mu = 1, sigma = 1), method = \u0026quot;L-BFGS-B\u0026quot;, lower = c(-Inf, 0), upper = c(Inf, Inf))\r## ## Call:\r## stats4::mle(minuslogl = LL, start = list(mu = 1, sigma = 1), ## method = \u0026quot;L-BFGS-B\u0026quot;, lower = c(-Inf, 0), upper = c(Inf, Inf))\r## ## Coefficients:\r## mu sigma ## 2.009174 1.481221\r\rDensity plot example\rGiven the data of x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59), we can estimate \\(\\hat{\\mu }\\) as sum(x)/length(x) or mean(x) and \\({{\\hat{\\sigma }}^{2}}\\) as sum((x-mean(x))^2)/(length(x)-1) or var(x). Note I use the sample variance formula.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.4, -1.05, 1, 0.37, -1.15, 0.73, 1.59)\r# mean\rsum(x)/length(x)\r## [1] 0.249\rmean(x)\r## [1] 0.249\r# variance\rsum((x - mean(x))^2)/(length(x) - 1)\r## [1] 0.9285211\rvar(x)\r## [1] 0.9285211\rWe can also plot a parametric density function. But prior we plot, we have to sort the data.\nx \u0026lt;- sort(x)\rplot(x, dnorm(x, mean = mean(x), sd = sd(x)), ylab = \u0026quot;Density\u0026quot;, type = \u0026quot;l\u0026quot;, col = \u0026quot;blue\u0026quot;, lwd = 3)\rLet’s also plot a graph of histogram using bin width ranging from -1.5 through 2.0.\nhist(x, breaks = seq(-1.5, 2, by = 0.5), prob = TRUE)\r\r\r\rUnivariate NonParametric Density Estimation\rSet-up\rConsider \\(i.i.d\\) data \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) with \\(F\\left( \\centerdot \\right)\\) an unknown \\(CDF\\) where \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) or \\(CDF\\) of \\(X\\) evaluated at \\(x\\). We can do a na?ve estimation as \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) as cumulative sums of relative frequency as:\n\\[{{F}_{n}}\\left( x \\right)={{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\le x \\right\\}\\] and \\(n\\to\\infty\\) yields \\({{F}_{n}}\\left( x \\right)\\to F\\left( x \\right)\\).\nThe \\(PDF\\) of \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) is given as \\(f\\left( x \\right)=\\frac{d}{dx}F\\left( x \\right)\\) and an obvious estimator is: \\[\\hat{f}\\left( x \\right)=\\frac{rise}{run}=\\frac{{{F}_{n}}\\left( x+h \\right)-{{F}_{n}}\\left( x-h \\right)}{x+h-\\left( x-h \\right)}=\\frac{{{F}_{n}}\\left( x+h \\right)-{{F}_{n}}\\left( x-h \\right)}{2h}={{n}^{-1}}\\frac{1}{2h}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}\\]\n\rNaive Kernel\rThe \\(k\\left( \\centerdot \\right)\\) can be any kernel function, If we define a uniform kernel function or also known as na?ve kernel function then \\[k\\left( z \\right)=\\left\\{ \\begin{matrix}\r{1}/{2}\\; \u0026amp; if\\ \\left| z \\right|\\le 1 \\\\\r0 \u0026amp; o.w \\\\\r\\end{matrix} \\right.\\] Where \\(\\left| {{z}_{i}} \\right|=\\left| \\frac{{{X}_{i}}-x}{h} \\right|\\) and therefore is symmetric and hence \\(\\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right]\\) means \\(2\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\). Then it is easy to see \\(\\hat{f}\\left( x \\right)\\) to be expressed as: \\[\\hat{f}\\left( x \\right)=\\frac{1}{2h}{{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}=\\frac{1}{2h}{{n}^{-1}}\\sum\\limits_{i=1}^{n}{k\\left( 2\\frac{{{X}_{i}}-x}{h} \\right)}=\\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)}\\]\nWe can use follwoing code for naive kernel.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.4, -1.05, 1, 0.37, -1.15, 0.73, 1.59)\rx \u0026lt;- sort(x)\rnaive_kernel \u0026lt;- function(x, y, h) {\rz \u0026lt;- (x - y)/h\rifelse(abs(z) \u0026lt;= 1, 1/2, 0)\r}\rnaive_density \u0026lt;- function(x, h) {\rval \u0026lt;- c()\rfor (i in 1:length(x)) {\rval[i] \u0026lt;- sum(naive_kernel(x, x[i], h)/(length(x) * h))\r}\rval\r}\rH \u0026lt;- c(0.5, 1, 1.5)\rnames \u0026lt;- as.vector(paste0(\u0026quot;H = \u0026quot;, H))\rdensity_data \u0026lt;- list()\rfor (i in 1:length(H)) {\rdensity_data[[i]] \u0026lt;- naive_density(x, H[i])\r}\rdensity_data \u0026lt;- do.call(cbind.data.frame, density_data)\rcolnames(density_data) \u0026lt;- names\rmatplot(density_data, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;x\u0026quot;, ylab = \u0026quot;Density\u0026quot;, pch = 1:length(H), col = 1:length(H), main = \u0026quot;Naive Density for various Smoothing Parameter H\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, legend = names, bty = \u0026quot;n\u0026quot;, pch = 1:length(H), col = 1:length(H))\r\rEpanechnikov kernel\rConsider another optimal kernel known as Epanechnikov kernel given by: \\[k\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left\\{ \\begin{matrix}\r\\frac{3}{4\\sqrt{5}}\\left( 1-\\frac{1}{5}{{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}^{2}} \\right) \u0026amp; if\\ \\left| \\frac{{{X}_{i}}-x}{h} \\right|\u0026lt;5 \\\\\r0 \u0026amp; o.w \\\\\r\\end{matrix} \\right.\\] Let’s use x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59) and compute the kernel estimator of the density function of every sample realization using bandwidth of \\(h=0.5\\), \\(h=1\\), \\(h=1.5\\), where, \\(h\\) is smoothing parameter restricted to lie in the range of \\((0,\\infty ]\\). We can use follwoing codes to estimate the density using Epanechnikov kernel.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.4, -1.05, 1, 0.37, -1.15, 0.73, 1.59)\rx \u0026lt;- sort(x)\repanichnikov_kernel \u0026lt;- function(x, y, h) {\rz \u0026lt;- (x - y)/h\rifelse(abs(z) \u0026lt; sqrt(5), (1 - z^2/5) * (3/(4 * sqrt(5))), 0)\r}\repanichnikov_density \u0026lt;- function(x, h) {\rval \u0026lt;- c()\rfor (i in 1:length(x)) {\rval[i] \u0026lt;- sum(epanichnikov_kernel(x, x[i], h)/(length(x) * h))\r}\rval\r}\rH \u0026lt;- c(0.5, 1, 1.5)\rnames \u0026lt;- as.vector(paste0(\u0026quot;H = \u0026quot;, H))\rdensity_data \u0026lt;- list()\rfor (i in 1:length(H)) {\rdensity_data[[i]] \u0026lt;- epanichnikov_density(x, H[i])\r}\rdensity_data \u0026lt;- do.call(cbind.data.frame, density_data)\rcolnames(density_data) \u0026lt;- names\rmatplot(density_data, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;x\u0026quot;, ylab = \u0026quot;Density\u0026quot;, pch = 1:length(H), col = 1:length(H), main = \u0026quot;Epanichnikov Density for various Smoothing Parameter H\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, legend = names, bty = \u0026quot;n\u0026quot;, pch = 1:length(H), col = 1:length(H))\r\rThree properties of kernel estimator\rFor any general nonnegative bounded kernel function \\(k\\left( v \\right)\\) where \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), the kernel estimator \\(\\hat{f}\\left( x \\right)\\) is a consistent estimator of \\(f\\left( x \\right)\\) that satisfies three conditions: First is area under a kernel to be unity. \\[\\int{k(v)dv=1}\\]\nSecond is the symmetry kernel \\[\\int{vk(v)dv=0}\\] which implies symmetry condition i.e. \\(k(v)=k(-v)\\). For asymmetric kernels see Abadir and Lawford (2004).\nThird is a positive constant. \\[\\int{{{v}^{2}}k(v)dv={{\\kappa }_{2}}\u0026gt;0}\\]\n\rThe big O and small o.\r\rTaylor series expansion\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) , we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots \\] For a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) that is \\(m\\) times differentiable, we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}\\] Wwhere \\({{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}\\) and and \\(\\xi\\) lies between \\(x\\) and \\({{x}_{0}}\\)\n\rMSE, variance and biases\rSay \\(\\hat{\\theta }\\) is an estimator for true \\(\\theta\\), then the Mean Squared Error \\(MSE\\) of an estimator \\(\\hat{\\theta }\\) is the mean of squared deviation between \\(\\hat{\\theta }\\) and \\(\\theta\\) and given as: \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-\\theta \\right)}^{2}} \\right] \\] \\[MSE=E\\left[ \\left( {{{\\hat{\\theta }}}^{2}}-2\\hat{\\theta }\\theta +{{\\theta }^{2}} \\right) \\right] \\] \\[MSE=E[{{\\hat{\\theta }}^{2}}]+E[{{\\theta }^{2}}]-2\\theta E[\\hat{\\theta }]\\] \\[MSE=\\underbrace{E[{{{\\hat{\\theta }}}^{2}}]-{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]}_{\\operatorname{var}(\\hat{\\theta })}+\\underbrace{{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]+E[{{\\theta }^{2}}]-2\\theta E[\\hat{\\theta }]}_{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+\\underbrace{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}_{squared\\ of\\ bias\\ of\\ \\hat{\\theta }}\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}\\]\nNote: \\(\\underbrace{E{{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}}_{\\operatorname{var}(\\hat{\\theta })}=\\underbrace{E[{{{\\hat{\\theta }}}^{2}}]-{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]}_{\\operatorname{var}(\\hat{\\theta })}\\).\nAnother way of solution is given as : \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-\\theta \\right)}^{2}} \\right] \\] \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-E[\\hat{\\theta }]+E[\\hat{\\theta }]-\\theta \\right)}^{2}} \\right]\\] \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}+{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}+2\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)\\left( E[\\hat{\\theta }]-\\theta \\right) \\right]\\] \\[MSE=\\underbrace{E{{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}}_{\\operatorname{var}(\\hat{\\theta })}+\\underbrace{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}_{squared\\ of\\ bias\\ of\\hat{\\theta }}+2E\\left[ \\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)\\left( E[\\hat{\\theta }]-\\theta \\right) \\right]\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}+2E\\left[ \\left( \\hat{\\theta }E[\\hat{\\theta }]+\\hat{\\theta }\\theta -E[\\hat{\\theta }]E[\\hat{\\theta }]-E[\\hat{\\theta }]\\theta \\right) \\right]\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}+2E\\left[ \\left( \\underbrace{\\hat{\\theta }E[\\hat{\\theta }]-E[\\hat{\\theta }]E[\\hat{\\theta }]}_{0}+\\underbrace{\\hat{\\theta }\\theta -E[\\hat{\\theta }]\\theta }_{0} \\right) \\right]\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}\\]\n\rTheorem 1.1.\rLet \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) observation having a three-times differentiable \\(PDF\\) \\(f(x)\\), and \\({{f}^{s}}(x)\\) denote the \\(s-th\\) order derivative of \\(f(x)\\ s=(1,2,3)\\). Let \\(x\\) be an interior point in the support of \\(X\\), and let \\(\\hat{f}(x)\\) be \\(\\frac{1}{2h}{{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}\\). Assume that the kernel function \\(k\\left( \\centerdot \\right)\\) bounded and satisfies: \\(\\int{k(v)dv=1}\\), \\(k(v)=k(-v)\\) and \\(\\int{{{v}^{2}}k(v)dv={{\\kappa }_{2}}\u0026gt;0}\\). And as \\(n\\to\\infty\\), \\(h\\to 0\\) and \\(nh\\to\\infty\\) then, the \\(MSE\\) of estimator \\(\\hat{f}(x)\\) is given as: \\[MSE\\left( \\hat{f}(x) \\right)=\\frac{{{h}^{4}}}{4}{{\\left[ {{\\kappa }_{2}}{{f}^{\\left( 2 \\right)}}(x) \\right]}^{2}}+\\frac{\\kappa f(x)}{nh}+o\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)=O\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)\\] Where, \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), \\({{\\kappa }_{2}}=\\int{{{v}^{2}}k(v)dv}\\) and \\(\\kappa =\\int{{{k}^{2}}(v)dv}\\)\n\rProof of Theorem 1.1\rMSE, variance and biases\rWe can express the relationship of MSE, variance and bias of estimator \\(MSE\\left( \\hat{f}(x) \\right)\\) as:\n\\[MSE\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( \\hat{f}(x) \\right)+{{\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}}^{2}}\\]\nThen we deal with \\(\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}\\) and \\(\\operatorname{var}\\left( \\hat{f}(x) \\right)\\) separately.\n\rBiases\rThe bias of \\(\\hat{f}(x)\\) is given as \\[bias\\left\\{ \\hat{f}(x) \\right\\}=E[\\hat{f}(x)]-f(x)\\]\n\\[bias\\left\\{ \\hat{f}(x) \\right\\}=E\\left[ \\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]-f(x)\\] By the identical distribution, we can write: \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{nh}nE\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}\\int{f({{x}_{1}})}k\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}-f(x)\\] Note: \\(\\frac{{{x}_{1}}-x}{h}=v\\); \\({{x}_{1}}-x=hv\\); \\({{x}_{1}}=x+hv\\); \\(\\frac{d{{x}_{1}}}{dv}=\\frac{d}{dv}\\left( x+hv \\right)=h\\) and \\(d{{x}_{1}}=hdv\\). \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}\\int{f(x+hv)}k\\left( v \\right)hdv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}h\\int{f(x+hv)}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{f(x+hv)}k\\left( v \\right)dv-f(x)\\] Let’s expand \\(f(x+hv)\\) with Taylor series expansion evaluated at \\(x\\). Since \\(f(x)\\) is only three times differentiable: \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)(x+hv-x)+\\frac{1}{2!}{{f}^{(2)}}(x){{(x+hv-x)}^{2}}+\\frac{1}{3!}{{f}^{(3)}}(\\tilde{x}){{(x+hv-x)}^{3}} \\right\\}}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)hv+\\frac{1}{2!}{{f}^{(2)}}(x){{h}^{2}}{{v}^{2}}+\\frac{1}{3!}{{f}^{(3)}}(\\tilde{x}){{h}^{3}}{{v}^{3}} \\right\\}}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)hv+\\frac{1}{2!}{{f}^{(2)}}(x){{h}^{2}}{{v}^{2}}+O({{h}^{3}}) \\right\\}}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)\\int{k\\left( v \\right)dv}+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x)\\int{{{v}^{2}}}k\\left( v \\right)dv+\\int{O({{h}^{3}})k\\left( v \\right)dv}-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)\\underbrace{\\int{k\\left( v \\right)dv}}_{1}+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x)\\underbrace{\\int{{{v}^{2}}}k\\left( v \\right)dv}_{{{\\kappa }_{2}}}+\\underbrace{\\int{O({{h}^{3}})k\\left( v \\right)dv}}_{O({{h}^{3}})}-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] By symmetrical definition \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\left[ \\left\\{ \\int\\limits_{-\\infty }^{0}{+\\int\\limits_{0}^{\\infty }{{}}} \\right\\}vk\\left( v \\right)dv \\right]+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\left[ \\left\\{ \\int\\limits_{-\\infty }^{0}{vk\\left( v \\right)dv+\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv}} \\right\\} \\right]+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] Then, we can switch the bound of definite integrals. Click for tutorial. \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\underbrace{\\left[ \\left\\{ -\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv+\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv}} \\right\\} \\right]}_{0}+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+o({{h}^{2}})\\]\n\rVariance\rThe variance of \\(\\hat{f}(x)\\) is given as:\n\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=E{{\\left( \\hat{f}(x)-E\\left[ \\hat{f}(x) \\right] \\right)}^{2}}=E\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]-{{E}^{2}}\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]\\]\n\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\operatorname{var}\\left[ \\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(b\\in \\mathbb{R}\\) be a constant and \\(y\\) be a random variable, then, \\(\\operatorname{var}[by]={{b}^{2}}\\operatorname{var}[y]\\), therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}\\operatorname{var}\\left[ \\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(\\operatorname{var}(a+b)=\\operatorname{var}(a)+\\operatorname{var}(b)+2\\operatorname{cov}(a,b)\\) and if \\(a\\bot b\\) then \\(2\\operatorname{cov}(a,b)=0\\). In above expression \\({{X}_{i}}\\) are independent observation therefore \\({{X}_{i}}\\bot {{X}_{j}}\\ \\forall \\ i\\ne j\\). Hence, we can express \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}\\left\\{ \\sum\\limits_{i=1}^{n}{\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]}+0 \\right\\}\\] Note, \\({{X}_{i}}\\) are also identical, therefore \\(\\operatorname{var}({{X}_{i}})=\\operatorname{var}({{X}_{j}})\\) so, \\(\\sum\\limits_{i=1}^{n}{\\operatorname{var}({{X}_{i}})}=n\\operatorname{var}({{X}_{1}})\\). Therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}n\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ E\\left[ {{k}^{2}}\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]-\\left[ E{{\\left( k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right)}^{2}} \\right] \\right\\}\\] Which is equivalent to: \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f({{x}_{1}}){{k}^{2}}\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}}-{{\\left[ \\int{f({{x}_{1}})k\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}} \\right]}^{2}} \\right\\}\\] Note: \\(\\frac{{{x}_{1}}-x}{h}=v\\); \\({{x}_{1}}-x=hv\\); \\({{x}_{1}}=x+hv\\); \\(\\frac{d{{x}_{1}}}{dv}=\\frac{d}{dv}\\left( x+hv \\right)=h\\) and \\(d{{x}_{1}}=hdv\\). \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-{{\\left[ \\int{f(x+hv)k\\left( v \\right)hdv} \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x+hv){{k}^{2}}\\left( v \\right)dv}-{{\\left[ h\\int{f(x+hv)k\\left( v \\right)dv} \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-\\underbrace{{{\\left[ \\int{f(x+hv)k\\left( v \\right)hdv} \\right]}^{2}}}_{O\\left( {{h}^{2}} \\right)} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-O\\left( {{h}^{2}} \\right) \\right\\}\\] Taylor series expansion \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x)+{{f}^{(1)}}(\\xi )(x+hv-x){{k}^{2}}\\left( v \\right)dv}-O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x){{k}^{2}}\\left( v \\right)dv}+\\int{{{f}^{(1)}}(\\xi )(hv){{k}^{2}}\\left( v \\right)dv}-O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ hf(x)\\underbrace{\\int{{{k}^{2}}\\left( v \\right)dv}}_{\\kappa }+\\underbrace{\\int{{{f}^{(1)}}(\\xi )(hv){{k}^{2}}\\left( v \\right)dv}}_{O\\left( {{h}^{2}} \\right)}-O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\kappa f(x)+O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{nh}\\left\\{ \\kappa f(x)+O\\left( h \\right) \\right\\}=O\\left( {{(nh)}^{-1}} \\right)\\]\nWe now know that the order of variance is \\(O\\left( {{(nh)}^{-1}} \\right)\\), the order of bias is \\(O\\left( {{h}^{2}} \\right)\\) and the order of biases square is \\(O\\left( {{h}^{4}} \\right)\\). As we know the the MSE is sum of variance and square of biases and pluggin the values of variance and bias, we get,\n\\[MSE\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( \\hat{f}(x) \\right)+{{\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}}^{2}}\\]\n\\[MSE\\left( \\hat{f}(x) \\right)=\\frac{{{h}^{4}}}{4}{{\\left[ {{\\kappa }_{2}}{{f}^{\\left( 2 \\right)}}(x) \\right]}^{2}}+\\frac{\\kappa f(x)}{nh}+o\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)=O\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)\\] Where, \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), \\({{\\kappa }_{2}}=\\int{{{v}^{2}}k(v)dv}\\) and \\(\\kappa =\\int{{{k}^{2}}(v)dv}\\)\n\r\r\rMultivariate NonParametric Density Estimation\rTheorem\rSuppose that \\({{X}_{1}},...,{{X}_{n}}\\) constitute an i.i.d \\(q\\)-vector \\(\\left( {{X}_{i}}\\in {{\\mathbb{R}}^{q}} \\right)\\) for some \\(q\u0026gt;1\\) having common PDF \\(f(x)=f({{x}_{1}},{{x}_{2}},...,{{x}_{q}})\\). Let \\({{X}_{ij}}\\) denote the \\(j-th\\) component of \\({{X}_{i}}(j=1,...,q)\\). Then the estimated pdf given by \\(\\hat{f}(x)\\)is constructed by product kernel function or the product of univariate kernel functions.\n\\[\\hat{f}(x)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{x}_{i1}}-{{x}_{1}}}{{{h}_{1}}} \\right)\\times k\\left( \\frac{{{x}_{i2}}-{{x}_{2}}}{{{h}_{2}}} \\right)}\\times \\cdots \\times k\\left( \\frac{{{x}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)\\] \\[\\hat{f}(x)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}}\\]\n\rBias term\rThe MSE consistency of \\(\\hat{f}(x)\\)is sum of variance and square of bias term. First, we define bias given as: \\[bias\\left( \\hat{f}(x) \\right)=E\\left( \\hat{f}(x) \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}} \\right)-f(x)\\] Lets define \\(\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{X}_{ij}}-{{x}_{i}}}{{{h}_{j}}} \\right)}=K\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\) \\[bias\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}E\\left( \\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}n\\left( E\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left( E\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)f\\left( {{X}_{i}} \\right)d{{x}_{i}}}-f(x)\\] Note: \\(d{{x}_{i}}\\) is vector comprise of \\(d{{x}_{i1}},d{{x}_{i2}},...,d{{x}_{iq}}\\) and let’s \\(\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left( \\frac{{{X}_{i1}}-{{x}_{1}}}{{{h}_{1}}},\\frac{{{X}_{i1}}-{{x}_{2}}}{{{h}_{2}}},...,\\frac{{{X}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)=\\psi =\\left( {{\\psi }_{1}},{{\\psi }_{2}},...,{{\\psi }_{q}} \\right)\\). This means, \\(\\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}}={{\\psi }_{j}}\\) and \\({{x}_{ij}}={{x}_{j}}+{{\\psi }_{j}}{{h}_{j}}\\) then \\(d{{x}_{ij}}={{h}_{j}}d{{\\psi }_{j}}\\) \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\underbrace{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}_{\\psi }\\underbrace{f\\left( {{X}_{i}} \\right)}_{f\\left( x+h\\psi \\right)}\\underbrace{d{{x}_{i}}}_{{{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }}-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)=({{h}_{1}}{{h}_{2}}...{{h}_{q}}){{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-f(x)\\] Now perform a multivariate Taylor expansion for: \\[f\\left( x+h\\psi \\right)=\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( x+h\\psi -x \\right)+\\frac{1}{2!}{{(x+h\\psi -x)}^{T}}{{f}^{2}}(x)\\left( x+h\\psi -x \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( x+h\\psi -x \\right)}^{3}}} \\right\\}\\] \\[f\\left( x+h\\psi \\right)=\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)+\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}} \\right\\}\\] \\[bias\\left( \\hat{f}(x) \\right)=\\int{K\\left( \\psi \\right)\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)+\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}} \\right\\}d\\psi }-f(x)\\] \\[\\begin{matrix}\rbias\\left( \\hat{f}(x) \\right)= \u0026amp; \\int{f(x)K\\left( \\psi \\right)d\\psi }+\\int{{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi }+\\int{\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi } \\\\\r{} \u0026amp; +\\int{\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}K\\left( \\psi \\right)d\\psi }-f(x)} \\\\\r\\end{matrix}\\] \\[bias\\left( \\hat{f}(x) \\right)=f\\left( x \\right)\\int{K\\left( \\psi \\right)d\\psi }+{{f}^{1}}{{(x)}^{T}}h\\int{\\psi K\\left( \\psi \\right)d\\psi }+\\int{\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi +O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)-f\\left( x \\right)}\\] \\[bias\\left( \\hat{f}(x) \\right)=f\\left( x \\right)\\underbrace{\\int{K\\left( \\psi \\right)d\\psi }}_{1}+{{f}^{1}}{{(x)}^{T}}h\\underbrace{\\int{\\psi K\\left( \\psi \\right)d\\psi }}_{0}+\\frac{{{h}^{T}}h}{2}\\int{K\\left( \\psi \\right){{\\psi }^{T}}{{f}^{2}}(x)\\psi d\\psi +O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)-f\\left( x \\right)}\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{{{h}^{T}}h{{\\psi }^{T}}{{f}^{2}}(x)\\psi K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{\\underbrace{{{h}^{T}}h\\underbrace{{{\\psi }^{T}}}_{1\\times q}\\underbrace{{{f}^{2}}(x)}_{q\\times q}\\underbrace{\\psi }_{q\\times 1}}_{1\\times 1}K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{{{h}^{T}}h\\sum\\limits_{l=1}^{q}{\\sum\\limits_{m=1}^{q}{f_{lm}^{\\left( 2 \\right)}\\left( x \\right){{\\psi }_{l}}{{\\psi }_{m}}}}K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] For \\(l\\ne m\\) the cross derivatives will be zero hence, we can re-write as: \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right){{\\kappa }_{2}}}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right)}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)=O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\underbrace{\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right)}}_{{{c}_{1}}}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)=O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)\\]\nWhere, \\({{c}_{1}}\\) is a constant.\n\rVariance term\rThe variance is given as: \\[\\operatorname{var}\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}} \\right)\\] Lets’s define \\(\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{X}_{ij}}-{{x}_{i}}}{{{h}_{j}}} \\right)}=K\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\) \\[\\operatorname{var}\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)\\] For \\(b\\in \\mathbb{R}\\) be a constant and \\(y\\) be a random variable, then, \\(\\operatorname{var}[by]={{b}^{2}}\\operatorname{var}[y]\\), therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}\\operatorname{var}\\left[ \\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(\\operatorname{var}(a+b)=\\operatorname{var}(a)+\\operatorname{var}(b)+2\\operatorname{cov}(a,b)\\) and if \\(a\\bot b\\) then \\(2\\operatorname{cov}(a,b)=0\\). In above expression \\({{X}_{i}}\\) are independent observation therefore \\({{X}_{i}}\\bot {{X}_{j}}\\ \\forall \\ i\\ne j\\). Hence, we can express \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}\\left\\{ \\sum\\limits_{i=1}^{n}{\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]}+0 \\right\\}\\] Note, \\({{X}_{i}}\\) are also identical, therefore \\(\\operatorname{var}({{X}_{i}})=\\operatorname{var}({{X}_{j}})\\) so, \\(\\sum\\limits_{i=1}^{n}{\\operatorname{var}({{X}_{i}})}=n\\operatorname{var}({{X}_{1}})\\). Therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}n\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]\\] The variance is given as: \\(\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=E{{\\left( \\hat{f}(x)-E\\left[ \\hat{f}(x) \\right] \\right)}^{2}}=E\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]-{{E}^{2}}\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]\\) \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ E{{\\left[ K\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]}^{2}}-\\left[ E{{\\left( K\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right)}^{2}} \\right] \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{f\\left( {{X}_{i}} \\right)\\left[ {{K}^{2}}\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}}-{{\\left[ \\int{f\\left( {{X}_{i}} \\right)\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}} \\right]}^{2}}\\] Note: \\(d{{x}_{i}}\\) is vector comprise of \\(d{{x}_{i1}},d{{x}_{i2}},...,d{{x}_{iq}}\\) and let’s \\(\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left( \\frac{{{X}_{i1}}-{{x}_{1}}}{{{h}_{1}}},\\frac{{{X}_{i1}}-{{x}_{2}}}{{{h}_{2}}},...,\\frac{{{X}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)=\\psi =\\left( {{\\psi }_{1}},{{\\psi }_{2}},...,{{\\psi }_{q}} \\right)\\). This means, \\(\\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}}={{\\psi }_{j}}\\) and \\({{x}_{ij}}={{x}_{j}}+{{\\psi }_{j}}{{h}_{j}}\\) then \\(d{{x}_{ij}}={{h}_{j}}d{{\\psi }_{j}}\\) \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{\\left[ {{K}^{2}}\\underbrace{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}_{\\psi } \\right]\\underbrace{f\\left( {{X}_{i}} \\right)}_{f\\left( x+h\\psi \\right)}\\underbrace{d{{x}_{i}}}_{{{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }}-{{\\left[ \\int{f\\left( {{X}_{i}} \\right)\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}} \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-{{\\left[ \\int{f\\left( x+h\\psi \\right)K\\left( \\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi } \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-\\underbrace{{{\\left[ {{({{h}_{1}}{{h}_{2}}...{{h}_{q}})}^{2}}\\int{f\\left( x+h\\psi \\right)K\\left( \\psi \\right)d\\psi } \\right]}^{2}}}_{O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] Now, we perform the Taylor series expansion \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{f\\left( x \\right){{K}^{2}}\\left( \\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }+\\int{{{f}^{(1)}}\\left( \\xi \\right)\\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\psi \\right){{K}^{2}}\\left( \\psi \\right)d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right)\\underbrace{\\int{{{K}^{2}}\\left( \\psi \\right)d\\psi }}_{\\mathbf{\\kappa }}+\\underbrace{\\int{{{f}^{(1)}}\\left( \\xi \\right)\\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\psi \\right){{K}^{2}}\\left( \\psi \\right)d\\psi }}_{O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)}-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right)\\mathbf{\\kappa }+O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right){{\\kappa }^{q}}+O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}=O\\left( \\frac{1}{n{{h}_{1}}...{{h}_{q}}} \\right)\\]\n\rMSE term\rSummarizing, we obtain the MSE as: \\[MSE\\left( \\hat{f}\\left( x \\right) \\right)=\\operatorname{var}\\left( \\hat{f}\\left( x \\right) \\right)+{{\\left[ bias\\left( \\hat{f}\\left( x \\right) \\right) \\right]}^{2}}\\] \\[MSE\\left( \\hat{f}\\left( x \\right) \\right)=O\\left( \\frac{1}{n{{h}_{1}}...{{h}_{q}}} \\right)+{{\\left[ O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right]}^{2}}=O\\left( {{\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)}^{2}}+{{\\left( n{{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)}^{-1}} \\right)\\]\nHence, if as \\(n\\to \\infty\\), \\({{\\max }_{1\\le l\\le q}}{{h}_{l}}\\to 0\\) and \\(n{{h}_{1}}...{{h}_{q}}\\to \\infty\\) then we have \\(\\hat{f}\\left( x \\right)\\to f\\left( x \\right)\\) in MSE, which \\(\\hat{f}\\left( x \\right)\\to f\\left( x \\right)\\) in probability.\n\rThe optimal band-width\r\\[MSE\\left( \\hat{f}\\left( x \\right) \\right)={{c}_{1}}{{h}^{4}}+{{c}_{2}}{{n}^{-1}}{{h}^{-q}}\\]\nNow, we can choose \\(h\\) to find the optimal value of above function and the \\(F.O.C\\) is given as:\n\\[\\frac{\\partial MSE\\left( \\hat{f}\\left( x \\right) \\right)}{\\partial h}=\\frac{{{c}_{1}}{{h}^{4}}+{{c}_{2}}{{n}^{-1}}{{h}^{-q}}}{\\partial h}=0\\]\n\\[4{{c}_{1}}{{h}^{3}}+{{c}_{2}}{{n}^{-1}}(-q){{h}^{-q-1}}=0\\]\n\\[\\frac{4{{c}_{1}}}{{{c}_{2}}q}n={{h}^{-q-1-3}}\\]\nLet’s define \\(\\frac{4{{c}_{1}}}{{{c}_{2}}q}=c\\) then\n\\[{{h}^{-q-4}}=cn\\]\n\\[{{h}^{*}}={{\\left[ cn \\right]}^{-\\frac{1}{4+q}}}\\]\n\r\rAsymptotic Normality of Density Estimators\rTheorem 1.2\nLet \\({{X}_{1}},...,{{X}_{n}}\\) be \\(i.i.d\\) \\(q-\\)vector with it’s \\(PDF\\) \\(f\\left( \\cdot \\right)\\) having three-times bounded continuous derivatives. Let \\(x\\) be an interior point of the support \\(X\\). If, as \\(n\\to \\infty\\), \\({{h}_{s}}\\to 0\\) for all \\(s=1,...,q\\), \\(n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}}\\to \\infty\\) and \\(\\left( n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}} \\right)\\sum\\nolimits_{s=1}^{q}{h_{s}^{6}}\\to 0\\), then, \\[\\sqrt{n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}}}\\left[ \\hat{f}\\left( x \\right)-f\\left( x \\right)-\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{s=1}^{q}{h_{s}^{2}{{f}_{ss}}\\left( x \\right)} \\right]\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,{{\\kappa }^{q}}f\\left( x \\right) \\right)\\]\nBefore, we begin, imagine that each band width is equal i.e. \\({{h}_{1}}={{h}_{2}}=...={{h}_{q}}\\). Let’s acknowledge that as \\(n\\to \\infty\\), \\(h\\to 0\\). If this is the case then, where does \\(n{{h}^{q}}\\) go? Does this go to zero or infinity? \\({{h}^{q}}\\) doesn’t converge to zero as fast as \\(n\\to \\infty\\), then \\(n{{h}^{q}}\\to \\infty\\) but \\(n{{h}^{6+q}}\\to 0\\). Therefore, because of this reason we can set up asymptotic normality.\nLet begin with following equation: \\[\\sqrt{n{{h}^{q}}}\\left[ \\hat{f}\\left( x \\right)-E\\left[ \\hat{f}\\left( x \\right) \\right] \\right]\\]\nThis can be re-expressed as: \\[\\sqrt{n{{h}^{q}}}\\frac{1}{n{{h}^{q}}}\\sum\\limits_{i=1}^{n}{\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right)-E\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right]}\\] Let’s define \\[k\\left( \\frac{{{X}_{i}}-x}{h} \\right)={{K}_{i}}\\] then, \\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}=\\sum\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{n{{h}^{q}}}}\\left( {{K}_{i}}-E{{K}_{i}} \\right)}=\\sum\\limits_{i=1}^{n}{{{Z}_{n,i}}}\\] Note that the \\({{Z}_{n,i}}\\) is double array (or data frame, column indexed with different \\(n\\) and row with different observation \\(i\\), each combination of \\(n\\) and \\(i\\) gives different value for \\({{Z}_{n,i}}\\))\nNow, we need to think for a strategy for asymptotic normality. Actually, there are four ways (at least) to think about the asymptotic normality: 1) Khinchi’s Law of Large Numbers; 2) Lindeberg-Levy Central Limit Theorem; 3) Lindelberg-Feller Central Limit Theorem and 4) Lyapunov Central Limit Theorem. We will use Lyapunov Central Limit Theorem, for brief introduction to these laws of large numbers and central limit theorem, please see the annex.\nWe will consider following four conditions:\n\\(E\\left[ {{Z}_{ni}} \\right]=\\frac{1}{\\sqrt{n{{h}^{q}}}}\\left[ E{{K}_{i}}-E{{K}_{i}} \\right]=0\\)\n\r\\(Var\\left( {{Z}_{n,i}} \\right)=\\frac{1}{n{{h}^{q}}}Var\\left( {{K}_{i}} \\right)=\\frac{1}{n{{h}^{q}}}\\left[ EK_{i}^{2}-{{\\left( E{{K}_{i}} \\right)}^{2}} \\right]={{h}^{q}}\\frac{1}{n{{h}^{q}}}Var\\left( {{K}_{i}} \\right)={{h}^{q}}\\frac{{{\\kappa }^{q}}f\\left( x \\right)}{n{{h}^{q}}}\\left( 1+o\\left( 1 \\right) \\right)={{\\kappa }^{q}}f\\left( x \\right)\\left( 1+o\\left( 1 \\right) \\right)\\)\n\r\\(\\sigma _{n}^{2}=\\sum\\limits_{i=1}^{n}{Var\\left( {{Z}_{n,i}} \\right)}={{\\kappa }^{q}}f\\left( x \\right)\\left( 1+o\\left( 1 \\right) \\right)\\) and taking limits we get \\(\\underset{n\\to \\infty }{\\mathop{\\lim }}\\,\\sigma _{n}^{2}={{\\kappa }^{q}}f\\left( x \\right)\\)\r\\(\\sum\\limits_{i=1}^{n}{E{{\\left| {{Z}_{n,i}} \\right|}^{2+\\delta }}}=nE{{\\left| {{Z}_{n,i}} \\right|}^{2+\\delta }}\\) if each \\(i\\) are i.i.d With Cram?r-Rao bound inequality we can write \\[n\\frac{1}{{{\\left| \\sqrt{n{{h}^{q}}} \\right|}^{2+\\delta }}}E{{\\left| {{K}_{i}}-E{{K}_{i}} \\right|}^{2+\\delta }}\\le \\frac{c}{{{n}^{{\\delta }/{2}\\;}}{{h}^{{\\delta }/{2}\\;}}}\\frac{1}{{{h}^{q}}}\\left[ E{{\\left| {{K}_{i}} \\right|}^{2+\\delta }}+{{\\left| E{{K}_{i}} \\right|}^{2+\\delta }} \\right]\\to 0\\] Note that \\(n{{h}^{q}}\\to \\infty\\) so \\(\\left( \\bullet \\right)\\to 0\\), further \\(E{{\\left| {{K}_{i}} \\right|}^{2+\\delta }}\\) is higher order than \\({{\\left| E{{K}_{i}} \\right|}^{2+\\delta }}\\) With all these conditions: \\[\\sigma _{n}^{-2}\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,1 \\right)\\] \\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,\\underset{n\\to \\infty }{\\mathop{\\lim }}\\,\\sigma _{n}^{2} \\right)\\] \\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,{{\\kappa }^{q}}f\\left( x \\right) \\right)\\]\n\r\r\r","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"5df485a513dc3cfcae192c0b399ebd42","permalink":"/post/2019-01-07-density-estimation/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/2019-01-07-density-estimation/","section":"post","summary":"Univariate Density Estimation\rParametric Desity Estimation\rDraw from a normal distribution\rGiven \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) draw from a normal distribution with mean of \\(\\mu\\) and variance of \\({{\\sigma }^{2}}\\) the joint \\(PDF\\) can be expressed as: \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\prod\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}}\\] \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\] The term \\(\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}\\) is a constant multiplying this term for \\(n\\) times gives \\({{\\left( \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}} \\right)}^{n}}=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}\\).","tags":["Density","R","Non Parametric","Theory","Proof"],"title":"Density Estimation","type":"post"},{"authors":["Shishir Shakya",""],"categories":null,"content":"","date":1546318800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546318800,"objectID":"eb16dbaf3e2a1c4f9e4ffb4da9c66c89","permalink":"/publication/2019-hte/","publishdate":"2019-01-01T00:00:00-05:00","relpermalink":"/publication/2019-hte/","section":"publication","summary":"The purpose of any field experiment is to learn treatment assignment policies or to map individual characteristics to treatment assignments. However, field experiments are expensive, and policy can affect different subpopulation differently. Understanding such heterogeneous treatment effect is important for the policymaker to design targeted policy. Using the Oregon health insurance experiment data, this paper at first estimates the heterogeneous treatment effects of random insurance assignment among uninsured low-income Oregonian adults. Then, this paper proposes an insurance assignment policy to maximize the overall self-reported health of those adults. This paper finds the impact of insurance among those adults are heterogeneous and suggests if the insurance had been assigned to the subpopulation with no living arrangements and subpopulation who are paying out-of-pocket for prescription medication would have had yielded the highest overall health outcomes. Heterogeneous treatment effects are estimated using Causal Tree method of Athey \u0026 Imbens (2015) which is the classification and regression tree (CART)– a machine learning algorithm redesigned for causal inference within Rubin’s potential outcome framework. The policy learning is based upon the strategy proposed by Athey \u0026 Wager (2017), which estimates the average treatment effect by a double machine learning technique and seeks to learn policy or (to whom to target with the intervention) by maximizing the average utility (in this paper health outcomes).","tags":[],"title":"Heterogeneous Treatment Effect and Optimal Policy Learning: Oregon Health Insurance Program","type":"publication"},{"authors":["Shishir Shakya"],"categories":null,"content":"","date":1546318800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546318800,"objectID":"2ed948d9c86d9f3bfba9ce906dccc5f2","permalink":"/publication/2019-shalecrime/","publishdate":"2019-01-01T00:00:00-05:00","relpermalink":"/publication/2019-shalecrime/","section":"publication","summary":"This paper examines resource curse channel of crime in the states of Montana, North Dakota, and West Virginia. These states are economically-small, relatively more rural, and have high levels of shale oil and gas reserves. By merging state level data from several sources, I develop a high dimensional panel data of potential variables that could explain the criminal activities. My identification strategy exploits a natural experiment of fracking boom and uses double post LASSO  method as suggested by (Belloni, Chernozhukov, \u0026 Hansen, 2013) for proper selection on observables confounders and implements  Xu (2017) Generalize Synthetic Control Method to absorb the unobserved time-varying confounding effect to maintain the “parallel trend” assumption.","tags":[],"title":"Shale Boom Bears Bitter Crime among Economically-small, Relatively Rural American States","type":"publication"},{"authors":null,"categories":["R"],"content":"\rReplication can have a different meaning to the different discipline. Replication in studies that uses public data sources involve sharing codes to ensure consistency in the results. Here, I share some chunk of the codes in five steps using RStudio, that can ensure ease in shareability. These 5 steps are the preambles prior I proceed toward data management.\nFor this a user will require to install R here and R-Studio here. A copy of this code is available here.\nClean the current workspace and session.\rThe very first step in to remove all the objects and plots from the current workspace and device in the R. For this, I use following codes.\nrm(list = ls())\rdev.off(dev.list()[\u0026quot;RStudioGD\u0026quot;])\r\rInstall and load R packages\rSecond step is to install the required R-packages (if r-packages don’t exist installed package directory of R) and load them into the library. In the following code, I first create a function called load_packages(). This function takes list of names of packages as argument, then check if the packages are already installed in the user’s package list, and if not then installs the packages from the cran. I always use load_packages(\u0026quot;rstudioapi\u0026quot;) package and it’s a mandatory package for mywork flow. Then I use load_packages() for other required packages. These packages depends upon the project. In the following example I use haven, and Hmisc packages.\nload_packages \u0026lt;- function(pkg){\rnew.pkg \u0026lt;- pkg[!(pkg %in% installed.packages()[, \u0026quot;Package\u0026quot;])]\rif (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE,\rrepos = \u0026quot;http://cran.us.r-project.org\u0026quot;)\rsapply(pkg, require, character.only = TRUE)\r}\rload_packages(\u0026quot;rstudioapi\u0026quot;) # This is a mandatory package.\rload_packages (c(\u0026quot;devtools\u0026quot;, \u0026quot;haven\u0026quot;, \u0026quot;Hmisc\u0026quot;)) # These packages depends upon the project.\r\rSetup Working directory.\rSetting up the working directory is my third step. It is very important because my working directory can always be different then my coauthor’s/referee’s working directory. So, I first save the R script in the desired location manually and my coauthors are also advised to do similar. Then I use following code. This code detects where R-script is saved and sets that as working directory. Note this chunk of code uses rstudioapi package, therefore it’s a mandatory package for my workflow.\npath \u0026lt;- dirname(rstudioapi::getActiveDocumentContext()$path)\rsetwd(path)\r\rThe Folders\rI never right click and create new folders. I always code to create a folder. The code helps me to track what I did in logical manner. Following codes directly create the folders in the working directory. I usually like to have folder called rawdata to dump the data downloaded from the internet then I also like another folder outcomes to save my final dataset for analysis, plots and tables. Sometimes, I can create folders within folder like rawdata/beadataset.\ndir.create(file.path(path, \u0026quot;rawdata\u0026quot;))\rdir.create(file.path(path, \u0026quot;outcomes\u0026quot;))\rdir.create(file.path(path, \u0026quot;rawdata/beadataset\u0026quot;))\r\rGetting data.\rNever download the data manually. If possible, always provide a download link and use script to download the data. And never touch the data. It seems counter-intuitive but, I never open data in excel. If I open data in excel, I make sure I don’t save or If I have to play around with data, I do that in separate folder and delete them ASAP.\nConsider following example, the data of GDP by State by Year is available from the Bureau of Economic Analysis website. These data have stable link (the link doesn’t change over time) and content of data are consistent. The data can be download in the .zip format. Again, I write script to unzip the data. The script checks if folder called rawdata has gdpstate_naics_all_C.zip file or not. If there exist no file, the script will download the file.\ngdpbystatebyyear \u0026lt;- \u0026quot;https://www.bea.gov/regional/zip/gdpstate/gdpstate_naics_all_C.zip\u0026quot;\rif (file.exists(\u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;) == FALSE) { # get the zip file\rdownload.file(gdpbystatebyyear,\rdestfile = \u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;, mode=\u0026quot;wb\u0026quot;)\r}\runzip(\u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;, exdir = paste0(path,\u0026quot;/rawdata/beadataset\u0026quot;))\rConsider another example, if data is not available from in the web, I can share them via my google drive. I upload the zip file in google drive then get the public shareable link. The object gdrivepublic comprises of the public shareable link. Like previous chunk of code, here I check if data exist or not then download.\ngdrivepublic \u0026lt;- \u0026quot;https://drive.google.com/uc?authuser=0\u0026amp;id=1AiZda_1-2nwrxI8fLD0Y6e5rTg7aocv0\u0026amp;export=download\u0026quot;\rif (file.exists(\u0026quot;datafromGoogleDrive.zip\u0026quot;) == FALSE) { # get the zip file\rdownload.file(gsub(\u0026quot;open\\\\?\u0026quot;, \u0026quot;uc\\\\?export=download\\\\\u0026amp;\u0026quot;, gdrivepublic), destfile = \u0026quot;datafromGoogleDrive.zip\u0026quot;, mode=\u0026quot;wb\u0026quot;)\r}\r\r","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"ecb40a5a1bcfd5aecaaf9e7a172ac4f7","permalink":"/post/2018-12-26-preamble-of-reproducible-research/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/2018-12-26-preamble-of-reproducible-research/","section":"post","summary":"Replication can have a different meaning to the different discipline. Replication in studies that uses public data sources involve sharing codes to ensure consistency in the results. Here, I share some chunk of the codes in five steps using RStudio, that can ensure ease in shareability. These 5 steps are the preambles prior I proceed toward data management.\nFor this a user will require to install R here and R-Studio here.","tags":["Reproducible","R"],"title":"Preambles for Reproducible Research","type":"post"},{"authors":null,"categories":["Theoritical Statistics"],"content":"\rTheorem 1: Gaussian Tail Inequality\rGiven \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) then, \\(P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\) and \\(P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\).\nProof of Gaussian Tail Inequality\rConsider a univariate \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\), then the probability density function is given as \\(\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\). Let’s take the derivative w.r.t \\(x\\) we get: \\[\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026#39;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)\\] Let’s define the gaussian tail inequality. \\[P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}\\] \\[P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{{\\phi }\u0026#39;\\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\left. {\\phi }\u0026#39;\\left( s \\right) \\right|_{\\varepsilon }^{\\infty }=-{{\\varepsilon }^{-1}}\\left[ {\\phi }\u0026#39;\\left( \\infty \\right)-{\\phi }\u0026#39;\\left( \\varepsilon \\right) \\right]\\] We know that \\(x\\phi \\left( x \\right)=-{\\phi }\u0026#39;\\left( x \\right)\\) \\[P\\left( X\u0026gt;\\varepsilon \\right)\\le -{{\\varepsilon }^{-1}}\\left[ 0-{\\phi }\u0026#39;\\left( \\varepsilon \\right) \\right]=\\frac{{\\phi }\u0026#39;\\left( \\varepsilon \\right)}{\\varepsilon }=\\frac{1}{\\varepsilon \\sqrt{2\\pi }}{{e}^{-\\frac{{{\\varepsilon }^{2}}}{2}}}\\le \\frac{{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\] Now, by the symmetry of distribution, \\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\]\n\rProof for Gaussian Tail Inequality for distribution of mean\rNow, let’s consider \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) and \\({{\\bar{X}}_{n}}={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{x}_{i}}}\\sim N\\left( 0,{{n}^{-1}} \\right)\\) therefore, \\({{\\bar{X}}_{n}}\\overset{d}{\\mathop{=}}\\,{{n}^{-{1}/{2}\\;}}Z\\) where \\(Z\\sim N\\left( 0,1 \\right)\\) and by Gaussian Tail Inequalities \\[P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-{1}/{2}\\;}}\\left| Z \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| Z \\right|\u0026gt;\\sqrt{n}\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\]\n\rExercise:\rImagine \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,{{\\sigma }^{2}} \\right)\\)and prove the gaussian tail inequality that \\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{{{\\sigma }^{2}}}{\\varepsilon }\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}2{{e}^{-{{{\\varepsilon }^{2}}}/{\\left( 2{{\\sigma }^{2}} \\right)}\\;}}\\]\n\r\rTheorem 2: Markov’s Inequality\rLet \\(X\\) be a non-negative random variable and \\(E\\left( X \\right)\\) exists, For any \\(t\u0026gt;0\\); \\(P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}\\)\nProof of Markov’s Inequality\rFor \\(X\u0026gt;0\\) we can write expectation of \\(X\\) as: \\[E\\left( X \\right)=\\int\\limits_{0}^{\\infty }{xp\\left( x \\right)dx}=\\int\\limits_{0}^{t}{xp\\left( x \\right)dx}+\\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\] \\[E\\left( X \\right)\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge t\\int\\limits_{t}^{\\infty }{p\\left( x \\right)dx}=tP\\left( X\u0026gt;t \\right)\\] \\[\\frac{E\\left( X \\right)}{t}\\ge P\\left( X\u0026gt;t \\right)\\] \\[P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}\\]\n\r\rTheorem 3: Chebyshev’s Inequality\rLet \\(\\mu =E\\left( X \\right)\\) and \\(Var\\left( X \\right)={{\\sigma }^{2}}\\), then \\(P\\left( \\left| X-\\mu \\right|\\ge t \\right)\\le \\frac{{{\\sigma }^{2}}}{{{t}^{2}}}\\) and \\(P\\left( \\left| Z \\right|\\ge k \\right)\\le \\frac{1}{{{k}^{2}}}\\)where \\(Z=\\frac{X-\\mu }{{{\\sigma }^{2}}}\\) and in particular \\(P\\left( \\left| Z \\right|\u0026gt;2 \\right)\\le \\frac{1}{4}\\) and \\(P\\left( \\left| Z \\right|\u0026gt;3 \\right)\\le \\frac{1}{9}\\).\nProof of Chebyshev’s Inequality\rLet’s take \\[P\\left( \\left| X-\\mu \\right|\u0026gt;t \\right)=P\\left( {{\\left| X-\\mu \\right|}^{2}}\u0026gt;{{t}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{t}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{t}^{2}}}\\] Let’s take \\[P\\left( \\left| \\frac{X-\\mu }{\\sigma } \\right|\u0026gt;\\sigma k \\right)=P\\left( {{\\left| \\frac{X-\\mu }{\\sigma } \\right|}^{2}}\u0026gt;{{\\sigma }^{2}}{{k}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{1}{{{k}^{2}}}\\]\n\r\rTheorem 4: Hoeffding Inequality\rIn probability theory, Hoeffding’s inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount. Hoeffding’s inequality was proven by Wassily Hoeffding in 1963. This inequality is sharper than Markov inequality and we can create upper bound without knowing the variance.\nIf \\(a\u0026lt;X\u0026lt;b\\) and \\(\\mu =E\\left( X \\right)\\) then \\(P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\).\nProof (part-A)\rLet’s assume \\(\\mu =0\\). If data is don’t have \\(\\mu =0\\), we can always center the data and \\(a\u0026lt;X\u0026lt;b\\). Now\n\\[X=\\gamma a+\\left( 1-\\gamma \\right)b\\] where \\(0\u0026lt;\\gamma \u0026lt;1\\) and \\(\\gamma =\\frac{X-a}{b-a}\\). With convexity we can write: \\[{{e}^{tX}}\\le \\gamma {{e}^{tb}}+\\left( 1-\\gamma \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\left( 1-\\frac{X-a}{b-a} \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\frac{b-X}{b-a}{{e}^{ta}}\\] \\[{{e}^{tX}}\\le \\frac{X{{e}^{tb}}-a{{e}^{tb}}+b{{e}^{ta}}-X{{e}^{ta}}}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}\\] Let’s take the expectation on the both sides: \\[E\\left( {{e}^{tX}} \\right)\\le E\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+E\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}E\\left( X \\right)\\] Since \\(\\mu =E\\left( X \\right)=0\\) therefore, \\[E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+0\\] \\[E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}\\]\nLet’s define \\[{{e}^{g\\left( t \\right)}}=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}\\]\nTaking \\(log\\) on the both sides:\n\\[\\log \\left( {{e}^{g\\left( t \\right)}} \\right)=\\log \\left( \\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a} \\right)\\]\n\\[g\\left( t \\right)=\\log \\left( {{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right) \\right)-\\log \\left( b-a \\right)\\]\n\\[g\\left( t \\right)=\\log \\left( {{e}^{ta}} \\right)+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)\\]\n\\[\\begin{equation}\rg\\left( t \\right)=ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)\r\\end{equation}\\]\r\rTaylor series expansion\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) , we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots \\] For a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) that is \\(m\\) times differentiable, we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}\\] where \\({{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}\\) and and \\(\\xi\\) lies between \\(x\\) and \\({{x}_{0}}\\).\n\rProof (part-B)\rNow, let’s evaluate \\(g\\left( t=0 \\right)\\) we get:\r\\[\\begin{equation}\rg\\left( t=0 \\right)=g\\left( 0 \\right)=0+\\log \\left( b-a \\right)-\\log \\left( b-a \\right)=0\r\\end{equation}\\]\rLet’s evaluate \\({g}\u0026#39;\\left( t=0 \\right)\\) but before that lets find \\({g}\u0026#39;\\left( t \\right)\\). \\[\\frac{dg\\left( t \\right)}{dt}={g}\u0026#39;\\left( t \\right)=\\frac{d\\left( ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right) \\right)}{dt}\\] \\[{g}\u0026#39;\\left( t \\right)=\\frac{d\\left( ta \\right)}{dt}+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}\\] \\[{g}\u0026#39;\\left( t \\right)=a+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}\\frac{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\underbrace{\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}}_{0}\\] \\[{g}\u0026#39;\\left( t \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}\\] Consider the second term: \\[\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right){{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a{{e}^{t\\left( b-a \\right)}}{{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a}=\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}\\] \\[{g}\u0026#39;\\left( t \\right)=a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}\\] Now Let’s evaluate \\({g}\u0026#39;\\left( t=0 \\right)\\), we get\r\\[\\begin{equation}\r{g}\u0026#39;\\left( t=0 \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{0\\left( b-a \\right)}}}{b-a{{e}^{0\\left( b-a \\right)}}}=a+\\frac{-a\\left( b-a \\right)}{b-a}=0\r\\end{equation}\\]\rNow let’s take\\({{g}\u0026#39;}\u0026#39;\\left( t \\right)\\).\n\\[\\frac{d{g}\u0026#39;\\left( t \\right)}{dt}={{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{d\\left( a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}=\\frac{d\\left( \\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}\\]\n\\[{{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{-a\\left( b-a \\right)\\left( -b \\right)\\left( -\\left( b-a \\right){{e}^{-t\\left( b-a \\right)}} \\right)}{{{\\left( a+b{{e}^{-t\\left( b-a \\right)}} \\right)}^{2}}}=\\frac{ab{{\\left( b-a \\right)}^{2}}\\left[ -{{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\]\nNow we can compare following two terms. \\[a{{e}^{t\\left( b-a \\right)}}\\ge a\\]\nNegate \\(b\\) and square on the both sides:\n\\[{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}\\ge {{\\left( a-b \\right)}^{2}}={{\\left( b-a \\right)}^{2}}\\]\n\\[\\frac{1}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{1}{{{\\left( b-a \\right)}^{2}}}\\]\nFrom above inequality, we can write:\n\\[{{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{-ab{{\\left( b-a \\right)}^{2}}\\left[ {{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{-ab{{\\left( b-a \\right)}^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\n\\[\\begin{equation}\r{g}\u0026#39;\u0026#39;\\left( t \\right)\\le -ab=\\frac{{{\\left( a-b \\right)}^{2}}-{{\\left( b-a \\right)}^{2}}}{4}\\le \\frac{{{\\left( b-a \\right)}^{2}}}{4}\r\\end{equation}\\]\rNow, with Taylor series expansion we have:\n\\[g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026#39;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( 0 \\right)+\\cdots\\]\nAnd with truncating Taylor series expansion, we can write. (Note this is not approximation, its exact)\n\\[g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026#39;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( \\xi \\right)=\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( \\xi \\right)\\le \\frac{1}{2!}{{t}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{4}\\]\n\\[\\begin{equation}\rg\\left( t \\right)\\le \\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\r\\end{equation}\\]\r\rProof (part-C)\rWe have bound \\(E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}\\) and \\({{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\) .\nConsider \\[P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\]\nAnd Now with Markov inequality:\n\\[P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}\\]\nNow we can make it sharper by following argument:\n\\[P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,\\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}\\]\n\rProof for sharper version with Chernoff’s method\rlet define: \\(u=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\\) and find the minima as setting FOC as \\({u}\u0026#39;\\left( t \\right)=\\varepsilon -\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0\\) and \\({{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}\\) then substituting to get:\n\\[{{u}_{\\min }}=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-{{\\left( \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}} \\right)}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}\\]\n\\[{{u}_{\\min }}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\nThe reason we want to get \\({{u}_{\\min }}\\) is to make sharper argument for the inequality or alternatively we would like to bound for the minima. Now substituting:\n\\[P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le {{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\n\\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nThis is very important results, because there is no mean or variance, so this result is very cogitative. If we observe any type of random variable whose functional form is unknown, the above statement is true.\n\rProof for random variable with non zero mean.\rNow we can apply with the mean ie. \\(\\mu =E\\left( X \\right)\\) and \\(Y=x-\\mu\\) i.e. \\(a-\\mu \u0026lt;Y\u0026lt;b-\\mu\\). And:\n\\[P\\left( \\left| Y \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-\\mu -a+\\mu \\right)}^{2}}}}}=2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nSo, \\(P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\) is known as Hoeffding’s Inequality. This shows that the variation of the random variable beyond its mean by certain amount \\(\\varepsilon\\) is upper bounded by \\(2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\). This is true for any random variable so it’s very powerful generalization.\n\rProof for bound of mean\rLet’s define \\({{\\bar{Y}}_{n}}=\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\\) and \\({{Y}_{i}}\\) are i.id then let’s bound it as:\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;\\varepsilon \\right)=P\\left( \\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;n\\varepsilon \\right)=P\\left( {{e}^{\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{n\\varepsilon }} \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\]\nNote, we introduce \\(t\\) there that’s for the flexibility that later, I can choose \\(t\\). Now with Markov inequality we can write under the assumption of i.i.d of \\({{Y}_{i}}\\)\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\le {{e}^{-tn\\varepsilon }}E\\left[ {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}} \\right]={{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\]\nSince, we have bound \\(E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}\\) as \\({{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\) , therefore,\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le {{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\le {{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\]\nLet’s try to put sharper bound and try and solve for \\(u\\left( t \\right)=tn\\varepsilon -n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\\) and the FOC is \\({u}\u0026#39;\\left( t \\right)=n\\varepsilon -n\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0\\) and solving we get \\[{{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}\\] and the plugging the value of \\({{t}^{*}}\\) on \\(u\\left( t \\right)\\) gives:\n\\[{{u}_{\\min }}={{t}^{*}}n\\varepsilon -n\\frac{{{t}^{*}}^{2}{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}n\\varepsilon -n\\frac{{{\\left( 4\\varepsilon \\right)}^{2}}}{{{\\left( {{\\left( b-a \\right)}^{2}} \\right)}^{2}}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}-\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\nThen,\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,{{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{\\frac{-2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nThen,\n\\[P\\left( \\left| {{{\\bar{Y}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nHence, this gives the bound on the mean.\n\rProof for Binominal\rHoeffding’s inequality for the \\({{Y}_{1}}\\sim Ber\\left( p \\right)\\) and it’s upper bound is \\(1\\) and lower bound is \\(0\\) so \\({{\\left( b-a \\right)}^{2}}=1\\) and with Hoeffding inequality \\[P\\left( \\left| {{{\\bar{X}}}_{n}}-p \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-2n{{\\varepsilon }^{2}}}}\\]\n\r\rTheorem 5: Kullback Leibler Distance\rProof for distance between density is greater than zero.\rProof that the distance between any two density \\(p\\) and \\(q\\) whose random variable is \\(X\\tilde{\\ }p\\) (\\(p\\) is some distribution) is always greater than or equal to zero.\nPrior answering this, let’s quickly note two inequality, namely Cauchy-Swartz Inequality and Jensen’s inequality.\nCauchy-Swartz Inequality\r\\(\\left| EXY \\right|\\le E\\left| XY \\right|\\le \\sqrt{E\\left( {{X}^{2}} \\right)}\\sqrt{E\\left( {{Y}^{2}} \\right)}\\).\n\rJensen’s inequality\rIf \\(g\\) is convex then\\(Eg\\left( X \\right)\\ge g\\left( EX \\right)\\). If \\(g\\) is concave, then\\(Eg\\left( X \\right)\\le g\\left( EX \\right)\\).\n\rKullback Leibler Distance\rThe distance between two density \\(p\\) and \\(q\\) is defined by the Kullback Leibler Distance, and given as:\n\\[D\\left( p,q \\right)=\\int{p\\left( x \\right)\\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right)}dx\\]\nBefore, I move ahead, note that the self-distance between density \\(p\\) to \\(p\\) is zero and given as: \\(D\\left( p,p \\right)=0\\) and by definition distance is always greater than and equal to zero so, one thing we have to confirm is that distance between two density \\(p\\) and \\(q\\) i.e. \\(D\\left( p,q \\right)\\ge 0\\). But we will use Jensen inequality to proof \\(D\\left( p,q \\right)\\ge 0\\). Since the \\(\\log\\) function is concave in nature, so we can write, Jensen inequality that:\n\\[-D\\left( p,q \\right)=E\\left[ \\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]\\le \\log \\left[ E\\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]=\\log \\int{p\\left( x \\right)\\frac{q\\left( x \\right)}{p\\left( x \\right)}dx}=\\log \\int{q\\left( x \\right)dx}=\\log \\left( 1 \\right)=0\\]\ni.e\n\\[-D\\left( p,q \\right)\\le 0\\] i.e. \\[D\\left( p,q \\right)\\ge 0\\]\n\r\r\rTheorem 6: Maximum of a random variable\rLet \\({{X}_{i}},\\ldots {{X}_{n}}\\) be random variable. Suppose there exist \\(\\sigma \u0026gt;0\\) such that \\(E\\left( {{e}^{t{{X}_{i}}}} \\right)\\le {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\). Then, \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}\\).\nMaximum of a random variable represents how to bound maximum value of a random variable? i.e. Say the random variable is \\({{X}_{1}},\\ldots ,{{X}_{n}}\\) and say it is arranged in ascending order such that \\({{X}_{\\left( 1 \\right)}}\\le {{X}_{\\left( 2 \\right)}}\\le \\ldots \\le {{X}_{\\left( n \\right)}}\\) and \\({{X}_{\\left( n \\right)}}={{E}_{\\max }}\\left\\{ {{X}_{1}},\\cdots ,{{X}_{n}} \\right\\}\\)how to compute the distribution of that maximum value? Now the interesting thing is, say, that we don’t know the exact distribution of \\(X\\), so can we say in general without knowing the distribution that what is the maximum of a random variable?\nLet’s start with the expectation of the moment generating function given as: \\(E{{e}^{t{{X}_{i}}}}\\) then, it is bounded by \\(E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\) i.e. \\(E{{e}^{t{{X}_{i}}}}\\le E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\). Now, can we bound the maximum of \\(E{{e}^{t{{X}_{i}}}}\\)or alternatively, what is the \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\) ?\nLet’s, start with \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\) and pre-multiply this with \\(t\\) and exponentiate. i.e. \\(\\exp \\left\\{ tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}} \\right\\}\\), bounding this gives also is same as bounding \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\). Now with Jensen’s inequality we can write:\n\\[{{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le E{{e}^{t\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}=E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{e}^{t{{X}_{i}}}}\\le \\sum\\limits_{i=1}^{n}{E{{e}^{t{{X}_{i}}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\] \\[{{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\] Taking \\(\\log\\) on the both sides \\[tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\log {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\] \\[tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}\\] Dividing both sides by \\(t\\), we get \\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}\\] Now, let’s take this \\(\\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}\\) and optimize w.r.t \\(t\\) we get: \\(\\log n=\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}\\) and \\(t={{\\sigma }^{-1}}\\sqrt{2\\log n}\\). Now plugging this value, we get:\n\\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}=\\frac{2\\log n+{{t}^{2}}{{\\sigma }^{2}}}{2t}=\\frac{2\\log n+{{\\left( {{\\sigma }^{-1}}\\sqrt{2\\log n} \\right)}^{2}}{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}\\] \\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{2\\log n+{{\\sigma }^{-2}}2\\log n{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}=\\frac{2\\sqrt{2}\\sqrt{2}\\log n}{2{{\\sigma }^{-1}}\\sqrt{2}\\sqrt{\\log n}}=\\sigma \\sqrt{2\\log n}\\] Hence \\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}\\]\n\r","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"91e2a476693769ccbc5adc50e9309aae","permalink":"/post/2018-12-29-proof-of-hoeffding-inequality/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/2018-12-29-proof-of-hoeffding-inequality/","section":"post","summary":"Theorem 1: Gaussian Tail Inequality\rGiven \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) then, \\(P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\) and \\(P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\).\nProof of Gaussian Tail Inequality\rConsider a univariate \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\), then the probability density function is given as \\(\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\). Let’s take the derivative w.r.t \\(x\\) we get: \\[\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026#39;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)\\] Let’s define the gaussian tail inequality.","tags":["Probability","R","Statistics","Theory","Proof","Learning"],"title":"Probability Inequality","type":"post"},{"authors":["Shishir Shakya","Rabindra Nepal","Kishor Sharma"],"categories":null,"content":"This paper was presented in the 65th North American Meetings of the Regional Science Association International Conference held on 7-10 November 2018 at San Antonio, TX. This paper is scheduled to be presented at the 56th Annual Meetings of the Public Choice Society, March 14-16, 2019 at the Hyatt Regency Downtown Louisville, Kentucky.\n","date":1543640400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543640400,"objectID":"0ff6586922027f0e32c6c98b00bae348","permalink":"/publication/2018-ijgei/","publishdate":"2018-12-01T00:00:00-05:00","relpermalink":"/publication/2018-ijgei/","section":"publication","summary":"This paper unfolds the short run and the long run causality between per capita electric power consumption (LEPC) and per capita gross domestic product (LGDP) for Nepal during the 1971–2010. The conventional Augmented Dickey Fuller (ADF) unit root tests indicate that the series are integrated of order independently and Johansen cointegration test confirms the presence of one cointegration among the variables. A vector error correction model (VECM) is then employed. It is found that LGDP Granger causes LEPC in the long run and weakly Granger causes in the short run. While reverse causality is found not to be true. Furthermore, impulse responses are included, which estimate how each variable behave upon the policy shock and variance decompositions segregate the portion of each variable on total variation. Finally, with a tight 67% confidence interval, in sample forecast and out of sample forecasts were made from 2011 till 2020. The results indicate that total electricity consumption has no causal role as a component of economic growth in Nepal. Thus, the electricity consumption policy should be designed and implemented as a cohesion to growth but not as cohesive to growth.","tags":[],"title":"Electricity consumption and economic growth: empirical evidence from a resource-rich landlocked economy","type":"publication"},{"authors":null,"categories":null,"content":" Gaussian Tail Inequality Given ${{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)$ then, $P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }$ and $P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}$.\nProof of Gaussian Tail Inequality Consider a univariate ${{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)$, then the probability density function is given as $\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}$.\nLet\u0026rsquo;s take the derivative w.r.t $x$ we get: $$\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026rsquo;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)$$\nLet\u0026rsquo;s define the gaussian tail inequality. $$P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}$$ $$P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{{\\phi }\u0026rsquo;\\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\left. {\\phi }\u0026rsquo;\\left( s \\right) \\right|_{\\varepsilon }^{\\infty }=-{{\\varepsilon }^{-1}}\\left[ {\\phi }\u0026rsquo;\\left( \\infty \\right)-{\\phi }\u0026rsquo;\\left( \\varepsilon \\right) \\right]$$\nWe know that $x\\phi \\left( x \\right)=-{\\phi }\u0026rsquo;\\left( x \\right)$ $$P\\left( X\u0026gt;\\varepsilon \\right)\\le -{{\\varepsilon }^{-1}}\\left[ 0-{\\phi }\u0026rsquo;\\left( \\varepsilon \\right) \\right]=\\frac{{\\phi }\u0026rsquo;\\left( \\varepsilon \\right)}{\\varepsilon }=\\frac{1}{\\varepsilon \\sqrt{2\\pi }}{{e}^{-\\frac{{{\\varepsilon }^{2}}}{2}}}\\le \\frac{{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }$$\nNow, by the symmetry of distribution, $$P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }$$\nProof for Gaussian Tail Inequality for distribution of mean Now, let\u0026rsquo;s consider ${{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)$ and ${{\\bar{X}}_{n}}={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{x}_{i}}}\\sim N\\left( 0,{{n}^{-1}} \\right)$ therefore, ${{\\bar{X}}_{n}}\\overset{d}{\\mathop{=}}\\,{{n}^{-{1}/{2}\\;}}Z$ where $Z\\sim N\\left( 0,1 \\right)$ and by Gaussian Tail Inequalities $$P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-{1}/{2}\\;}}\\left| Z \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| Z \\right|\u0026gt;\\sqrt{n}\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}$$\nExercise: Imagine ${{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,{{\\sigma }^{2}} \\right)$and prove the gaussian tail inequality that $$P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{{{\\sigma }^{2}}}{\\varepsilon }\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}2{{e}^{-{{{\\varepsilon }^{2}}}/{\\left( 2{{\\sigma }^{2}} \\right)}\\;}}$$\n${{x}_{1}},\\cdots,{{x}_{n}}\\sim N\\left(0,1\\right)$\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"367f8b88c3135109678cc996e894b6fe","permalink":"/tutorial/01slt/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/01slt/","section":"tutorial","summary":"Gaussian Tail Inequality Given ${{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)$ then, $P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }$ and $P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}$.\nProof of Gaussian Tail Inequality Consider a univariate ${{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)$, then the probability density function is given as $\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}$.\nLet\u0026rsquo;s take the derivative w.r.t $x$ we get: $$\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026rsquo;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)$$","tags":null,"title":"Statistical Learning Theory","type":"docs"},{"authors":null,"categories":null,"content":" Before, I start the solution of Hoeffding\u0026rsquo;s Inequality, let me first quickly note Markov\u0026rsquo;s Inequality and Chebyshev\u0026rsquo;s Inequality.\nMarkov\u0026rsquo;s Inequality Let $X$ be a non-negative random variable and $E\\left( X \\right)$ exists, For any $t\u0026gt;0$; $P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}$\nProof of Markov\u0026rsquo;s Inequality For $X\u0026gt;0$ we can write expectation of $X$ as: $$E\\left( X \\right)=\\int\\limits_{0}^{\\infty }{xp\\left( x \\right)dx}=\\int\\limits_{0}^{t}{xp\\left( x \\right)dx}+\\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}$$ $$E\\left( X \\right)\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge t\\int\\limits_{t}^{\\infty }{p\\left( x \\right)dx}=tP\\left( X\u0026gt;t \\right)$$ $$\\frac{E\\left( X \\right)}{t}\\ge P\\left( X\u0026gt;t \\right)$$ $$P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}$$\nChebyshev\u0026rsquo;s Inequality Let $\\mu =E\\left( X \\right)$ and $Var\\left( X \\right)={{\\sigma }^{2}}$, then $P\\left( \\left| X-\\mu \\right|\\ge t \\right)\\le \\frac{{{\\sigma }^{2}}}{{{t}^{2}}}$ and $P\\left( \\left| Z \\right|\\ge k \\right)\\le \\frac{1}{{{k}^{2}}}$where $Z=\\frac{X-\\mu }{{{\\sigma }^{2}}}$ and in particular $P\\left( \\left| Z \\right|\u0026gt;2 \\right)\\le \\frac{1}{4}$ and $P\\left( \\left| Z \\right|\u0026gt;3 \\right)\\le \\frac{1}{9}$.\nProof of Chebyshev\u0026rsquo;s Inequality Let\u0026rsquo;s take $$P\\left( \\left| X-\\mu \\right|\u0026gt;t \\right)=P\\left( {{\\left| X-\\mu \\right|}^{2}}\u0026gt;{{t}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{t}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{t}^{2}}}$$ Let\u0026rsquo;s take $$P\\left( \\left| \\frac{X-\\mu }{\\sigma } \\right|\u0026gt;\\sigma k \\right)=P\\left( {{\\left| \\frac{X-\\mu }{\\sigma } \\right|}^{2}}\u0026gt;{{\\sigma }^{2}}{{k}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{1}{{{k}^{2}}}$$\nHoeffding Inequality In probability theory, Hoeffding\u0026rsquo;s inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount. Hoeffding\u0026rsquo;s inequality was proven by Wassily Hoeffding in 1963. This inequality is sharper than Markov inequality and we can create upper bound without knowing the variance.\nIf $a\u0026lt;X\u0026lt;b$ and $\\mu =E\\left( X \\right)$ then,\n$P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$\nThe detail proof will follow 3 parts.\nProof (part-A) Let\u0026rsquo;s assume $\\mu =0$. If data is don\u0026rsquo;t have $\\mu =0$, we can always center the data and $a\u0026lt;X\u0026lt;b$. Now\n$$X=\\gamma a+\\left( 1-\\gamma \\right)b$$ where $0\u0026lt;\\gamma \u0026lt;1$ and $\\gamma =\\frac{X-a}{b-a}$. With convexity we can write: $${{e}^{tX}}\\le \\gamma {{e}^{tb}}+\\left( 1-\\gamma \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\left( 1-\\frac{X-a}{b-a} \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\frac{b-X}{b-a}{{e}^{ta}}$$ $${{e}^{tX}}\\le \\frac{X{{e}^{tb}}-a{{e}^{tb}}+b{{e}^{ta}}-X{{e}^{ta}}}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}$$ Let\u0026rsquo;s take the expectation on the both sides: $$E\\left( {{e}^{tX}} \\right)\\le E\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+E\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}E\\left( X \\right)$$ Since $\\mu =E\\left( X \\right)=0$ therefore, $$E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+0$$ $$E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}$$\nLet\u0026rsquo;s define $${{e}^{g\\left( t \\right)}}=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}$$\nTaking $log$ on the both sides:\n$$\\log \\left( {{e}^{g\\left( t \\right)}} \\right)=\\log \\left( \\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a} \\right)$$\n$$g\\left( t \\right)=\\log \\left( {{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right) \\right)-\\log \\left( b-a \\right)$$\n$$g\\left( t \\right)=\\log \\left( {{e}^{ta}} \\right)+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)$$\n\\begin{equation} g\\left( t \\right)=ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right) \\end{equation}\nTaylor series expansion For a univariate function $g(x)$evaluated at ${{x}_{0}}$ , we can express with Taylor series expansion as: $$g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots $$ For a univariate function $g(x)$evaluated at ${{x}_{0}}$ that is $m$ times differentiable, we can express with Taylor series expansion as: $$g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}$$ where ${{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}$ and and $\\xi$ lies between $x$ and ${{x}_{0}}$.\nProof (part-B) Now, let\u0026rsquo;s evaluate $g\\left( t=0 \\right)$ we get: \\begin{equation} g\\left( t=0 \\right)=g\\left( 0 \\right)=0+\\log \\left( b-a \\right)-\\log \\left( b-a \\right)=0 \\end{equation}\nLet\u0026rsquo;s evaluate ${g}\u0026rsquo;\\left( t=0 \\right)$ but before that lets find ${g}\u0026rsquo;\\left( t \\right)$. $$\\frac{dg\\left( t \\right)}{dt}={g}\u0026rsquo;\\left( t \\right)=\\frac{d\\left( ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right) \\right)}{dt}$$ $${g}\u0026rsquo;\\left( t \\right)=\\frac{d\\left( ta \\right)}{dt}+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}$$ $${g}\u0026rsquo;\\left( t \\right)=a+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}\\frac{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\underbrace{\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}}_{0}$$ $${g}\u0026rsquo;\\left( t \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}$$ Consider the second term: $$\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right){{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a{{e}^{t\\left( b-a \\right)}}{{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a}=\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}$$ $${g}\u0026rsquo;\\left( t \\right)=a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}$$ Now Let\u0026rsquo;s evaluate ${g}\u0026rsquo;\\left( t=0 \\right)$, we get \\begin{equation} {g}\u0026rsquo;\\left( t=0 \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{0\\left( b-a \\right)}}}{b-a{{e}^{0\\left( b-a \\right)}}}=a+\\frac{-a\\left( b-a \\right)}{b-a}=0 \\end{equation}\nNow let\u0026rsquo;s take${{g}\u0026lsquo;}\u0026rsquo;\\left( t \\right)$.\n$$\\frac{d{g}\u0026rsquo;\\left( t \\right)}{dt}={{g}\u0026lsquo;}\u0026rsquo;\\left( t \\right)=\\frac{d\\left( a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}=\\frac{d\\left( \\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}$$\n$${{g}\u0026lsquo;}\u0026rsquo;\\left( t \\right)=\\frac{-a\\left( b-a \\right)\\left( -b \\right)\\left( -\\left( b-a \\right){{e}^{-t\\left( b-a \\right)}} \\right)}{{{\\left( a+b{{e}^{-t\\left( b-a \\right)}} \\right)}^{2}}}=\\frac{ab{{\\left( b-a \\right)}^{2}}\\left[ -{{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}$$\nNow we can compare following two terms. $$a{{e}^{t\\left( b-a \\right)}}\\ge a$$\nNegate $b$ and square on the both sides:\n$${{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}\\ge {{\\left( a-b \\right)}^{2}}={{\\left( b-a \\right)}^{2}}$$\n$$\\frac{1}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{1}{{{\\left( b-a \\right)}^{2}}}$$\nFrom above inequality, we can write:\n$${{g}\u0026lsquo;}\u0026rsquo;\\left( t \\right)=\\frac{-ab{{\\left( b-a \\right)}^{2}}\\left[ {{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{-ab{{\\left( b-a \\right)}^{2}}}{{{\\left( b-a \\right)}^{2}}}$$\n\\begin{equation} {g}\u0026rdquo;\\left( t \\right)\\le -ab=\\frac{{{\\left( a-b \\right)}^{2}}-{{\\left( b-a \\right)}^{2}}}{4}\\le \\frac{{{\\left( b-a \\right)}^{2}}}{4} \\end{equation}\nNow, with Taylor series expansion we have:\n$$g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026rsquo;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026lsquo;}\u0026rsquo;\\left( 0 \\right)+\\cdots$$\nAnd with truncating Taylor series expansion, we can write. (Note this is not approximation, its exact)\n$$g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026rsquo;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026lsquo;}\u0026rsquo;\\left( \\xi \\right)=\\frac{1}{2!}{{t}^{2}}{{g}\u0026lsquo;}\u0026rsquo;\\left( \\xi \\right)\\le \\frac{1}{2!}{{t}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{4}$$\n\\begin{equation} g\\left( t \\right)\\le \\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8} \\end{equation}\nProof (part-C) We have bound $E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}$ and ${{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}$ .\nConsider $$P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)$$\nAnd Now with Markov inequality:\n$$P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}$$\nNow we can make it sharper by following argument:\n$$P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,\\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}$$\nProof for sharper version with Chernoff\u0026rsquo;s method let define: $u=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}$ and find the minima as setting FOC as ${u}\u0026rsquo;\\left( t \\right)=\\varepsilon -\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0$ and ${{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}$ then substituting to get:\n$${{u}_{\\min }}=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-{{\\left( \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}} \\right)}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}$$\n$${{u}_{\\min }}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}$$\nThe reason we want to get ${{u}_{\\min }}$ is to make sharper argument for the inequality or alternatively we would like to bound for the minima. Now substituting:\n$$P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le {{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$$\n$$P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$$\nThis is very important results, because there is no mean or variance, so this result is very cogitative. If we observe any type of random variable whose functional form is unknown, the above statement is true.\nProof for random variable with non zero mean. Now we can apply with the mean ie. $\\mu =E\\left( X \\right)$ and $Y=x-\\mu$ i.e. $a-\\mu \u0026lt;Y\u0026lt;b-\\mu$. And:\n$$P\\left( \\left| Y \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-\\mu -a+\\mu \\right)}^{2}}}}}=2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$$\nSo, $P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$ is known as Hoeffding\u0026rsquo;s Inequality. This shows that the variation of the random variable beyond its mean by certain amount $\\varepsilon$ is upper bounded by $2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$. This is true for any random variable so it\u0026rsquo;s very powerful generalization.\nProof for bound of mean Let\u0026rsquo;s define ${{\\bar{Y}}_{n}}=\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}$ and ${{Y}_{i}}$ are i.id then let\u0026rsquo;s bound it as:\n$$P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;\\varepsilon \\right)=P\\left( \\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;n\\varepsilon \\right)=P\\left( {{e}^{\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{n\\varepsilon }} \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)$$\nNote, we introduce $t$ there that\u0026rsquo;s for the flexibility that later, I can choose $t$. Now with Markov inequality we can write under the assumption of i.i.d of ${{Y}_{i}}$\n$$P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\le {{e}^{-tn\\varepsilon }}E\\left[ {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}} \\right]={{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}$$\nSince, we have bound $E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}$ as ${{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}$ , therefore,\n$$P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le {{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\le {{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}$$\nLet\u0026rsquo;s try to put sharper bound and try and solve for $u\\left( t \\right)=tn\\varepsilon -n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}$ and the FOC is ${u}\u0026rsquo;\\left( t \\right)=n\\varepsilon -n\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0$ and solving we get $${{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}$$ and the plugging the value of ${{t}^{*}}$ on $u\\left( t \\right)$ gives:\n$${{u}_{\\min }}={{t}^{*}}n\\varepsilon -n\\frac{{{t}^{*}}^{2}{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}n\\varepsilon -n\\frac{{{\\left( 4\\varepsilon \\right)}^{2}}}{{{\\left( {{\\left( b-a \\right)}^{2}} \\right)}^{2}}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}-\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}$$\nThen,\n$$P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,{{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{\\frac{-2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$$\nThen,\n$$P\\left( \\left| {{{\\bar{Y}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}$$\nHence, this gives the bound on the mean.\nProof for Binominal Hoeffding\u0026rsquo;s inequality for the ${{Y}_{1}}\\sim Ber\\left( p \\right)$ and it\u0026rsquo;s upper bound is $1$ and lower bound is $0$ so ${{\\left( b-a \\right)}^{2}}=1$ and with Hoeffding inequality $$P\\left( \\left| {{{\\bar{X}}}_{n}}-p \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-2n{{\\varepsilon }^{2}}}}$$\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"784aabd4c37f66b5a618dcebf4256117","permalink":"/tutorial/02slt/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/02slt/","section":"tutorial","summary":"Before, I start the solution of Hoeffding\u0026rsquo;s Inequality, let me first quickly note Markov\u0026rsquo;s Inequality and Chebyshev\u0026rsquo;s Inequality.\nMarkov\u0026rsquo;s Inequality Let $X$ be a non-negative random variable and $E\\left( X \\right)$ exists, For any $t\u0026gt;0$; $P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}$\nProof of Markov\u0026rsquo;s Inequality For $X\u0026gt;0$ we can write expectation of $X$ as: $$E\\left( X \\right)=\\int\\limits_{0}^{\\infty }{xp\\left( x \\right)dx}=\\int\\limits_{0}^{t}{xp\\left( x \\right)dx}+\\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}$$ $$E\\left( X \\right)\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge t\\int\\limits_{t}^{\\infty }{p\\left( x \\right)dx}=tP\\left( X\u0026gt;t \\right)$$ $$\\frac{E\\left( X \\right)}{t}\\ge P\\left( X\u0026gt;t \\right)$$ $$P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}$$","tags":null,"title":"Statistical Learning Theory","type":"docs"},{"authors":null,"categories":null,"content":" Kullback Leibler Distance Proof for distance between density is greater than zero. Proof that the distance between any two density $p$ and $q$ whose random variable is $X\\tilde{\\ }p$ ($p$ is some distribution) is always greater than or equal to zero.\nPrior answering this, let\u0026rsquo;s quickly note two inequality, namely Cauchy-Swartz Inequality and Jensen\u0026rsquo;s inequality.\nCauchy-Swartz Inequality $\\left| EXY \\right|\\le E\\left| XY \\right|\\le \\sqrt{E\\left( {{X}^{2}} \\right)}\\sqrt{E\\left( {{Y}^{2}} \\right)}$.\nJensen\u0026rsquo;s inequality If $g$ is convex then$Eg\\left( X \\right)\\ge g\\left( EX \\right)$. If $g$ is concave, then$Eg\\left( X \\right)\\le g\\left( EX \\right)$.\nKullback Leibler Distance The distance between two density $p$ and $q$ is defined by the Kullback Leibler Distance, and given as:\n$$D\\left( p,q \\right)=\\int{p\\left( x \\right)\\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right)}dx$$\nBefore, I move ahead, note that the self-distance between density $p$ to $p$ is zero and given as: $D\\left( p,p \\right)=0$ and by definition distance is always greater than and equal to zero so, one thing we have to confirm is that distance between two density $p$ and $q$ i.e. $D\\left( p,q \\right)\\ge 0$. But we will use Jensen inequality to proof $D\\left( p,q \\right)\\ge 0$. Since the $\\log$ function is concave in nature, so we can write, Jensen inequality that:\n$$-D\\left( p,q \\right)=E\\left[ \\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]\\le \\log \\left[ E\\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]=\\log \\int{p\\left( x \\right)\\frac{q\\left( x \\right)}{p\\left( x \\right)}dx}=\\log \\int{q\\left( x \\right)dx}=\\log \\left( 1 \\right)=0$$\ni.e\n$$-D\\left( p,q \\right)\\le 0$$ i.e. $$D\\left( p,q \\right)\\ge 0$$\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"8ddee15d6c006dbe17258f8392d61f5c","permalink":"/tutorial/03slt/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/03slt/","section":"tutorial","summary":"Kullback Leibler Distance Proof for distance between density is greater than zero. Proof that the distance between any two density $p$ and $q$ whose random variable is $X\\tilde{\\ }p$ ($p$ is some distribution) is always greater than or equal to zero.\nPrior answering this, let\u0026rsquo;s quickly note two inequality, namely Cauchy-Swartz Inequality and Jensen\u0026rsquo;s inequality.\nCauchy-Swartz Inequality $\\left| EXY \\right|\\le E\\left| XY \\right|\\le \\sqrt{E\\left( {{X}^{2}} \\right)}\\sqrt{E\\left( {{Y}^{2}} \\right)}$.\nJensen\u0026rsquo;s inequality If $g$ is convex then$Eg\\left( X \\right)\\ge g\\left( EX \\right)$.","tags":null,"title":"Statistical Learning Theory","type":"docs"},{"authors":null,"categories":null,"content":" Maximum of a random variable Let ${{X}_{i}},\\ldots {{X}_{n}}$ be random variable. Suppose there exist $\\sigma \u0026gt;0$ such that $E\\left( {{e}^{t{{X}_{i}}}} \\right)\\le {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$. Then, $E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}$.\nMaximum of a random variable represents how to bound maximum value of a random variable? i.e. Say the random variable is ${{X}_{1}},\\ldots ,{{X}_{n}}$ and say it is arranged in ascending order such that ${{X}_{\\left( 1 \\right)}}\\le {{X}_{\\left( 2 \\right)}}\\le \\ldots \\le {{X}_{\\left( n \\right)}}$ and ${{X}_{\\left( n \\right)}}={{E}_{\\max }}\\left{ {{X}_{1}},\\cdots ,{{X}_{n}} \\right}$ how to compute the distribution of that maximum value? Now the interesting thing is, say, that we don\u0026rsquo;t know the exact distribution of $X$, so can we say in general without knowing the distribution that what is the maximum of a random variable?\nLet\u0026rsquo;s start with the expectation of the moment generating function given as: $E{{e}^{t{{X}_{i}}}}$ then, it is bounded by $E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$ i.e. $E{{e}^{t{{X}_{i}}}}\\le E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$. Now, can we bound the maximum of $E{{e}^{t{{X}_{i}}}}$or alternatively, what is the $E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}$ ?\nLet\u0026rsquo;s, start with $E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}$ and pre-multiply this with $t$ and exponentiate. i.e. $\\exp \\left{ tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}} \\right}$, bounding this gives also is same as bounding $E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}$. Now with Jensen\u0026rsquo;s inequality we can write:\n$${{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le E{{e}^{t\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}=E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{e}^{t{{X}_{i}}}}\\le \\sum\\limits_{i=1}^{n}{E{{e}^{t{{X}_{i}}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$$\n$${{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$$\nTaking $\\log$ on the both sides\n$$tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\log {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$$\n$$tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}$$\nDividing both sides by $t$, we get\n$$E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}$$\nNow, let\u0026rsquo;s take this $\\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}$ and optimize w.r.t $t$ we get: $\\log n=\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}$ and $t={{\\sigma }^{-1}}\\sqrt{2\\log n}$. Now plugging this value, we get:\n$$E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}=\\frac{2\\log n+{{t}^{2}}{{\\sigma }^{2}}}{2t}=\\frac{2\\log n+{{\\left( {{\\sigma }^{-1}}\\sqrt{2\\log n} \\right)}^{2}}{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}$$ $$E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{2\\log n+{{\\sigma }^{-2}}2\\log n{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}=\\frac{2\\sqrt{2}\\sqrt{2}\\log n}{2{{\\sigma }^{-1}}\\sqrt{2}\\sqrt{\\log n}}=\\sigma \\sqrt{2\\log n}$$ Hence $$E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}$$\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"b9e41dcab1db8c546bd8760b919e7dfb","permalink":"/tutorial/04slt/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/04slt/","section":"tutorial","summary":"Maximum of a random variable Let ${{X}_{i}},\\ldots {{X}_{n}}$ be random variable. Suppose there exist $\\sigma \u0026gt;0$ such that $E\\left( {{e}^{t{{X}_{i}}}} \\right)\\le {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}$. Then, $E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}$.\nMaximum of a random variable represents how to bound maximum value of a random variable? i.e. Say the random variable is ${{X}_{1}},\\ldots ,{{X}_{n}}$ and say it is arranged in ascending order such that ${{X}_{\\left( 1 \\right)}}\\le {{X}_{\\left( 2 \\right)}}\\le \\ldots \\le {{X}_{\\left( n \\right)}}$ and ${{X}_{\\left( n \\right)}}={{E}_{\\max }}\\left{ {{X}_{1}},\\cdots ,{{X}_{n}} \\right}$ how to compute the distribution of that maximum value?","tags":null,"title":"Statistical Learning Theory","type":"docs"},{"authors":null,"categories":null,"content":" Shiny Tutorial The following two links show my online course on the Shiny App development and deployment. This course was co-designed with Prof. Greg DeAngelo and was taught in summer of 2017 and 2018 for BUDA 550: Business Data Visualization course of Business and Data Analytics (M.S).\nPart-1 | Part-2\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"e494ac20dfc1c3c77423878efd755ccb","permalink":"/tutorial/01shiny/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/01shiny/","section":"tutorial","summary":"Shiny Tutorial The following two links show my online course on the Shiny App development and deployment. This course was co-designed with Prof. Greg DeAngelo and was taught in summer of 2017 and 2018 for BUDA 550: Business Data Visualization course of Business and Data Analytics (M.S).\nPart-1 | Part-2","tags":null,"title":"Shiny Tutorial","type":"docs"},{"authors":["Joshua C. Hall","James E. Payne","Shishir Shakya"],"categories":null,"content":"This paper is under review in Energy Source journal.\n","date":1533096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533096000,"objectID":"59ac9465f9cccee5e92ee93aa18a56e3","permalink":"/publication/2018-federalregulation/","publishdate":"2018-08-01T00:00:00-04:00","relpermalink":"/publication/2018-federalregulation/","section":"publication","summary":"A large body of literature finds that the energy sector is important to economic growth and development. The production of energy has negative environmental impacts, however, which has resulted in the sector being highly regulated. While several studies examine the effect of particular regulations on the energy sector, in this study we use the recently developed measure of regulation called RegData to estimate the impact of federal regulations on the energy sector. We employ a panel ARDL model to find an inverted U-shaped relationship between federal regulations and U.S. energy sector output. Federal regulations appear to increase energy sector outputs at low levels and then decline as regulations accumulate. ","tags":[],"title":"Federal Regulations and U.S. Energy Sector Output","type":"publication"},{"authors":["Shishir Shakya"],"categories":null,"content":"This paper was presented in the 65th North American Meetings of the Regional Science Association International Conference held on 7-10 November 2018 at San Antonio, TX. This paper is scheduled to be presented at the 56th Annual Meetings of the Public Choice Society, March 14-16, 2019 at the Hyatt Regency Downtown Louisville, Kentucky.\n","date":1530417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530417600,"objectID":"e4dcd1c2b451365f9e614a6985415d93","permalink":"/publication/2018-economicfreedom/","publishdate":"2018-07-01T00:00:00-04:00","relpermalink":"/publication/2018-economicfreedom/","section":"publication","summary":"With innovative mindsets, entrepreneurs organize and manage various factors of production and seize un-grasped profit opportunities. Along with mindset, willingness, and ability, they also require freedom to act on their vision which can largely depend upon their institutional settings. Ample studies find a positive association between economic freedom and entrepreneurship at the international level, however, limited studies venture on the regional level. This paper provides newer insights into economic freedom and entrepreneurship in the US from 2004 to 2015 using high dimensional panel data and machine learning method designed for causal inference. The startup density, taken from Kauffman Index of Startup Activities, proxies the entrepreneurship and the Economic Freedom of North America (EFNA) index proxies economic freedom. The startup density is defined for the employer firms less than one year old that employ at least one person and measures the number of newly established employer businesses to the total employer business population (in 1,000s).To select on observable confounders, I implement the double-post-LASSO method on big-data of nearly 200 variables related to socio, economic, demographic, housing, political confounding features. I find an increase in labor market freedom affects startups density positively but overall business adversely. Furthermore, less restrictive government, tax, and economic freedom adversely affects startups density but affects overall business effectively.","tags":[],"title":"Economic Freedom and Entrepreneurial Startup Activities in United States","type":"publication"},{"authors":["Shishir Shakya","Bingxin Li","Xiaoli L. Etienne"],"categories":null,"content":"This paper is still under development.\n","date":1522555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522555200,"objectID":"e3d29b79db6bc47076fa744eebac5c4a","permalink":"/publication/2018-shalebandtheory/","publishdate":"2018-04-01T00:00:00-04:00","relpermalink":"/publication/2018-shalebandtheory/","section":"publication","summary":"The fracking boom led the U.S. regional oil producers to gain more market power. We evaluate the dynamics of. regional producers of U.S energy markets. The strong stance of U.S. regional producers who operate in the free-market setting threatens OPEC producers, as suggested by Meredith (2018), as U.S. producers are enjoying a second wave of growth so extraordinary that in 2018 their increase in liquids production could equal global demand growth. This paper contributes to provide econometric modelling and intuitive theoretical argument on the behavior of U.S. shale oil producers. This paper contributes to the scant literature in several ways. First, we provide a generic game theoretical setting to explain the producer’s behaviors in responses to the price changes. Secondly, we provide empirical estimation and decompose the spillovers of the producer’s behavior and price responses for oil and gas with an evolutionary approach. Thirdly, we provide several robustness checks on our estimations.","tags":[],"title":"Impact of Shale Boom on Oil and Natural Gas Prices","type":"publication"},{"authors":["Zarina Ismailova","Shishir Shakya","Xiaoli L. Etienne","Fabio Mattos"],"categories":null,"content":"*This paper was presented at Paper presented at the NCCC-134 Conference on Applied Commodity Price Analysis, Forecasting, and Market Risk Management in Minneapolis, Minnesota, April 16-17, 2018.\n","date":1522555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522555200,"objectID":"9ef0ccc0da48f5fe33086e51b7c08a45","permalink":"/publication/2018-lumber/","publishdate":"2018-04-01T00:00:00-04:00","relpermalink":"/publication/2018-lumber/","section":"publication","summary":"The impact of new information from public reports has been widely investigated in many commodity markets, but little attention has been paid to the lumber market. In this paper, we examine the impact of two housing market reports, namely the New Residential Construction (Housing Starts) and the New Residential Sales reports, on the U.S. lumber futures market. Our results suggest that the housing starts report indeed affect lumber market volatility, while the New Residential sales report exerts a minor impact on lumber price volatility. We further find that the effect of the two reports on volatility differs depending on the level of inventory and nature of the news. When the level of inventory is low, larger-than-expected housing starts has the largest effect on lumber volatility. During periods of abundant inventory, lower-than-expected housing starts increases the volatility most. For the new home sales reports, we find that while lower-than-expected sales do not affect the volatility of lumber prices, larger-than-expected sales do increase the volatility.","tags":[],"title":"Quantifying the Announcement Effects in the U.S. Lumber Market","type":"publication"},{"authors":["Shishir Shakya"],"categories":null,"content":"This paper was presented in the 58th Annual Meetings of Southern Regional Science Association held on March 15 – 17, 2018 at Philadelphia, PA Courtyard Philadelphia Downtown. This paper is still under development.\n","date":1519880400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519880400,"objectID":"9b40ef8402ec3396e4c9a2806b281d14","permalink":"/publication/2018-networkautoregressivemodel/","publishdate":"2018-03-01T00:00:00-05:00","relpermalink":"/publication/2018-networkautoregressivemodel/","section":"publication","summary":"A higher real compensation is associated with a higher real productivity but, in US, since 1970s, they have steadily diverged and has intrigued academics and policymakers. Several researches on the productivity–compensation gap has existed for some time, most work in this field has been conducted at the total nonfarm business sector or similar aggregate level with trend analysis. However, this paper illuminates the productivity-compensation gap in two approaches: first, I look at the industry level for all US states for which I implement a spatial analysis for each industry and second, I discuss how the states level trade networks of producers of NAICS industry explains the productivity compensation gaps. The spatial spillover of wage or income is also a long-examined strand of regional/spatial literature. The standard literatures use the spatial contiguity matrix. I complement the spatial contiguity matrix with the interregional industry trade network as the contiguity matrix to explain how the labor compensation of several industries in United States correlates among states. The data of the interregional industry trade is retrieved from the Commodity Flow Survey Public Use Microdata File known as CFS PUMS. The data of compensation, employment and industrial output of 51 states of U.S. for 71 industries is retrieved from the IO-SNAP software. With the trade network contiguity matrix, I find many equivalent results and few newer insights of labor compensation when compare to the results generated using the spatial contiguity matrix. The results suggest potential existence of productivity- compensation gap both spatially as well as regional trade network.","tags":[],"title":"Compensation-Productivity Gap–Mixing Spatial and Trade Network Dependency Approach","type":"publication"},{"authors":["Alexandre R. Scarcioffolo","Shishir Shakya","Joshua C. Hall"],"categories":null,"content":"*This paper is revised and resubmitted at the Energy Policy journal.\n","date":1514782800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514782800,"objectID":"38955a7673dd783a142795180833656b","permalink":"/publication/2018-vermont/","publishdate":"2018-01-01T00:00:00-05:00","relpermalink":"/publication/2018-vermont/","section":"publication","summary":"In 2012, Vermont became the first state in the United States to ban hydraulic fracturing for natural gas and oil production. We explain the political economy of the antifracking movement in the Vermont General Assembly. We implement the elastic-net penalized binomial logit regressions to assess the best model to explain voting outcomes by members of the Vermont House of Representatives. We control for legislator-specific characteristics, median voter preferences, and special interests. Our results show that Democrats were more likely to vote for a fracking ban while legislators representing districts with a higher percentage of population of working-age and with a higher poverty rate were likely to vote against anti-fracking. Legislators receiving more campaign donations were more likely to vote for anti-fracking, but the effect is small in magnitude. Contributions from the energy sector were not significant.","tags":[],"title":"The Political Economy of Vermont’s Anti-Fracking Movement","type":"publication"},{"authors":["Shishir Shakya","Nabraj Lama"],"categories":null,"content":"","date":1417410000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417410000,"objectID":"28a24b5dec3e2aedf8c81323e66dbb13","permalink":"/publication/2014-quotas/","publishdate":"2014-12-01T00:00:00-05:00","relpermalink":"/publication/2014-quotas/","section":"publication","summary":"Government of Nepal provides quotas and reservations for women, indigenous nationalities, Madhesi, untouchables, disables and people of backward areas. These statuses are not homogenous in economic sense. We proposed few other decision trees (rules) that can predict household poverty in Nepal based on 14,907 household observations employing classification and regression tree (CART) approach. These decision rules were based on few practically answerable questions (for respondents) and can be cross checked easily by the enumerators. We modeled 5 different scenarios that respondents were likely to answer the asked questions. These decision rules were 94% to in worst-case scenario 70% accurate in out-of-sample dataset. These proposed meaningful decision rules can be helpful on policy making and implementation that relate to positively discriminate (quota and reservation) for those who lie below poverty line. ","tags":[],"title":"Possible Decision Rules to Allocate Quotas and Reservations to Ensure Equity for Nepalese Poor","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]