[{"authors":["admin"],"categories":null,"content":"I am a Ph.D. candidate in Economics and a fellow with the Regional Research Institute at West Virginia University.\nI am an applied economist, with a focus on health, public/regional economics, causal machine learning, and policy evaluation. My research explores the U.S. health care policies, the opioid epidemic, and Medicaid.\nI will be available for interviews at the ASSA/AEA Annual Meeting in San Diego, CA (January 2020) and the NARSC Annual Meetings in Pittsburgh, PA (November 2019).\n","date":1568678400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1568678400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Ph.D. candidate in Economics and a fellow with the Regional Research Institute at West Virginia University.\nI am an applied economist, with a focus on health, public/regional economics, causal machine learning, and policy evaluation. My research explores the U.S. health care policies, the opioid epidemic, and Medicaid.\nI will be available for interviews at the ASSA/AEA Annual Meeting in San Diego, CA (January 2020) and the NARSC Annual Meetings in Pittsburgh, PA (November 2019).","tags":null,"title":"Shishir Shakya","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" Potential Outcome Framework There are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference.\nFor an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say $T$) on some outcomes (say capacity to solve riddle quizzes say $Y$).\nHe randomly sampled the total population and divided the total sample population $N$ to the control group of ${{N}_{co}}$ and treatment group ${{N}_{Tr}}$ such that ${{N}_{co}}+{{N}_{Tr}}=N$.\nSay the treatment group are exposed to the treatment i.e. $T=1$ for intervening with performance enhancing drug and control group is not intervened or given a placebo i.e. $T=0$.\nAfter this experiment, researcher takes test of all the individual and records numbers of minutes to solve the questions.\nThe average outcome of treated group is $E\\left[ Y\\left( 1 \\right)|T=1 \\right]$ and average outcome of control group is given as $E\\left[ Y\\left( 0 \\right)|T=0 \\right]$. Both averages can be estimated easily as the data are observed. The simple difference of means $SDoM$ between average outcome of treated and average outcome of control group given as:\n$$SDoM=E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=0 \\right]$$\nThe potential outcome framework, however, look at this problem and ask a simple question i.e. “What is the counterfactual?”. In another word, what would be the outcome of those who are treated if they were not been treated i.e. $E\\left[ Y\\left( 0 \\right)|T=1 \\right]=??$. Similarly, what would be the outcome of control if they had been treated i.e. $E\\left[ Y\\left( 1 \\right)|T=0 \\right]=??$.\nThen, the average treatment effect is given as:\n$$ATE=\\left( E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=1 \\right] \\right)-\\left( E\\left[ Y\\left( 0 \\right)|T=0 \\right]-E\\left[ Y\\left( 1 \\right)|T=0 \\right] \\right)$$\nWhere,\n$E\\left[ Y\\left( 1 \\right)|T=1 \\right]$ represents, given treated, the average outcome of treated group (observed in data).\n$E\\left[ Y\\left( 0 \\right)|T=1 \\right]$ represents, given treated, the average outcome of treated group if they were controlled (unobserved).\n$E\\left[ Y\\left( 0 \\right)|T=0 \\right]$ represents, given controlled, the average outcome of controlled group (observed in data).\n$E\\left[ Y\\left( 1 \\right)|T=0 \\right]$ represents, given controlled, the average outcome of controlled group if they were treated (unobserved).\nWe can also correct this estimate with sampling weights as:\n$$ATE=\\lambda \\left( E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=1 \\right] \\right)-\\left( 1-\\lambda \\right)\\left( E\\left[ Y\\left( 0 \\right)|T=0 \\right]-E\\left[ Y\\left( 1 \\right)|T=0 \\right] \\right)$$\nWhere, $\\lambda =\\frac{{{N}_{Tr}}}{N}$ or proportion of treated group and $1-\\lambda =1-\\frac{{{N}_{Tr}}}{N}=\\frac{{{N}_{co}}}{N}$ proportion of control group.\nIf we closely look at estimate of $ATE$ we find that each counterfactual of treated and control group is missing data problem, this is the fundamental problem of causal inference.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"Potential Outcome Framework There are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference.\nFor an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say $T$) on some outcomes (say capacity to solve riddle quizzes say $Y$).","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Shishir Shakya"],"categories":null,"content":"# #Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. #  \n# #Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature. #  \nSupplementary notes can be added here, including code and math.\n","date":1568678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568678400,"objectID":"717fbe20356c5711bda173936323ad9a","permalink":"/publication/2019-jmp/","publishdate":"2019-09-17T00:00:00Z","relpermalink":"/publication/2019-jmp/","section":"publication","summary":"I use the Oregon Health Insurance Experiment data (random assignment of Medicaid) to estimate the heterogeneous treatment effects of Medicaid on health care use, personal finance, health, and wellbeing. I perform the cluster-robust generalized random forest $-$ a causal machine learning approach to reveal policy reforms. These reforms prioritize Medicaid allotments to subgroups that are likely to benefit the most and improve average outcomes by about 4% to 10% over a random assignment baseline.","tags":null,"title":"Heterogeneous Effects of Medicaid and Efficient Policy Learning: Evidence from the Oregon Health Insurance Experiment","type":"publication"},{"authors":["Tuyen Pham","Shishir Shakya"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"99f948990d9f7622e66a5f040f934763","permalink":"/publication/2019-prop8/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/2019-prop8/","section":"publication","summary":"Voters in counties with DaVita clinics tend to vote to reject the caps on dialysis clinics' revenues.","tags":null,"title":"California Proposition 8 - Voters Reject the “Fair Pricing” for the Dialysis Act","type":"publication"},{"authors":["Shishir Shakya","Alicia Plemmons"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"9b40ef8402ec3396e4c9a2806b281d14","permalink":"/publication/2018-networkautoregressivemodel/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publication/2018-networkautoregressivemodel/","section":"publication","summary":"The industry trade networks can explain the industry level productivity-compensation gap.","tags":null,"title":"Can Trade Networks Explain the Productivity-Compensation Gap?","type":"publication"},{"authors":["Shishir Shakya"],"categories":null,"content":"","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"b1648a326bcbf3f13f967471169e3128","permalink":"/publication/2019-pdmp/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2019-pdmp/","section":"publication","summary":"Prescription Drug Monitoring programs (PDMP) are ineffective in reducing prescription opioid overdose deaths overall but the effects are heterogeneous across states with ``must-access'' or Mandatory PDMP states. I find that marijuana and naloxone access laws, poverty level, income, and education confound the impact of must-access PDMPs on prescription opioid overdose deaths.","tags":null,"title":"Impact of Must-access Prescription Drug Monitoring Program on Prescription Opioid Overdose Death Rates","type":"publication"},{"authors":["Alexandre R. Scarcioffolo","Shishir Shakya","Jousha C. Hall"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"38955a7673dd783a142795180833656b","permalink":"/publication/2018-vermont/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/2018-vermont/","section":"publication","summary":"We show the politial economy of Vermont’s Anti-Fracking bill that was passed in 2012..","tags":null,"title":"The Political Economy of Vermont’s Anti-Fracking Movement","type":"publication"},{"authors":["Shishir Shakya"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"e4dcd1c2b451365f9e614a6985415d93","permalink":"/publication/2018-economicfreedom/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/2018-economicfreedom/","section":"publication","summary":"I find startup density is sensitive to economic freedom while startup survival and startup job creation are only susceptible to the labor market freedom.","tags":null,"title":"Regional Institutional Quality and Startup Activities in the US","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["Non Parametric Econometrics"],"content":"\rUnivariate Density Estimation\rParametric Desity Estimation\rDraw from a normal distribution\rGiven \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) draw from a normal distribution with mean of \\(\\mu\\) and variance of \\({{\\sigma }^{2}}\\) the joint \\(PDF\\) can be expressed as: \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\prod\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}}\\] \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\] The term \\(\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}\\) is a constant multiplying this term for \\(n\\) times gives \\({{\\left( \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}} \\right)}^{n}}=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}\\). \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times {{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times {{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\] With the index law of addition i.e. we can add the indices for the same base \\[{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times {{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times {{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}={{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\left\\{ {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}+{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}+\\cdots +{{\\left( {{X}_{n}}-\\mu \\right)}^{2}} \\right\\}}}={{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\limits_{i=1}^{n}{{{\\left( Xi-\\mu \\right)}^{2}}}}}\\] Therefore, \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}}\\]\n\rThe log-likelihood function\rTaking the logarithm, we get the log-likelihood function as: \\[\\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\ln \\left[ \\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}} \\right]\\]\nWith the property of log i.e. multiplication inside the log can be turned into addition outside the log, and vice versa or \\(\\ln (ab)=\\ln (a)+\\ln (b)\\) \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln \\left( \\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}} \\right)+\\ln \\left( {{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}} \\right)\\] With natural log property i.e. when \\({{e}^{y}}=x\\) then \\(\\ln (x)=\\ln ({{e}^{y}})=y\\) \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln \\left( {{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{-\\frac{n}{2}}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\] \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln {{\\sigma }^{2}}-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\] \\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}};\\mu ,{{\\sigma }^{2}} \\right)=-\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\n\rLogliklihood function optimization\rTo find the optimum value of \\(L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}};\\mu ,{{\\sigma }^{2}} \\right)=-\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\), we take the first order condition w.r.t. \\(\\mu\\) and \\({{\\sigma }^{2}}\\). The necessary first order condition w.r.t \\(\\mu\\) is given as: \\[\\frac{\\partial L\\left( \\mu ,{{\\sigma }^{2}} \\right)}{\\partial \\mu }=\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\] Here, \\(\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right) \\right\\}=0\\) and \\(\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right) \\right\\}=0\\), so we only need to solve for \\[\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\] \\[-\\frac{1}{2{{\\sigma }^{2}}}\\frac{\\partial }{\\partial \\mu }\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}=0\\] \\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ \\frac{\\partial }{\\partial \\mu }\\left\\{ {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}+{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}+\\cdots +{{\\left( {{X}_{n}}-\\mu \\right)}^{2}} \\right\\} \\right]=0\\] \\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ \\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\mu }+\\frac{\\partial {{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{\\partial \\mu }+\\cdots +\\frac{\\partial {{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{\\partial \\mu } \\right]=0\\] With the chain rule i.e \\(\\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\mu }=\\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\left( {{X}_{1}}-\\mu \\right)}\\frac{\\partial \\left( {{X}_{1}}-\\mu \\right)}{\\partial \\mu }=2\\left( {{X}_{1}}-\\mu \\right)\\left( 1 \\right)=2\\left( {{X}_{1}}-\\mu \\right)\\). Hence, \\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ 2\\left( {{X}_{1}}-\\mu \\right)+2\\left( {{X}_{2}}-\\mu \\right)+\\cdots +2\\left( {{X}_{n}}-\\mu \\right) \\right]=0\\] \\[-\\frac{1}{{{\\sigma }^{2}}}\\sum\\limits_{i=1}^{n}{\\left( {{X}_{i}}-\\mu \\right)}=0\\] Since \\({{\\sigma }^{2}}\\ne 0\\), So, \\[\\sum\\limits_{i=1}^{n}{\\left( {{X}_{i}}-\\mu \\right)}=0\\] \\[\\sum\\limits_{i=1}^{n}{{{X}_{i}}}-\\sum\\limits_{i=1}^{n}{\\mu }=0\\] \\[\\sum\\limits_{i=1}^{n}{{{X}_{i}}}-n\\mu =0\\] \\[\\hat{\\mu }={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{X}_{i}}}\\]\nThe necessary first order condition w.r.t \\({{\\sigma }^{2}}\\) is given as:\n\\[\\frac{\\partial L\\left( \\mu ,{{\\sigma }^{2}} \\right)}{\\partial {{\\sigma }^{2}}}=\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\n\\[-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ \\frac{n}{2}\\ln \\left( 2\\pi \\right) \\right\\}-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ -\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right) \\right\\}-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ \\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\n\\[0-\\frac{n}{2}\\frac{1}{{{\\sigma }^{2}}}-\\frac{1}{2}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\frac{\\partial {{\\left( {{\\sigma }^{2}} \\right)}^{-1}}}{\\partial {{\\sigma }^{2}}}=0\\]\n\\[-\\frac{1}{2}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\left( -1 \\right){{\\left( {{\\sigma }^{2}} \\right)}^{-2}}=\\frac{n}{2{{\\sigma }^{2}}}\\]\n\\[\\frac{1}{2{{\\left( {{\\sigma }^{2}} \\right)}^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}=\\frac{n}{2{{\\sigma }^{2}}}\\]\n\\[{{\\hat{\\sigma }}^{2}}={{n}^{-1}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\n\\(\\hat{\\mu }\\) and \\({{\\hat{\\sigma }}^{2}}\\) above are the maximum likelihood estimator of \\(\\mu\\) and \\({{\\sigma }^{2}}\\), respectively, the resulting estimator of \\(f\\left( x \\right)\\) is: \\[\\hat{f}\\left( x \\right)=\\frac{1}{\\sqrt{2\\pi {{{\\hat{\\sigma }}}^{2}}}}{{e}^{\\left[ -\\frac{1}{2}\\left( \\frac{x-\\hat{\\mu }}{{{{\\hat{\\sigma }}}^{2}}} \\right) \\right]}}\\]\n\rSimulation example\rLet’s simulate 10000 random observation from a normal distribution with mean of 2 and standard deviation of 1.5. To reproduce the results we will use set.seed() function.\nset.seed(1234)\rN \u0026lt;- 10000\rmu \u0026lt;- 2\rsigma \u0026lt;- 1.5\rx \u0026lt;- rnorm(n = N, mean = mu, sd = sigma)\rWe can use mean() and sd() function to find the mean and sigma\n# mean\rsum(x)/length(x)\r## [1] 2.009174\rmean(x)\r## [1] 2.009174\r# standard deviation\rsqrt(sum((x - mean(x))^2)/(length(x) - 1))\r## [1] 1.481294\rsd(x)\r## [1] 1.481294\rHowever, if can also simulate and try the optimization using the mle function from the stat 4 package in R.\nLL \u0026lt;- function(mu, sigma) {\rR \u0026lt;- dnorm(x, mu, sigma)\r-sum(log(R))\r}\rstats4::mle(LL, start = list(mu = 1, sigma = 1))\r## ## Call:\r## stats4::mle(minuslogl = LL, start = list(mu = 1, sigma = 1))\r## ## Coefficients:\r## mu sigma ## 2.009338 1.481157\rTo supress the warnings in R and garanatee the solution we can use following codes.\nstats4::mle(LL, start = list(mu = 1, sigma = 1), method = \u0026quot;L-BFGS-B\u0026quot;, lower = c(-Inf, 0), upper = c(Inf, Inf))\r## ## Call:\r## stats4::mle(minuslogl = LL, start = list(mu = 1, sigma = 1), ## method = \u0026quot;L-BFGS-B\u0026quot;, lower = c(-Inf, 0), upper = c(Inf, Inf))\r## ## Coefficients:\r## mu sigma ## 2.009174 1.481221\r\rDensity plot example\rGiven the data of x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59), we can estimate \\(\\hat{\\mu }\\) as sum(x)/length(x) or mean(x) and \\({{\\hat{\\sigma }}^{2}}\\) as sum((x-mean(x))^2)/(length(x)-1) or var(x). Note I use the sample variance formula.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.4, -1.05, 1, 0.37, -1.15, 0.73, 1.59)\r# mean\rsum(x)/length(x)\r## [1] 0.249\rmean(x)\r## [1] 0.249\r# variance\rsum((x - mean(x))^2)/(length(x) - 1)\r## [1] 0.9285211\rvar(x)\r## [1] 0.9285211\rWe can also plot a parametric density function. But prior we plot, we have to sort the data.\nx \u0026lt;- sort(x)\rplot(x, dnorm(x, mean = mean(x), sd = sd(x)), ylab = \u0026quot;Density\u0026quot;, type = \u0026quot;l\u0026quot;, col = \u0026quot;blue\u0026quot;, lwd = 3)\rLet’s also plot a graph of histogram using bin width ranging from -1.5 through 2.0.\nhist(x, breaks = seq(-1.5, 2, by = 0.5), prob = TRUE)\r\r\r\rUnivariate NonParametric Density Estimation\rSet-up\rConsider \\(i.i.d\\) data \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) with \\(F\\left( \\centerdot \\right)\\) an unknown \\(CDF\\) where \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) or \\(CDF\\) of \\(X\\) evaluated at \\(x\\). We can do a na?ve estimation as \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) as cumulative sums of relative frequency as:\n\\[{{F}_{n}}\\left( x \\right)={{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\le x \\right\\}\\] and \\(n\\to\\infty\\) yields \\({{F}_{n}}\\left( x \\right)\\to F\\left( x \\right)\\).\nThe \\(PDF\\) of \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) is given as \\(f\\left( x \\right)=\\frac{d}{dx}F\\left( x \\right)\\) and an obvious estimator is: \\[\\hat{f}\\left( x \\right)=\\frac{rise}{run}=\\frac{{{F}_{n}}\\left( x+h \\right)-{{F}_{n}}\\left( x-h \\right)}{x+h-\\left( x-h \\right)}=\\frac{{{F}_{n}}\\left( x+h \\right)-{{F}_{n}}\\left( x-h \\right)}{2h}={{n}^{-1}}\\frac{1}{2h}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}\\]\n\rNaive Kernel\rThe \\(k\\left( \\centerdot \\right)\\) can be any kernel function, If we define a uniform kernel function or also known as na?ve kernel function then \\[k\\left( z \\right)=\\left\\{ \\begin{matrix}\r{1}/{2}\\; \u0026amp; if\\ \\left| z \\right|\\le 1 \\\\\r0 \u0026amp; o.w \\\\\r\\end{matrix} \\right.\\] Where \\(\\left| {{z}_{i}} \\right|=\\left| \\frac{{{X}_{i}}-x}{h} \\right|\\) and therefore is symmetric and hence \\(\\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right]\\) means \\(2\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\). Then it is easy to see \\(\\hat{f}\\left( x \\right)\\) to be expressed as: \\[\\hat{f}\\left( x \\right)=\\frac{1}{2h}{{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}=\\frac{1}{2h}{{n}^{-1}}\\sum\\limits_{i=1}^{n}{k\\left( 2\\frac{{{X}_{i}}-x}{h} \\right)}=\\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)}\\]\nWe can use follwoing code for naive kernel.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.4, -1.05, 1, 0.37, -1.15, 0.73, 1.59)\rx \u0026lt;- sort(x)\rnaive_kernel \u0026lt;- function(x, y, h) {\rz \u0026lt;- (x - y)/h\rifelse(abs(z) \u0026lt;= 1, 1/2, 0)\r}\rnaive_density \u0026lt;- function(x, h) {\rval \u0026lt;- c()\rfor (i in 1:length(x)) {\rval[i] \u0026lt;- sum(naive_kernel(x, x[i], h)/(length(x) * h))\r}\rval\r}\rH \u0026lt;- c(0.5, 1, 1.5)\rnames \u0026lt;- as.vector(paste0(\u0026quot;H = \u0026quot;, H))\rdensity_data \u0026lt;- list()\rfor (i in 1:length(H)) {\rdensity_data[[i]] \u0026lt;- naive_density(x, H[i])\r}\rdensity_data \u0026lt;- do.call(cbind.data.frame, density_data)\rcolnames(density_data) \u0026lt;- names\rmatplot(density_data, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;x\u0026quot;, ylab = \u0026quot;Density\u0026quot;, pch = 1:length(H), col = 1:length(H), main = \u0026quot;Naive Density for various Smoothing Parameter H\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, legend = names, bty = \u0026quot;n\u0026quot;, pch = 1:length(H), col = 1:length(H))\r\rEpanechnikov kernel\rConsider another optimal kernel known as Epanechnikov kernel given by: \\[k\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left\\{ \\begin{matrix}\r\\frac{3}{4\\sqrt{5}}\\left( 1-\\frac{1}{5}{{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}^{2}} \\right) \u0026amp; if\\ \\left| \\frac{{{X}_{i}}-x}{h} \\right|\u0026lt;5 \\\\\r0 \u0026amp; o.w \\\\\r\\end{matrix} \\right.\\] Let’s use x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59) and compute the kernel estimator of the density function of every sample realization using bandwidth of \\(h=0.5\\), \\(h=1\\), \\(h=1.5\\), where, \\(h\\) is smoothing parameter restricted to lie in the range of \\((0,\\infty ]\\). We can use follwoing codes to estimate the density using Epanechnikov kernel.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.4, -1.05, 1, 0.37, -1.15, 0.73, 1.59)\rx \u0026lt;- sort(x)\repanichnikov_kernel \u0026lt;- function(x, y, h) {\rz \u0026lt;- (x - y)/h\rifelse(abs(z) \u0026lt; sqrt(5), (1 - z^2/5) * (3/(4 * sqrt(5))), 0)\r}\repanichnikov_density \u0026lt;- function(x, h) {\rval \u0026lt;- c()\rfor (i in 1:length(x)) {\rval[i] \u0026lt;- sum(epanichnikov_kernel(x, x[i], h)/(length(x) * h))\r}\rval\r}\rH \u0026lt;- c(0.5, 1, 1.5)\rnames \u0026lt;- as.vector(paste0(\u0026quot;H = \u0026quot;, H))\rdensity_data \u0026lt;- list()\rfor (i in 1:length(H)) {\rdensity_data[[i]] \u0026lt;- epanichnikov_density(x, H[i])\r}\rdensity_data \u0026lt;- do.call(cbind.data.frame, density_data)\rcolnames(density_data) \u0026lt;- names\rmatplot(density_data, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;x\u0026quot;, ylab = \u0026quot;Density\u0026quot;, pch = 1:length(H), col = 1:length(H), main = \u0026quot;Epanichnikov Density for various Smoothing Parameter H\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, legend = names, bty = \u0026quot;n\u0026quot;, pch = 1:length(H), col = 1:length(H))\r\rThree properties of kernel estimator\rFor any general nonnegative bounded kernel function \\(k\\left( v \\right)\\) where \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), the kernel estimator \\(\\hat{f}\\left( x \\right)\\) is a consistent estimator of \\(f\\left( x \\right)\\) that satisfies three conditions: First is area under a kernel to be unity. \\[\\int{k(v)dv=1}\\]\nSecond is the symmetry kernel \\[\\int{vk(v)dv=0}\\] which implies symmetry condition i.e. \\(k(v)=k(-v)\\). For asymmetric kernels see Abadir and Lawford (2004).\nThird is a positive constant. \\[\\int{{{v}^{2}}k(v)dv={{\\kappa }_{2}}\u0026gt;0}\\]\n\rThe big O and small o.\r\rTaylor series expansion\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) , we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots \\] For a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) that is \\(m\\) times differentiable, we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}\\] Wwhere \\({{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}\\) and and \\(\\xi\\) lies between \\(x\\) and \\({{x}_{0}}\\)\n\rMSE, variance and biases\rSay \\(\\hat{\\theta }\\) is an estimator for true \\(\\theta\\), then the Mean Squared Error \\(MSE\\) of an estimator \\(\\hat{\\theta }\\) is the mean of squared deviation between \\(\\hat{\\theta }\\) and \\(\\theta\\) and given as: \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-\\theta \\right)}^{2}} \\right] \\] \\[MSE=E\\left[ \\left( {{{\\hat{\\theta }}}^{2}}-2\\hat{\\theta }\\theta +{{\\theta }^{2}} \\right) \\right] \\] \\[MSE=E[{{\\hat{\\theta }}^{2}}]+E[{{\\theta }^{2}}]-2\\theta E[\\hat{\\theta }]\\] \\[MSE=\\underbrace{E[{{{\\hat{\\theta }}}^{2}}]-{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]}_{\\operatorname{var}(\\hat{\\theta })}+\\underbrace{{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]+E[{{\\theta }^{2}}]-2\\theta E[\\hat{\\theta }]}_{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+\\underbrace{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}_{squared\\ of\\ bias\\ of\\ \\hat{\\theta }}\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}\\]\nNote: \\(\\underbrace{E{{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}}_{\\operatorname{var}(\\hat{\\theta })}=\\underbrace{E[{{{\\hat{\\theta }}}^{2}}]-{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]}_{\\operatorname{var}(\\hat{\\theta })}\\).\nAnother way of solution is given as : \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-\\theta \\right)}^{2}} \\right] \\] \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-E[\\hat{\\theta }]+E[\\hat{\\theta }]-\\theta \\right)}^{2}} \\right]\\] \\[MSE=E\\left[ {{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}+{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}+2\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)\\left( E[\\hat{\\theta }]-\\theta \\right) \\right]\\] \\[MSE=\\underbrace{E{{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}}_{\\operatorname{var}(\\hat{\\theta })}+\\underbrace{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}_{squared\\ of\\ bias\\ of\\hat{\\theta }}+2E\\left[ \\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)\\left( E[\\hat{\\theta }]-\\theta \\right) \\right]\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}+2E\\left[ \\left( \\hat{\\theta }E[\\hat{\\theta }]+\\hat{\\theta }\\theta -E[\\hat{\\theta }]E[\\hat{\\theta }]-E[\\hat{\\theta }]\\theta \\right) \\right]\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}+2E\\left[ \\left( \\underbrace{\\hat{\\theta }E[\\hat{\\theta }]-E[\\hat{\\theta }]E[\\hat{\\theta }]}_{0}+\\underbrace{\\hat{\\theta }\\theta -E[\\hat{\\theta }]\\theta }_{0} \\right) \\right]\\] \\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}\\]\n\rTheorem 1.1.\rLet \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) observation having a three-times differentiable \\(PDF\\) \\(f(x)\\), and \\({{f}^{s}}(x)\\) denote the \\(s-th\\) order derivative of \\(f(x)\\ s=(1,2,3)\\). Let \\(x\\) be an interior point in the support of \\(X\\), and let \\(\\hat{f}(x)\\) be \\(\\frac{1}{2h}{{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}\\). Assume that the kernel function \\(k\\left( \\centerdot \\right)\\) bounded and satisfies: \\(\\int{k(v)dv=1}\\), \\(k(v)=k(-v)\\) and \\(\\int{{{v}^{2}}k(v)dv={{\\kappa }_{2}}\u0026gt;0}\\). And as \\(n\\to\\infty\\), \\(h\\to 0\\) and \\(nh\\to\\infty\\) then, the \\(MSE\\) of estimator \\(\\hat{f}(x)\\) is given as: \\[MSE\\left( \\hat{f}(x) \\right)=\\frac{{{h}^{4}}}{4}{{\\left[ {{\\kappa }_{2}}{{f}^{\\left( 2 \\right)}}(x) \\right]}^{2}}+\\frac{\\kappa f(x)}{nh}+o\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)=O\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)\\] Where, \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), \\({{\\kappa }_{2}}=\\int{{{v}^{2}}k(v)dv}\\) and \\(\\kappa =\\int{{{k}^{2}}(v)dv}\\)\n\rProof of Theorem 1.1\rMSE, variance and biases\rWe can express the relationship of MSE, variance and bias of estimator \\(MSE\\left( \\hat{f}(x) \\right)\\) as:\n\\[MSE\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( \\hat{f}(x) \\right)+{{\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}}^{2}}\\]\nThen we deal with \\(\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}\\) and \\(\\operatorname{var}\\left( \\hat{f}(x) \\right)\\) separately.\n\rBiases\rThe bias of \\(\\hat{f}(x)\\) is given as \\[bias\\left\\{ \\hat{f}(x) \\right\\}=E[\\hat{f}(x)]-f(x)\\]\n\\[bias\\left\\{ \\hat{f}(x) \\right\\}=E\\left[ \\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]-f(x)\\] By the identical distribution, we can write: \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{nh}nE\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}\\int{f({{x}_{1}})}k\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}-f(x)\\] Note: \\(\\frac{{{x}_{1}}-x}{h}=v\\); \\({{x}_{1}}-x=hv\\); \\({{x}_{1}}=x+hv\\); \\(\\frac{d{{x}_{1}}}{dv}=\\frac{d}{dv}\\left( x+hv \\right)=h\\) and \\(d{{x}_{1}}=hdv\\). \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}\\int{f(x+hv)}k\\left( v \\right)hdv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}h\\int{f(x+hv)}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{f(x+hv)}k\\left( v \\right)dv-f(x)\\] Let’s expand \\(f(x+hv)\\) with Taylor series expansion evaluated at \\(x\\). Since \\(f(x)\\) is only three times differentiable: \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)(x+hv-x)+\\frac{1}{2!}{{f}^{(2)}}(x){{(x+hv-x)}^{2}}+\\frac{1}{3!}{{f}^{(3)}}(\\tilde{x}){{(x+hv-x)}^{3}} \\right\\}}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)hv+\\frac{1}{2!}{{f}^{(2)}}(x){{h}^{2}}{{v}^{2}}+\\frac{1}{3!}{{f}^{(3)}}(\\tilde{x}){{h}^{3}}{{v}^{3}} \\right\\}}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)hv+\\frac{1}{2!}{{f}^{(2)}}(x){{h}^{2}}{{v}^{2}}+O({{h}^{3}}) \\right\\}}k\\left( v \\right)dv-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)\\int{k\\left( v \\right)dv}+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x)\\int{{{v}^{2}}}k\\left( v \\right)dv+\\int{O({{h}^{3}})k\\left( v \\right)dv}-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)\\underbrace{\\int{k\\left( v \\right)dv}}_{1}+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x)\\underbrace{\\int{{{v}^{2}}}k\\left( v \\right)dv}_{{{\\kappa }_{2}}}+\\underbrace{\\int{O({{h}^{3}})k\\left( v \\right)dv}}_{O({{h}^{3}})}-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})-f(x)\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] By symmetrical definition \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\left[ \\left\\{ \\int\\limits_{-\\infty }^{0}{+\\int\\limits_{0}^{\\infty }{{}}} \\right\\}vk\\left( v \\right)dv \\right]+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\left[ \\left\\{ \\int\\limits_{-\\infty }^{0}{vk\\left( v \\right)dv+\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv}} \\right\\} \\right]+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] Then, we can switch the bound of definite integrals. Click for tutorial. \\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\underbrace{\\left[ \\left\\{ -\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv+\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv}} \\right\\} \\right]}_{0}+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\] \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+o({{h}^{2}})\\]\n\rVariance\rThe variance of \\(\\hat{f}(x)\\) is given as:\n\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=E{{\\left( \\hat{f}(x)-E\\left[ \\hat{f}(x) \\right] \\right)}^{2}}=E\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]-{{E}^{2}}\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]\\]\n\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\operatorname{var}\\left[ \\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(b\\in \\mathbb{R}\\) be a constant and \\(y\\) be a random variable, then, \\(\\operatorname{var}[by]={{b}^{2}}\\operatorname{var}[y]\\), therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}\\operatorname{var}\\left[ \\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(\\operatorname{var}(a+b)=\\operatorname{var}(a)+\\operatorname{var}(b)+2\\operatorname{cov}(a,b)\\) and if \\(a\\bot b\\) then \\(2\\operatorname{cov}(a,b)=0\\). In above expression \\({{X}_{i}}\\) are independent observation therefore \\({{X}_{i}}\\bot {{X}_{j}}\\ \\forall \\ i\\ne j\\). Hence, we can express \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}\\left\\{ \\sum\\limits_{i=1}^{n}{\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]}+0 \\right\\}\\] Note, \\({{X}_{i}}\\) are also identical, therefore \\(\\operatorname{var}({{X}_{i}})=\\operatorname{var}({{X}_{j}})\\) so, \\(\\sum\\limits_{i=1}^{n}{\\operatorname{var}({{X}_{i}})}=n\\operatorname{var}({{X}_{1}})\\). Therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}n\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ E\\left[ {{k}^{2}}\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]-\\left[ E{{\\left( k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right)}^{2}} \\right] \\right\\}\\] Which is equivalent to: \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f({{x}_{1}}){{k}^{2}}\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}}-{{\\left[ \\int{f({{x}_{1}})k\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}} \\right]}^{2}} \\right\\}\\] Note: \\(\\frac{{{x}_{1}}-x}{h}=v\\); \\({{x}_{1}}-x=hv\\); \\({{x}_{1}}=x+hv\\); \\(\\frac{d{{x}_{1}}}{dv}=\\frac{d}{dv}\\left( x+hv \\right)=h\\) and \\(d{{x}_{1}}=hdv\\). \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-{{\\left[ \\int{f(x+hv)k\\left( v \\right)hdv} \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x+hv){{k}^{2}}\\left( v \\right)dv}-{{\\left[ h\\int{f(x+hv)k\\left( v \\right)dv} \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-\\underbrace{{{\\left[ \\int{f(x+hv)k\\left( v \\right)hdv} \\right]}^{2}}}_{O\\left( {{h}^{2}} \\right)} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-O\\left( {{h}^{2}} \\right) \\right\\}\\] Taylor series expansion \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x)+{{f}^{(1)}}(\\xi )(x+hv-x){{k}^{2}}\\left( v \\right)dv}-O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x){{k}^{2}}\\left( v \\right)dv}+\\int{{{f}^{(1)}}(\\xi )(hv){{k}^{2}}\\left( v \\right)dv}-O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ hf(x)\\underbrace{\\int{{{k}^{2}}\\left( v \\right)dv}}_{\\kappa }+\\underbrace{\\int{{{f}^{(1)}}(\\xi )(hv){{k}^{2}}\\left( v \\right)dv}}_{O\\left( {{h}^{2}} \\right)}-O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\kappa f(x)+O\\left( {{h}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{nh}\\left\\{ \\kappa f(x)+O\\left( h \\right) \\right\\}=O\\left( {{(nh)}^{-1}} \\right)\\]\nWe now know that the order of variance is \\(O\\left( {{(nh)}^{-1}} \\right)\\), the order of bias is \\(O\\left( {{h}^{2}} \\right)\\) and the order of biases square is \\(O\\left( {{h}^{4}} \\right)\\). As we know the the MSE is sum of variance and square of biases and pluggin the values of variance and bias, we get,\n\\[MSE\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( \\hat{f}(x) \\right)+{{\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}}^{2}}\\]\n\\[MSE\\left( \\hat{f}(x) \\right)=\\frac{{{h}^{4}}}{4}{{\\left[ {{\\kappa }_{2}}{{f}^{\\left( 2 \\right)}}(x) \\right]}^{2}}+\\frac{\\kappa f(x)}{nh}+o\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)=O\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)\\] Where, \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), \\({{\\kappa }_{2}}=\\int{{{v}^{2}}k(v)dv}\\) and \\(\\kappa =\\int{{{k}^{2}}(v)dv}\\)\n\r\r\rMultivariate NonParametric Density Estimation\rTheorem\rSuppose that \\({{X}_{1}},...,{{X}_{n}}\\) constitute an i.i.d \\(q\\)-vector \\(\\left( {{X}_{i}}\\in {{\\mathbb{R}}^{q}} \\right)\\) for some \\(q\u0026gt;1\\) having common PDF \\(f(x)=f({{x}_{1}},{{x}_{2}},...,{{x}_{q}})\\). Let \\({{X}_{ij}}\\) denote the \\(j-th\\) component of \\({{X}_{i}}(j=1,...,q)\\). Then the estimated pdf given by \\(\\hat{f}(x)\\)is constructed by product kernel function or the product of univariate kernel functions.\n\\[\\hat{f}(x)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{x}_{i1}}-{{x}_{1}}}{{{h}_{1}}} \\right)\\times k\\left( \\frac{{{x}_{i2}}-{{x}_{2}}}{{{h}_{2}}} \\right)}\\times \\cdots \\times k\\left( \\frac{{{x}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)\\] \\[\\hat{f}(x)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}}\\]\n\rBias term\rThe MSE consistency of \\(\\hat{f}(x)\\)is sum of variance and square of bias term. First, we define bias given as: \\[bias\\left( \\hat{f}(x) \\right)=E\\left( \\hat{f}(x) \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}} \\right)-f(x)\\] Lets define \\(\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{X}_{ij}}-{{x}_{i}}}{{{h}_{j}}} \\right)}=K\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\) \\[bias\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}E\\left( \\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}n\\left( E\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left( E\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right)-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)f\\left( {{X}_{i}} \\right)d{{x}_{i}}}-f(x)\\] Note: \\(d{{x}_{i}}\\) is vector comprise of \\(d{{x}_{i1}},d{{x}_{i2}},...,d{{x}_{iq}}\\) and let’s \\(\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left( \\frac{{{X}_{i1}}-{{x}_{1}}}{{{h}_{1}}},\\frac{{{X}_{i1}}-{{x}_{2}}}{{{h}_{2}}},...,\\frac{{{X}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)=\\psi =\\left( {{\\psi }_{1}},{{\\psi }_{2}},...,{{\\psi }_{q}} \\right)\\). This means, \\(\\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}}={{\\psi }_{j}}\\) and \\({{x}_{ij}}={{x}_{j}}+{{\\psi }_{j}}{{h}_{j}}\\) then \\(d{{x}_{ij}}={{h}_{j}}d{{\\psi }_{j}}\\) \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\underbrace{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}_{\\psi }\\underbrace{f\\left( {{X}_{i}} \\right)}_{f\\left( x+h\\psi \\right)}\\underbrace{d{{x}_{i}}}_{{{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }}-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)=({{h}_{1}}{{h}_{2}}...{{h}_{q}}){{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-f(x)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-f(x)\\] Now perform a multivariate Taylor expansion for: \\[f\\left( x+h\\psi \\right)=\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( x+h\\psi -x \\right)+\\frac{1}{2!}{{(x+h\\psi -x)}^{T}}{{f}^{2}}(x)\\left( x+h\\psi -x \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( x+h\\psi -x \\right)}^{3}}} \\right\\}\\] \\[f\\left( x+h\\psi \\right)=\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)+\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}} \\right\\}\\] \\[bias\\left( \\hat{f}(x) \\right)=\\int{K\\left( \\psi \\right)\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)+\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}} \\right\\}d\\psi }-f(x)\\] \\[\\begin{matrix}\rbias\\left( \\hat{f}(x) \\right)= \u0026amp; \\int{f(x)K\\left( \\psi \\right)d\\psi }+\\int{{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi }+\\int{\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi } \\\\\r{} \u0026amp; +\\int{\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}K\\left( \\psi \\right)d\\psi }-f(x)} \\\\\r\\end{matrix}\\] \\[bias\\left( \\hat{f}(x) \\right)=f\\left( x \\right)\\int{K\\left( \\psi \\right)d\\psi }+{{f}^{1}}{{(x)}^{T}}h\\int{\\psi K\\left( \\psi \\right)d\\psi }+\\int{\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi +O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)-f\\left( x \\right)}\\] \\[bias\\left( \\hat{f}(x) \\right)=f\\left( x \\right)\\underbrace{\\int{K\\left( \\psi \\right)d\\psi }}_{1}+{{f}^{1}}{{(x)}^{T}}h\\underbrace{\\int{\\psi K\\left( \\psi \\right)d\\psi }}_{0}+\\frac{{{h}^{T}}h}{2}\\int{K\\left( \\psi \\right){{\\psi }^{T}}{{f}^{2}}(x)\\psi d\\psi +O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)-f\\left( x \\right)}\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{{{h}^{T}}h{{\\psi }^{T}}{{f}^{2}}(x)\\psi K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{\\underbrace{{{h}^{T}}h\\underbrace{{{\\psi }^{T}}}_{1\\times q}\\underbrace{{{f}^{2}}(x)}_{q\\times q}\\underbrace{\\psi }_{q\\times 1}}_{1\\times 1}K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{{{h}^{T}}h\\sum\\limits_{l=1}^{q}{\\sum\\limits_{m=1}^{q}{f_{lm}^{\\left( 2 \\right)}\\left( x \\right){{\\psi }_{l}}{{\\psi }_{m}}}}K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] For \\(l\\ne m\\) the cross derivatives will be zero hence, we can re-write as: \\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right){{\\kappa }_{2}}}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right)}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)=O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)\\] \\[bias\\left( \\hat{f}(x) \\right)=\\underbrace{\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right)}}_{{{c}_{1}}}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)=O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)\\]\nWhere, \\({{c}_{1}}\\) is a constant.\n\rVariance term\rThe variance is given as: \\[\\operatorname{var}\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}} \\right)\\] Lets’s define \\(\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{X}_{ij}}-{{x}_{i}}}{{{h}_{j}}} \\right)}=K\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\) \\[\\operatorname{var}\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)\\] For \\(b\\in \\mathbb{R}\\) be a constant and \\(y\\) be a random variable, then, \\(\\operatorname{var}[by]={{b}^{2}}\\operatorname{var}[y]\\), therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}\\operatorname{var}\\left[ \\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(\\operatorname{var}(a+b)=\\operatorname{var}(a)+\\operatorname{var}(b)+2\\operatorname{cov}(a,b)\\) and if \\(a\\bot b\\) then \\(2\\operatorname{cov}(a,b)=0\\). In above expression \\({{X}_{i}}\\) are independent observation therefore \\({{X}_{i}}\\bot {{X}_{j}}\\ \\forall \\ i\\ne j\\). Hence, we can express \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}\\left\\{ \\sum\\limits_{i=1}^{n}{\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]}+0 \\right\\}\\] Note, \\({{X}_{i}}\\) are also identical, therefore \\(\\operatorname{var}({{X}_{i}})=\\operatorname{var}({{X}_{j}})\\) so, \\(\\sum\\limits_{i=1}^{n}{\\operatorname{var}({{X}_{i}})}=n\\operatorname{var}({{X}_{1}})\\). Therefore, \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}n\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]\\] The variance is given as: \\(\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=E{{\\left( \\hat{f}(x)-E\\left[ \\hat{f}(x) \\right] \\right)}^{2}}=E\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]-{{E}^{2}}\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]\\) \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ E{{\\left[ K\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]}^{2}}-\\left[ E{{\\left( K\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right)}^{2}} \\right] \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{f\\left( {{X}_{i}} \\right)\\left[ {{K}^{2}}\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}}-{{\\left[ \\int{f\\left( {{X}_{i}} \\right)\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}} \\right]}^{2}}\\] Note: \\(d{{x}_{i}}\\) is vector comprise of \\(d{{x}_{i1}},d{{x}_{i2}},...,d{{x}_{iq}}\\) and let’s \\(\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left( \\frac{{{X}_{i1}}-{{x}_{1}}}{{{h}_{1}}},\\frac{{{X}_{i1}}-{{x}_{2}}}{{{h}_{2}}},...,\\frac{{{X}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)=\\psi =\\left( {{\\psi }_{1}},{{\\psi }_{2}},...,{{\\psi }_{q}} \\right)\\). This means, \\(\\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}}={{\\psi }_{j}}\\) and \\({{x}_{ij}}={{x}_{j}}+{{\\psi }_{j}}{{h}_{j}}\\) then \\(d{{x}_{ij}}={{h}_{j}}d{{\\psi }_{j}}\\) \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{\\left[ {{K}^{2}}\\underbrace{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}_{\\psi } \\right]\\underbrace{f\\left( {{X}_{i}} \\right)}_{f\\left( x+h\\psi \\right)}\\underbrace{d{{x}_{i}}}_{{{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }}-{{\\left[ \\int{f\\left( {{X}_{i}} \\right)\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}} \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-{{\\left[ \\int{f\\left( x+h\\psi \\right)K\\left( \\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi } \\right]}^{2}} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-\\underbrace{{{\\left[ {{({{h}_{1}}{{h}_{2}}...{{h}_{q}})}^{2}}\\int{f\\left( x+h\\psi \\right)K\\left( \\psi \\right)d\\psi } \\right]}^{2}}}_{O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)} \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] Now, we perform the Taylor series expansion \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{f\\left( x \\right){{K}^{2}}\\left( \\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }+\\int{{{f}^{(1)}}\\left( \\xi \\right)\\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\psi \\right){{K}^{2}}\\left( \\psi \\right)d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right)\\underbrace{\\int{{{K}^{2}}\\left( \\psi \\right)d\\psi }}_{\\mathbf{\\kappa }}+\\underbrace{\\int{{{f}^{(1)}}\\left( \\xi \\right)\\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\psi \\right){{K}^{2}}\\left( \\psi \\right)d\\psi }}_{O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)}-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right)\\mathbf{\\kappa }+O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\] \\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right){{\\kappa }^{q}}+O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}=O\\left( \\frac{1}{n{{h}_{1}}...{{h}_{q}}} \\right)\\]\n\rMSE term\rSummarizing, we obtain the MSE as: \\[MSE\\left( \\hat{f}\\left( x \\right) \\right)=\\operatorname{var}\\left( \\hat{f}\\left( x \\right) \\right)+{{\\left[ bias\\left( \\hat{f}\\left( x \\right) \\right) \\right]}^{2}}\\] \\[MSE\\left( \\hat{f}\\left( x \\right) \\right)=O\\left( \\frac{1}{n{{h}_{1}}...{{h}_{q}}} \\right)+{{\\left[ O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right]}^{2}}=O\\left( {{\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)}^{2}}+{{\\left( n{{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)}^{-1}} \\right)\\]\nHence, if as \\(n\\to \\infty\\), \\({{\\max }_{1\\le l\\le q}}{{h}_{l}}\\to 0\\) and \\(n{{h}_{1}}...{{h}_{q}}\\to \\infty\\) then we have \\(\\hat{f}\\left( x \\right)\\to f\\left( x \\right)\\) in MSE, which \\(\\hat{f}\\left( x \\right)\\to f\\left( x \\right)\\) in probability.\n\rThe optimal band-width\r\\[MSE\\left( \\hat{f}\\left( x \\right) \\right)={{c}_{1}}{{h}^{4}}+{{c}_{2}}{{n}^{-1}}{{h}^{-q}}\\]\nNow, we can choose \\(h\\) to find the optimal value of above function and the \\(F.O.C\\) is given as:\n\\[\\frac{\\partial MSE\\left( \\hat{f}\\left( x \\right) \\right)}{\\partial h}=\\frac{{{c}_{1}}{{h}^{4}}+{{c}_{2}}{{n}^{-1}}{{h}^{-q}}}{\\partial h}=0\\]\n\\[4{{c}_{1}}{{h}^{3}}+{{c}_{2}}{{n}^{-1}}(-q){{h}^{-q-1}}=0\\]\n\\[\\frac{4{{c}_{1}}}{{{c}_{2}}q}n={{h}^{-q-1-3}}\\]\nLet’s define \\(\\frac{4{{c}_{1}}}{{{c}_{2}}q}=c\\) then\n\\[{{h}^{-q-4}}=cn\\]\n\\[{{h}^{*}}={{\\left[ cn \\right]}^{-\\frac{1}{4+q}}}\\]\n\r\rAsymptotic Normality of Density Estimators\rTheorem 1.2\nLet \\({{X}_{1}},...,{{X}_{n}}\\) be \\(i.i.d\\) \\(q-\\)vector with it’s \\(PDF\\) \\(f\\left( \\cdot \\right)\\) having three-times bounded continuous derivatives. Let \\(x\\) be an interior point of the support \\(X\\). If, as \\(n\\to \\infty\\), \\({{h}_{s}}\\to 0\\) for all \\(s=1,...,q\\), \\(n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}}\\to \\infty\\) and \\(\\left( n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}} \\right)\\sum\\nolimits_{s=1}^{q}{h_{s}^{6}}\\to 0\\), then, \\[\\sqrt{n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}}}\\left[ \\hat{f}\\left( x \\right)-f\\left( x \\right)-\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{s=1}^{q}{h_{s}^{2}{{f}_{ss}}\\left( x \\right)} \\right]\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,{{\\kappa }^{q}}f\\left( x \\right) \\right)\\]\nBefore, we begin, imagine that each band width is equal i.e. \\({{h}_{1}}={{h}_{2}}=...={{h}_{q}}\\). Let’s acknowledge that as \\(n\\to \\infty\\), \\(h\\to 0\\). If this is the case then, where does \\(n{{h}^{q}}\\) go? Does this go to zero or infinity? \\({{h}^{q}}\\) doesn’t converge to zero as fast as \\(n\\to \\infty\\), then \\(n{{h}^{q}}\\to \\infty\\) but \\(n{{h}^{6+q}}\\to 0\\). Therefore, because of this reason we can set up asymptotic normality.\nLet begin with following equation: \\[\\sqrt{n{{h}^{q}}}\\left[ \\hat{f}\\left( x \\right)-E\\left[ \\hat{f}\\left( x \\right) \\right] \\right]\\]\nThis can be re-expressed as: \\[\\sqrt{n{{h}^{q}}}\\frac{1}{n{{h}^{q}}}\\sum\\limits_{i=1}^{n}{\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right)-E\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right]}\\] Let’s define \\[k\\left( \\frac{{{X}_{i}}-x}{h} \\right)={{K}_{i}}\\] then, \\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}=\\sum\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{n{{h}^{q}}}}\\left( {{K}_{i}}-E{{K}_{i}} \\right)}=\\sum\\limits_{i=1}^{n}{{{Z}_{n,i}}}\\] Note that the \\({{Z}_{n,i}}\\) is double array (or data frame, column indexed with different \\(n\\) and row with different observation \\(i\\), each combination of \\(n\\) and \\(i\\) gives different value for \\({{Z}_{n,i}}\\))\nNow, we need to think for a strategy for asymptotic normality. Actually, there are four ways (at least) to think about the asymptotic normality: 1) Khinchi’s Law of Large Numbers; 2) Lindeberg-Levy Central Limit Theorem; 3) Lindelberg-Feller Central Limit Theorem and 4) Lyapunov Central Limit Theorem. We will use Lyapunov Central Limit Theorem, for brief introduction to these laws of large numbers and central limit theorem, please see the annex.\nWe will consider following four conditions:\n\\(E\\left[ {{Z}_{ni}} \\right]=\\frac{1}{\\sqrt{n{{h}^{q}}}}\\left[ E{{K}_{i}}-E{{K}_{i}} \\right]=0\\)\n\r\\(Var\\left( {{Z}_{n,i}} \\right)=\\frac{1}{n{{h}^{q}}}Var\\left( {{K}_{i}} \\right)=\\frac{1}{n{{h}^{q}}}\\left[ EK_{i}^{2}-{{\\left( E{{K}_{i}} \\right)}^{2}} \\right]={{h}^{q}}\\frac{1}{n{{h}^{q}}}Var\\left( {{K}_{i}} \\right)={{h}^{q}}\\frac{{{\\kappa }^{q}}f\\left( x \\right)}{n{{h}^{q}}}\\left( 1+o\\left( 1 \\right) \\right)={{\\kappa }^{q}}f\\left( x \\right)\\left( 1+o\\left( 1 \\right) \\right)\\)\n\r\\(\\sigma _{n}^{2}=\\sum\\limits_{i=1}^{n}{Var\\left( {{Z}_{n,i}} \\right)}={{\\kappa }^{q}}f\\left( x \\right)\\left( 1+o\\left( 1 \\right) \\right)\\) and taking limits we get \\(\\underset{n\\to \\infty }{\\mathop{\\lim }}\\,\\sigma _{n}^{2}={{\\kappa }^{q}}f\\left( x \\right)\\)\r\\(\\sum\\limits_{i=1}^{n}{E{{\\left| {{Z}_{n,i}} \\right|}^{2+\\delta }}}=nE{{\\left| {{Z}_{n,i}} \\right|}^{2+\\delta }}\\) if each \\(i\\) are i.i.d With Cram?r-Rao bound inequality we can write \\[n\\frac{1}{{{\\left| \\sqrt{n{{h}^{q}}} \\right|}^{2+\\delta }}}E{{\\left| {{K}_{i}}-E{{K}_{i}} \\right|}^{2+\\delta }}\\le \\frac{c}{{{n}^{{\\delta }/{2}\\;}}{{h}^{{\\delta }/{2}\\;}}}\\frac{1}{{{h}^{q}}}\\left[ E{{\\left| {{K}_{i}} \\right|}^{2+\\delta }}+{{\\left| E{{K}_{i}} \\right|}^{2+\\delta }} \\right]\\to 0\\] Note that \\(n{{h}^{q}}\\to \\infty\\) so \\(\\left( \\bullet \\right)\\to 0\\), further \\(E{{\\left| {{K}_{i}} \\right|}^{2+\\delta }}\\) is higher order than \\({{\\left| E{{K}_{i}} \\right|}^{2+\\delta }}\\) With all these conditions: \\[\\sigma _{n}^{-2}\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,1 \\right)\\] \\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,\\underset{n\\to \\infty }{\\mathop{\\lim }}\\,\\sigma _{n}^{2} \\right)\\] \\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,{{\\kappa }^{q}}f\\left( x \\right) \\right)\\]\n\r\r\r","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"5df485a513dc3cfcae192c0b399ebd42","permalink":"/post/2019-01-07-density-estimation/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/2019-01-07-density-estimation/","section":"post","summary":"Univariate Density Estimation\rParametric Desity Estimation\rDraw from a normal distribution\rGiven \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) draw from a normal distribution with mean of \\(\\mu\\) and variance of \\({{\\sigma }^{2}}\\) the joint \\(PDF\\) can be expressed as: \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\prod\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}}\\] \\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\] The term \\(\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}\\) is a constant multiplying this term for \\(n\\) times gives \\({{\\left( \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}} \\right)}^{n}}=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}\\).","tags":["Density,","R,","Non Parametric,","Theory,","Proof,"],"title":"Density Estimation","type":"post"},{"authors":null,"categories":["Causal Inference"],"content":"\rPotential Outcome Framework\rThere are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference. For an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say \\(T\\)) on some outcomes (say capacity to solve riddle quizzes say \\(Y\\)).\nHe randomly sampled the total population and divided the total sample population \\(N\\) to the control group of \\({{N}_{co}}\\) and treatment group \\({{N}_{Tr}}\\) such that \\({{N}_{co}}+{{N}_{Tr}}=N\\).\nSay the treatment group are exposed to the treatment i.e. \\(T=1\\) for intervening with performance enhancing drug and control group is not intervened or given a placebo i.e. \\(T=0\\).\nAfter this experiment, researcher takes test of all the individual and records numbers of minutes to solve the questions.\nThe average outcome of treated group is \\(E\\left[ Y\\left( 1 \\right)|T=1 \\right]\\) and average outcome of control group is given as \\(E\\left[ Y\\left( 0 \\right)|T=0 \\right]\\). Both averages can be estimated easily as the data are observed. The simple difference of means \\(SDoM\\) between average outcome of treated and average outcome of control group given as:\n\\[SDoM=E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=0 \\right]\\]\nThe potential outcome framework, however, look at this problem and ask a simple question i.e. “What is the counterfactual?”. In another word, what would be the outcome of those who are treated if they were not been treated i.e. \\(E\\left[ Y\\left( 0 \\right)|T=1 \\right]=??\\). Similarly, what would be the outcome of control if they had been treated i.e. \\(E\\left[ Y\\left( 1 \\right)|T=0 \\right]=??\\). Then, the average treatment effect is given as:\n\\[ATE=\\left\\{ E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=1 \\right] \\right\\}-\\left\\{ E\\left[ Y\\left( 0 \\right)|T=0 \\right]-E\\left[ Y\\left( 1 \\right)|T=0 \\right] \\right\\}\\]\nWhere,\n\\(E\\left[ Y\\left( 1 \\right)|T=1 \\right]\\) represents, given treated, the average outcome of treated group (observed in data). \\(E\\left[ Y\\left( 0 \\right)|T=1 \\right]\\) represents, given treated, the average outcome of treated group if they were controlled (unobserved). \\(E\\left[ Y\\left( 0 \\right)|T=0 \\right]\\) represents, given controlled, the average outcome of controlled group (observed in data). \\(E\\left[ Y\\left( 1 \\right)|T=0 \\right]\\) represents, given controlled, the average outcome of controlled group if they were treated (unobserved).\nWe can also correct this estimate with sampling weights as:\n\\[ATE=\\lambda \\left\\{ E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=1 \\right] \\right\\}-\\left( 1-\\lambda \\right)\\left\\{ E\\left[ Y\\left( 0 \\right)|T=0 \\right]-E\\left[ Y\\left( 1 \\right)|T=0 \\right] \\right\\}\\]\nWhere, \\(\\lambda =\\frac{{{N}_{Tr}}}{N}\\) or proportion of treated group and \\(1-\\lambda =1-\\frac{{{N}_{Tr}}}{N}=\\frac{{{N}_{co}}}{N}\\) proportion of control group.\nIf we closely look at estimate of \\(ATE\\) we find that each counterfactual of treated and control group is missing data problem, this is the fundamental problem of causal inference.\n.\n\r","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"f7fad2a171fac2421db9ece83ba65afa","permalink":"/post/2018-12-01-rubins-potential-outcome-framework/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/2018-12-01-rubins-potential-outcome-framework/","section":"post","summary":"Potential Outcome Framework\rThere are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference. For an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say \\(T\\)) on some outcomes (say capacity to solve riddle quizzes say \\(Y\\)).","tags":["Causal Inference,","Theory,","Proof,"],"title":"Rubin's Potential Outcome Framework","type":"post"},{"authors":["Shishir Shakya"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2ed948d9c86d9f3bfba9ce906dccc5f2","permalink":"/publication/2019-shalecrime/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/2019-shalecrime/","section":"publication","summary":"Shale boom is associated with a significant rise in violent crimes with the annual victimization cost of 15.68 million (2008 dollar).","tags":null,"title":"The Dark Side of the Shale Boom--Increased Crime among Economically-small, Relatively Rural American States","type":"publication"},{"authors":null,"categories":["R"],"content":"\rReplication can have a different meaning to the different discipline. Replication in studies that uses public data sources involve sharing codes to ensure consistency in the results. Here, I share some chunk of the codes in five steps using RStudio, that can ensure ease in shareability. These 5 steps are the preambles prior I proceed toward data management.\nFor this a user will require to install R here and R-Studio here. A copy of this code is available here.\nClean the current workspace and session.\rThe very first step in to remove all the objects and plots from the current workspace and device in the R. For this, I use following codes.\nrm(list = ls())\rdev.off(dev.list()[\u0026quot;RStudioGD\u0026quot;])\r\rInstall and load R packages\rSecond step is to install the required R-packages (if r-packages don’t exist installed package directory of R) and load them into the library. In the following code, I first create a function called load_packages(). This function takes list of names of packages as argument, then check if the packages are already installed in the user’s package list, and if not then installs the packages from the cran. I always use load_packages(\u0026quot;rstudioapi\u0026quot;) package and it’s a mandatory package for mywork flow. Then I use load_packages() for other required packages. These packages depends upon the project. In the following example I use haven, and Hmisc packages.\nload_packages \u0026lt;- function(pkg){\rnew.pkg \u0026lt;- pkg[!(pkg %in% installed.packages()[, \u0026quot;Package\u0026quot;])]\rif (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE,\rrepos = \u0026quot;http://cran.us.r-project.org\u0026quot;)\rsapply(pkg, require, character.only = TRUE)\r}\rload_packages(\u0026quot;rstudioapi\u0026quot;) # This is a mandatory package.\rload_packages (c(\u0026quot;devtools\u0026quot;, \u0026quot;haven\u0026quot;, \u0026quot;Hmisc\u0026quot;)) # These packages depends upon the project.\r\rSetup Working directory.\rSetting up the working directory is my third step. It is very important because my working directory can always be different then my coauthor’s/referee’s working directory. So, I first save the R script in the desired location manually and my coauthors are also advised to do similar. Then I use following code. This code detects where R-script is saved and sets that as working directory. Note this chunk of code uses rstudioapi package, therefore it’s a mandatory package for my workflow.\npath \u0026lt;- dirname(rstudioapi::getActiveDocumentContext()$path)\rsetwd(path)\r\rThe Folders\rI never right click and create new folders. I always code to create a folder. The code helps me to track what I did in logical manner. Following codes directly create the folders in the working directory. I usually like to have folder called rawdata to dump the data downloaded from the internet then I also like another folder outcomes to save my final dataset for analysis, plots and tables. Sometimes, I can create folders within folder like rawdata/beadataset.\ndir.create(file.path(path, \u0026quot;rawdata\u0026quot;))\rdir.create(file.path(path, \u0026quot;outcomes\u0026quot;))\rdir.create(file.path(path, \u0026quot;rawdata/beadataset\u0026quot;))\r\rGetting data.\rNever download the data manually. If possible, always provide a download link and use script to download the data. And never touch the data. It seems counter-intuitive but, I never open data in excel. If I open data in excel, I make sure I don’t save or If I have to play around with data, I do that in separate folder and delete them ASAP.\nConsider following example, the data of GDP by State by Year is available from the Bureau of Economic Analysis website. These data have stable link (the link doesn’t change over time) and content of data are consistent. The data can be download in the .zip format. Again, I write script to unzip the data. The script checks if folder called rawdata has gdpstate_naics_all_C.zip file or not. If there exist no file, the script will download the file.\ngdpbystatebyyear \u0026lt;- \u0026quot;https://www.bea.gov/regional/zip/gdpstate/gdpstate_naics_all_C.zip\u0026quot;\rif (file.exists(\u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;) == FALSE) { # get the zip file\rdownload.file(gdpbystatebyyear,\rdestfile = \u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;, mode=\u0026quot;wb\u0026quot;)\r}\runzip(\u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;, exdir = paste0(path,\u0026quot;/rawdata/beadataset\u0026quot;))\rConsider another example, if data is not available from in the web, I can share them via my google drive. I upload the zip file in google drive then get the public shareable link. The object gdrivepublic comprises of the public shareable link. Like previous chunk of code, here I check if data exist or not then download.\ngdrivepublic \u0026lt;- \u0026quot;https://drive.google.com/uc?authuser=0\u0026amp;id=1AiZda_1-2nwrxI8fLD0Y6e5rTg7aocv0\u0026amp;export=download\u0026quot;\rif (file.exists(\u0026quot;datafromGoogleDrive.zip\u0026quot;) == FALSE) { # get the zip file\rdownload.file(gsub(\u0026quot;open\\\\?\u0026quot;, \u0026quot;uc\\\\?export=download\\\\\u0026amp;\u0026quot;, gdrivepublic), destfile = \u0026quot;datafromGoogleDrive.zip\u0026quot;, mode=\u0026quot;wb\u0026quot;)\r}\r\r","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"ecb40a5a1bcfd5aecaaf9e7a172ac4f7","permalink":"/post/2018-12-26-preamble-of-reproducible-research/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/2018-12-26-preamble-of-reproducible-research/","section":"post","summary":"Replication can have a different meaning to the different discipline. Replication in studies that uses public data sources involve sharing codes to ensure consistency in the results. Here, I share some chunk of the codes in five steps using RStudio, that can ensure ease in shareability. These 5 steps are the preambles prior I proceed toward data management.\nFor this a user will require to install R here and R-Studio here.","tags":["Reproducible,","R,"],"title":"Preambles for Reproducible Research","type":"post"},{"authors":null,"categories":["Theoritical Statistics"],"content":"\rTheorem 1: Gaussian Tail Inequality\rGiven \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) then, \\(P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\) and \\(P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\).\nProof of Gaussian Tail Inequality\rConsider a univariate \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\), then the probability density function is given as \\(\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\). Let’s take the derivative w.r.t \\(x\\) we get: \\[\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026#39;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)\\] Let’s define the gaussian tail inequality. \\[P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}\\] \\[P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{{\\phi }\u0026#39;\\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\left. {\\phi }\u0026#39;\\left( s \\right) \\right|_{\\varepsilon }^{\\infty }=-{{\\varepsilon }^{-1}}\\left[ {\\phi }\u0026#39;\\left( \\infty \\right)-{\\phi }\u0026#39;\\left( \\varepsilon \\right) \\right]\\] We know that \\(x\\phi \\left( x \\right)=-{\\phi }\u0026#39;\\left( x \\right)\\) \\[P\\left( X\u0026gt;\\varepsilon \\right)\\le -{{\\varepsilon }^{-1}}\\left[ 0-{\\phi }\u0026#39;\\left( \\varepsilon \\right) \\right]=\\frac{{\\phi }\u0026#39;\\left( \\varepsilon \\right)}{\\varepsilon }=\\frac{1}{\\varepsilon \\sqrt{2\\pi }}{{e}^{-\\frac{{{\\varepsilon }^{2}}}{2}}}\\le \\frac{{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\] Now, by the symmetry of distribution, \\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\]\n\rProof for Gaussian Tail Inequality for distribution of mean\rNow, let’s consider \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) and \\({{\\bar{X}}_{n}}={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{x}_{i}}}\\sim N\\left( 0,{{n}^{-1}} \\right)\\) therefore, \\({{\\bar{X}}_{n}}\\overset{d}{\\mathop{=}}\\,{{n}^{-{1}/{2}\\;}}Z\\) where \\(Z\\sim N\\left( 0,1 \\right)\\) and by Gaussian Tail Inequalities \\[P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-{1}/{2}\\;}}\\left| Z \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| Z \\right|\u0026gt;\\sqrt{n}\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\]\n\rExercise:\rImagine \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,{{\\sigma }^{2}} \\right)\\)and prove the gaussian tail inequality that \\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{{{\\sigma }^{2}}}{\\varepsilon }\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}2{{e}^{-{{{\\varepsilon }^{2}}}/{\\left( 2{{\\sigma }^{2}} \\right)}\\;}}\\]\n\r\rTheorem 2: Markov’s Inequality\rLet \\(X\\) be a non-negative random variable and \\(E\\left( X \\right)\\) exists, For any \\(t\u0026gt;0\\); \\(P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}\\)\nProof of Markov’s Inequality\rFor \\(X\u0026gt;0\\) we can write expectation of \\(X\\) as: \\[E\\left( X \\right)=\\int\\limits_{0}^{\\infty }{xp\\left( x \\right)dx}=\\int\\limits_{0}^{t}{xp\\left( x \\right)dx}+\\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\] \\[E\\left( X \\right)\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge t\\int\\limits_{t}^{\\infty }{p\\left( x \\right)dx}=tP\\left( X\u0026gt;t \\right)\\] \\[\\frac{E\\left( X \\right)}{t}\\ge P\\left( X\u0026gt;t \\right)\\] \\[P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}\\]\n\r\rTheorem 3: Chebyshev’s Inequality\rLet \\(\\mu =E\\left( X \\right)\\) and \\(Var\\left( X \\right)={{\\sigma }^{2}}\\), then \\(P\\left( \\left| X-\\mu \\right|\\ge t \\right)\\le \\frac{{{\\sigma }^{2}}}{{{t}^{2}}}\\) and \\(P\\left( \\left| Z \\right|\\ge k \\right)\\le \\frac{1}{{{k}^{2}}}\\)where \\(Z=\\frac{X-\\mu }{{{\\sigma }^{2}}}\\) and in particular \\(P\\left( \\left| Z \\right|\u0026gt;2 \\right)\\le \\frac{1}{4}\\) and \\(P\\left( \\left| Z \\right|\u0026gt;3 \\right)\\le \\frac{1}{9}\\).\nProof of Chebyshev’s Inequality\rLet’s take \\[P\\left( \\left| X-\\mu \\right|\u0026gt;t \\right)=P\\left( {{\\left| X-\\mu \\right|}^{2}}\u0026gt;{{t}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{t}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{t}^{2}}}\\] Let’s take \\[P\\left( \\left| \\frac{X-\\mu }{\\sigma } \\right|\u0026gt;\\sigma k \\right)=P\\left( {{\\left| \\frac{X-\\mu }{\\sigma } \\right|}^{2}}\u0026gt;{{\\sigma }^{2}}{{k}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{1}{{{k}^{2}}}\\]\n\r\rTheorem 4: Hoeffding Inequality\rIn probability theory, Hoeffding’s inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount. Hoeffding’s inequality was proven by Wassily Hoeffding in 1963. This inequality is sharper than Markov inequality and we can create upper bound without knowing the variance.\nIf \\(a\u0026lt;X\u0026lt;b\\) and \\(\\mu =E\\left( X \\right)\\) then \\(P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\).\nProof (part-A)\rLet’s assume \\(\\mu =0\\). If data is don’t have \\(\\mu =0\\), we can always center the data and \\(a\u0026lt;X\u0026lt;b\\). Now\n\\[X=\\gamma a+\\left( 1-\\gamma \\right)b\\] where \\(0\u0026lt;\\gamma \u0026lt;1\\) and \\(\\gamma =\\frac{X-a}{b-a}\\). With convexity we can write: \\[{{e}^{tX}}\\le \\gamma {{e}^{tb}}+\\left( 1-\\gamma \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\left( 1-\\frac{X-a}{b-a} \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\frac{b-X}{b-a}{{e}^{ta}}\\] \\[{{e}^{tX}}\\le \\frac{X{{e}^{tb}}-a{{e}^{tb}}+b{{e}^{ta}}-X{{e}^{ta}}}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}\\] Let’s take the expectation on the both sides: \\[E\\left( {{e}^{tX}} \\right)\\le E\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+E\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}E\\left( X \\right)\\] Since \\(\\mu =E\\left( X \\right)=0\\) therefore, \\[E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+0\\] \\[E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}\\]\nLet’s define \\[{{e}^{g\\left( t \\right)}}=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}\\]\nTaking \\(log\\) on the both sides:\n\\[\\log \\left( {{e}^{g\\left( t \\right)}} \\right)=\\log \\left( \\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a} \\right)\\]\n\\[g\\left( t \\right)=\\log \\left( {{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right) \\right)-\\log \\left( b-a \\right)\\]\n\\[g\\left( t \\right)=\\log \\left( {{e}^{ta}} \\right)+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)\\]\n\\[\\begin{equation}\rg\\left( t \\right)=ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)\r\\end{equation}\\]\r\rTaylor series expansion\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) , we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots \\] For a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) that is \\(m\\) times differentiable, we can express with Taylor series expansion as: \\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}\\] where \\({{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}\\) and and \\(\\xi\\) lies between \\(x\\) and \\({{x}_{0}}\\).\n\rProof (part-B)\rNow, let’s evaluate \\(g\\left( t=0 \\right)\\) we get:\r\\[\\begin{equation}\rg\\left( t=0 \\right)=g\\left( 0 \\right)=0+\\log \\left( b-a \\right)-\\log \\left( b-a \\right)=0\r\\end{equation}\\]\rLet’s evaluate \\({g}\u0026#39;\\left( t=0 \\right)\\) but before that lets find \\({g}\u0026#39;\\left( t \\right)\\). \\[\\frac{dg\\left( t \\right)}{dt}={g}\u0026#39;\\left( t \\right)=\\frac{d\\left( ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right) \\right)}{dt}\\] \\[{g}\u0026#39;\\left( t \\right)=\\frac{d\\left( ta \\right)}{dt}+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}\\] \\[{g}\u0026#39;\\left( t \\right)=a+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}\\frac{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\underbrace{\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}}_{0}\\] \\[{g}\u0026#39;\\left( t \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}\\] Consider the second term: \\[\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right){{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a{{e}^{t\\left( b-a \\right)}}{{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a}=\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}\\] \\[{g}\u0026#39;\\left( t \\right)=a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}\\] Now Let’s evaluate \\({g}\u0026#39;\\left( t=0 \\right)\\), we get\r\\[\\begin{equation}\r{g}\u0026#39;\\left( t=0 \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{0\\left( b-a \\right)}}}{b-a{{e}^{0\\left( b-a \\right)}}}=a+\\frac{-a\\left( b-a \\right)}{b-a}=0\r\\end{equation}\\]\rNow let’s take\\({{g}\u0026#39;}\u0026#39;\\left( t \\right)\\).\n\\[\\frac{d{g}\u0026#39;\\left( t \\right)}{dt}={{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{d\\left( a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}=\\frac{d\\left( \\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}\\]\n\\[{{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{-a\\left( b-a \\right)\\left( -b \\right)\\left( -\\left( b-a \\right){{e}^{-t\\left( b-a \\right)}} \\right)}{{{\\left( a+b{{e}^{-t\\left( b-a \\right)}} \\right)}^{2}}}=\\frac{ab{{\\left( b-a \\right)}^{2}}\\left[ -{{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\]\nNow we can compare following two terms. \\[a{{e}^{t\\left( b-a \\right)}}\\ge a\\]\nNegate \\(b\\) and square on the both sides:\n\\[{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}\\ge {{\\left( a-b \\right)}^{2}}={{\\left( b-a \\right)}^{2}}\\]\n\\[\\frac{1}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{1}{{{\\left( b-a \\right)}^{2}}}\\]\nFrom above inequality, we can write:\n\\[{{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{-ab{{\\left( b-a \\right)}^{2}}\\left[ {{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{-ab{{\\left( b-a \\right)}^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\n\\[\\begin{equation}\r{g}\u0026#39;\u0026#39;\\left( t \\right)\\le -ab=\\frac{{{\\left( a-b \\right)}^{2}}-{{\\left( b-a \\right)}^{2}}}{4}\\le \\frac{{{\\left( b-a \\right)}^{2}}}{4}\r\\end{equation}\\]\rNow, with Taylor series expansion we have:\n\\[g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026#39;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( 0 \\right)+\\cdots\\]\nAnd with truncating Taylor series expansion, we can write. (Note this is not approximation, its exact)\n\\[g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026#39;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( \\xi \\right)=\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( \\xi \\right)\\le \\frac{1}{2!}{{t}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{4}\\]\n\\[\\begin{equation}\rg\\left( t \\right)\\le \\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\r\\end{equation}\\]\r\rProof (part-C)\rWe have bound \\(E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}\\) and \\({{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\) .\nConsider \\[P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\]\nAnd Now with Markov inequality:\n\\[P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}\\]\nNow we can make it sharper by following argument:\n\\[P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,\\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}\\]\n\rProof for sharper version with Chernoff’s method\rlet define: \\(u=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\\) and find the minima as setting FOC as \\({u}\u0026#39;\\left( t \\right)=\\varepsilon -\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0\\) and \\({{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}\\) then substituting to get:\n\\[{{u}_{\\min }}=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-{{\\left( \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}} \\right)}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}\\]\n\\[{{u}_{\\min }}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\nThe reason we want to get \\({{u}_{\\min }}\\) is to make sharper argument for the inequality or alternatively we would like to bound for the minima. Now substituting:\n\\[P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le {{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\n\\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nThis is very important results, because there is no mean or variance, so this result is very cogitative. If we observe any type of random variable whose functional form is unknown, the above statement is true.\n\rProof for random variable with non zero mean.\rNow we can apply with the mean ie. \\(\\mu =E\\left( X \\right)\\) and \\(Y=x-\\mu\\) i.e. \\(a-\\mu \u0026lt;Y\u0026lt;b-\\mu\\). And:\n\\[P\\left( \\left| Y \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-\\mu -a+\\mu \\right)}^{2}}}}}=2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nSo, \\(P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\) is known as Hoeffding’s Inequality. This shows that the variation of the random variable beyond its mean by certain amount \\(\\varepsilon\\) is upper bounded by \\(2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\). This is true for any random variable so it’s very powerful generalization.\n\rProof for bound of mean\rLet’s define \\({{\\bar{Y}}_{n}}=\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\\) and \\({{Y}_{i}}\\) are i.id then let’s bound it as:\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;\\varepsilon \\right)=P\\left( \\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;n\\varepsilon \\right)=P\\left( {{e}^{\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{n\\varepsilon }} \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\]\nNote, we introduce \\(t\\) there that’s for the flexibility that later, I can choose \\(t\\). Now with Markov inequality we can write under the assumption of i.i.d of \\({{Y}_{i}}\\)\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\le {{e}^{-tn\\varepsilon }}E\\left[ {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}} \\right]={{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\]\nSince, we have bound \\(E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}\\) as \\({{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\) , therefore,\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le {{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\le {{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\]\nLet’s try to put sharper bound and try and solve for \\(u\\left( t \\right)=tn\\varepsilon -n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\\) and the FOC is \\({u}\u0026#39;\\left( t \\right)=n\\varepsilon -n\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0\\) and solving we get \\[{{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}\\] and the plugging the value of \\({{t}^{*}}\\) on \\(u\\left( t \\right)\\) gives:\n\\[{{u}_{\\min }}={{t}^{*}}n\\varepsilon -n\\frac{{{t}^{*}}^{2}{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}n\\varepsilon -n\\frac{{{\\left( 4\\varepsilon \\right)}^{2}}}{{{\\left( {{\\left( b-a \\right)}^{2}} \\right)}^{2}}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}-\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\nThen,\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,{{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{\\frac{-2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nThen,\n\\[P\\left( \\left| {{{\\bar{Y}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nHence, this gives the bound on the mean.\n\rProof for Binominal\rHoeffding’s inequality for the \\({{Y}_{1}}\\sim Ber\\left( p \\right)\\) and it’s upper bound is \\(1\\) and lower bound is \\(0\\) so \\({{\\left( b-a \\right)}^{2}}=1\\) and with Hoeffding inequality \\[P\\left( \\left| {{{\\bar{X}}}_{n}}-p \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-2n{{\\varepsilon }^{2}}}}\\]\n\r\rTheorem 5: Kullback Leibler Distance\rProof for distance between density is greater than zero.\rProof that the distance between any two density \\(p\\) and \\(q\\) whose random variable is \\(X\\tilde{\\ }p\\) (\\(p\\) is some distribution) is always greater than or equal to zero.\nPrior answering this, let’s quickly note two inequality, namely Cauchy-Swartz Inequality and Jensen’s inequality.\nCauchy-Swartz Inequality\r\\(\\left| EXY \\right|\\le E\\left| XY \\right|\\le \\sqrt{E\\left( {{X}^{2}} \\right)}\\sqrt{E\\left( {{Y}^{2}} \\right)}\\).\n\rJensen’s inequality\rIf \\(g\\) is convex then\\(Eg\\left( X \\right)\\ge g\\left( EX \\right)\\). If \\(g\\) is concave, then\\(Eg\\left( X \\right)\\le g\\left( EX \\right)\\).\n\rKullback Leibler Distance\rThe distance between two density \\(p\\) and \\(q\\) is defined by the Kullback Leibler Distance, and given as:\n\\[D\\left( p,q \\right)=\\int{p\\left( x \\right)\\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right)}dx\\]\nBefore, I move ahead, note that the self-distance between density \\(p\\) to \\(p\\) is zero and given as: \\(D\\left( p,p \\right)=0\\) and by definition distance is always greater than and equal to zero so, one thing we have to confirm is that distance between two density \\(p\\) and \\(q\\) i.e. \\(D\\left( p,q \\right)\\ge 0\\). But we will use Jensen inequality to proof \\(D\\left( p,q \\right)\\ge 0\\). Since the \\(\\log\\) function is concave in nature, so we can write, Jensen inequality that:\n\\[-D\\left( p,q \\right)=E\\left[ \\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]\\le \\log \\left[ E\\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]=\\log \\int{p\\left( x \\right)\\frac{q\\left( x \\right)}{p\\left( x \\right)}dx}=\\log \\int{q\\left( x \\right)dx}=\\log \\left( 1 \\right)=0\\]\ni.e\n\\[-D\\left( p,q \\right)\\le 0\\] i.e. \\[D\\left( p,q \\right)\\ge 0\\]\n\r\r\rTheorem 6: Maximum of a random variable\rLet \\({{X}_{i}},\\ldots {{X}_{n}}\\) be random variable. Suppose there exist \\(\\sigma \u0026gt;0\\) such that \\(E\\left( {{e}^{t{{X}_{i}}}} \\right)\\le {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\). Then, \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}\\).\nMaximum of a random variable represents how to bound maximum value of a random variable? i.e. Say the random variable is \\({{X}_{1}},\\ldots ,{{X}_{n}}\\) and say it is arranged in ascending order such that \\({{X}_{\\left( 1 \\right)}}\\le {{X}_{\\left( 2 \\right)}}\\le \\ldots \\le {{X}_{\\left( n \\right)}}\\) and \\({{X}_{\\left( n \\right)}}={{E}_{\\max }}\\left\\{ {{X}_{1}},\\cdots ,{{X}_{n}} \\right\\}\\)how to compute the distribution of that maximum value? Now the interesting thing is, say, that we don’t know the exact distribution of \\(X\\), so can we say in general without knowing the distribution that what is the maximum of a random variable?\nLet’s start with the expectation of the moment generating function given as: \\(E{{e}^{t{{X}_{i}}}}\\) then, it is bounded by \\(E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\) i.e. \\(E{{e}^{t{{X}_{i}}}}\\le E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\). Now, can we bound the maximum of \\(E{{e}^{t{{X}_{i}}}}\\)or alternatively, what is the \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\) ?\nLet’s, start with \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\) and pre-multiply this with \\(t\\) and exponentiate. i.e. \\(\\exp \\left\\{ tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}} \\right\\}\\), bounding this gives also is same as bounding \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\). Now with Jensen’s inequality we can write:\n\\[{{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le E{{e}^{t\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}=E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{e}^{t{{X}_{i}}}}\\le \\sum\\limits_{i=1}^{n}{E{{e}^{t{{X}_{i}}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\] \\[{{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\] Taking \\(\\log\\) on the both sides \\[tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\log {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\] \\[tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}\\] Dividing both sides by \\(t\\), we get \\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}\\] Now, let’s take this \\(\\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}\\) and optimize w.r.t \\(t\\) we get: \\(\\log n=\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}\\) and \\(t={{\\sigma }^{-1}}\\sqrt{2\\log n}\\). Now plugging this value, we get:\n\\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}=\\frac{2\\log n+{{t}^{2}}{{\\sigma }^{2}}}{2t}=\\frac{2\\log n+{{\\left( {{\\sigma }^{-1}}\\sqrt{2\\log n} \\right)}^{2}}{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}\\] \\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{2\\log n+{{\\sigma }^{-2}}2\\log n{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}=\\frac{2\\sqrt{2}\\sqrt{2}\\log n}{2{{\\sigma }^{-1}}\\sqrt{2}\\sqrt{\\log n}}=\\sigma \\sqrt{2\\log n}\\] Hence \\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}\\]\n\r","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"91e2a476693769ccbc5adc50e9309aae","permalink":"/post/2018-12-29-proof-of-hoeffding-inequality/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/2018-12-29-proof-of-hoeffding-inequality/","section":"post","summary":"Theorem 1: Gaussian Tail Inequality\rGiven \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) then, \\(P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\) and \\(P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\).\nProof of Gaussian Tail Inequality\rConsider a univariate \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\), then the probability density function is given as \\(\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\). Let’s take the derivative w.r.t \\(x\\) we get: \\[\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026#39;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)\\] Let’s define the gaussian tail inequality.","tags":["Probability,","R,","Statistics,","Theory,","Proof,"],"title":"Probability Inequality","type":"post"},{"authors":["Shishir Shakya","Rabindra Nepal","Kishor Sharma"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"0ff6586922027f0e32c6c98b00bae348","permalink":"/publication/2018-ijgei/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/2018-ijgei/","section":"publication","summary":"We proposed thumb-rules that can predict household poverty in Nepal based on 14,907 household observations employing classification and regression tree (CART) approach.","tags":null,"title":"Electricity consumption and economic growth: empirical evidence from a resource-rich landlocked economy (Published)","type":"publication"},{"authors":["Zarina Ismailova","Xiaoli L. Etienne","Shishir Shakya","Fabio Mattos"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"9ef0ccc0da48f5fe33086e51b7c08a45","permalink":"/publication/2018-lumber/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/publication/2018-lumber/","section":"publication","summary":"In this paper, we examine the impact of two housing market reports, namely the New Residential Construction (Housing Starts) and the New Residential Sales reports, on the U.S. lumber futures market.","tags":null,"title":"Quantifying the Announcement Effects in the U.S. Lumber Market","type":"publication"},{"authors":["Jousha C. Hall","Shishir Shakya"],"categories":null,"content":"","date":1519862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"59ac9465f9cccee5e92ee93aa18a56e3","permalink":"/publication/2018-federalregulation/","publishdate":"2018-03-01T00:00:00Z","relpermalink":"/publication/2018-federalregulation/","section":"publication","summary":"We show inverted U-shaped relationship between industry-level regulations and industry output.","tags":null,"title":"Federal Regulations and U.S. Energy Sector Output","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Shishir Shakya"],"categories":null,"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"28a24b5dec3e2aedf8c81323e66dbb13","permalink":"/publication/2014-quotas/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publication/2014-quotas/","section":"publication","summary":"We proposed thumb-rules that can predict household poverty in Nepal based on 14,907 household observations employing classification and regression tree (CART) approach.","tags":null,"title":"Possible Decision Rules to Allocate Quotas and Reservations to Ensure Equity for Nepalese Poor (Published)","type":"publication"},{"authors":["Feiyu Liu","Shishir Shakya"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2a9b9d115fdd649be9ecc35e753e088b","permalink":"/publication/2018-efficiencytest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/2018-efficiencytest/","section":"publication","summary":"We find evidence that returns predictability associated with market conditions.","tags":null,"title":"Efficiency Test of Commodity Futures Market","type":"publication"}]